{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyber Security Statistical Inference Pipeline\n",
    "\n",
    "Using [this](https://www.kaggle.com/datasets/dnkumars/cybersecurity-intrusion-detection-dataset) cyber security dataset on Kaggle, we will be using this dataset to discuss about most of the statistical tools that we have been learning about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: What can lead to an `Attack`? üí≠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Cleaning\n",
    "\n",
    "Basic cleaning of data set and som feature understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cybersecurity_intrusion_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See who is more corrolated with `attack_detected`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions\n",
    "\n",
    "Looking at the distributions for attack vs non-attack scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_compare = ['network_packet_size', 'session_duration', 'failed_logins', 'ip_reputation_score']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, feature in enumerate(features_to_compare, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.histplot(df[df['attack_detected'] == 0][feature], label='Normal', kde=True, color='blue', bins=30)\n",
    "    sns.histplot(df[df['attack_detected'] == 1][feature], label='Attack', kde=True, color='red', bins=30)\n",
    "    plt.title(f'Distribution of {feature} (Attack vs Normal)')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DImensionality Reduction\n",
    "\n",
    "Looking at the ELBO PCA curve to decide the number of PCs we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(columns=['attack_detected'])\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features.select_dtypes(include=[np.number]))\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(scaled_features)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "elbo_curve = np.log(1 - cumulative_variance) \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(elbo_curve) + 1), elbo_curve, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Log(1 - Cumulative Explained Variance)')\n",
    "plt.title('ELBO Curve for PCA Component Selection')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct PCA to three dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(scaled_features)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance by Component: {explained_variance}\")\n",
    "print(f\"Total Explained Variance: {np.sum(explained_variance):.4f}\")\n",
    "\n",
    "df_pca = pd.DataFrame(pca_result, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca['attack_detected'] = df['attack_detected']\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(df_pca['PC1'], df_pca['PC2'], df_pca['PC3'], \n",
    "                     c=df_pca['attack_detected'], cmap='coolwarm', alpha=0.6)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({explained_variance[0]*100:.2f}% Variance)')\n",
    "ax.set_ylabel(f'PC2 ({explained_variance[1]*100:.2f}% Variance)')\n",
    "ax.set_zlabel(f'PC3 ({explained_variance[2]*100:.2f}% Variance)')\n",
    "ax.set_title('3D PCA Projection of Data')\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Attack Detected')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Let's look at some feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.countplot(x='protocol_type', hue='attack_detected', data=df, ax=axes[0])\n",
    "axes[0].set_title('Protocol Type Distribution by Attack Status')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=90)\n",
    "\n",
    "sns.countplot(x='encryption_used', hue='attack_detected', data=df, ax=axes[1])\n",
    "axes[1].set_title('Encryption Method vs Attack Detection')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=90)\n",
    "\n",
    "sns.boxplot(x='attack_detected', y='ip_reputation_score', data=df, ax=axes[2])\n",
    "axes[2].set_title('IP Reputation Score vs Attack Detection')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le  # encoder for inverse transformation later\n",
    "\n",
    "X = df.drop(columns=['attack_detected'])\n",
    "y = df['attack_detected']\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(12, 6))\n",
    "importances[:10].plot(kind='bar')\n",
    "plt.title('Top 10 Important Features for Attack Detection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature interaction effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['network_packet_size', 'failed_logins', 'session_duration', 'ip_reputation_score', 'attack_detected']], hue='attack_detected')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(300)\n",
    "G = nx.Graph()\n",
    "\n",
    "# session_id as nodes\n",
    "for session in df_sample['session_id']:\n",
    "    G.add_node(session, attack=df_sample[df_sample['session_id'] == session]['attack_detected'].values[0])\n",
    "\n",
    "# edges based on similarities\n",
    "for i, row1 in df_sample.iterrows():\n",
    "    for j, row2 in df_sample.iterrows():\n",
    "        if i >= j: \n",
    "            continue\n",
    "        \n",
    "        same_attack_status = row1['attack_detected'] == row2['attack_detected']\n",
    "        same_protocol = row1['protocol_type'] == row2['protocol_type']\n",
    "        same_encryption = row1['encryption_used'] == row2['encryption_used']\n",
    "\n",
    "        # if sessions share attack status, protocol, or encryption, connect them\n",
    "        if same_attack_status or same_protocol or same_encryption:\n",
    "            G.add_edge(row1['session_id'], row2['session_id'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "node_colors = ['red' if G.nodes[node]['attack'] == 1 else 'blue' for node in G.nodes]\n",
    "\n",
    "nx.draw(G, pos, with_labels=False, node_size=10, node_color=node_colors, edge_color='gray', alpha=0.5)\n",
    "plt.title(\"Session-Based Network Graph (Red = Attack, Blue = Normal)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph uses each `session_id` as a node and try to construct a graph around each node by looking at smilarity metric based on `attack_detected`, `protocol_type`, and `encryption_used`. This tries to identify connections between each node and see if the three similarity featuires are actually sort of important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsitcial Analysis: What Leads to An Attack?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric & Non-parametric Hypotheiss Testing ü§î\n",
    "We can begin with some simple questions from the observations we ahve made in EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Do attack sessions have significantly different session durations compared to normal sessions?\n",
    "> * We can do Two-sample t-test (if gaussian distribution) or KS-test (if non-gaussian distribution).\n",
    "> * Hypothesis:\n",
    ">   * Null (H‚ÇÄ): The mean session duration for attack and normal traffic is the same.\n",
    ">   * Alternative (H‚ÇÅ): Attack sessions have a different mean session duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' check the normality of `session_duration` distribution\n",
    "\n",
    "- We can use the KDE (Kernel Density Estimation) visualization, it creates a smooth curve that approximates the data's underlying distribution.\n",
    "\n",
    "- If the histogram resembles a bell curve (unimodal & symmetric), it suggests normality.\n",
    "\n",
    "- However, it does not provide a formal normality test (such as the Shapiro-Wilk test or Kolmogorov-Smirnov test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into attack vs normal session durations\n",
    "attack_sessions = df[df['attack_detected'] == 1]['session_duration']\n",
    "normal_sessions = df[df['attack_detected'] == 0]['session_duration']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(attack_sessions, kde=True, label='Attack', color='red', bins=30)\n",
    "sns.histplot(normal_sessions, kde=True, label='Normal', color='blue', bins=30)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Session Duration (Attack vs Normal)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, we can scheck normality via `QQ-Plot`, `Shapiro-Wilk Test`, and `Kolmogorov-Smirnov Test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro, kstest, probplot\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "probplot(attack_sessions, dist=\"norm\", plot=axes[0])\n",
    "axes[0].set_title(\"QQ-Plot for Attack Session Durations\")\n",
    "\n",
    "probplot(normal_sessions, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title(\"QQ-Plot for Normal Session Durations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "shapiro_attack = shapiro(attack_sessions.sample(min(5000, len(attack_sessions))))\n",
    "shapiro_normal = shapiro(normal_sessions.sample(min(5000, len(normal_sessions))))\n",
    "print(f\"Shapiro-Wilk Test (Attack): W={shapiro_attack[0]:.3f}, p={shapiro_attack[1]:.3f}\")\n",
    "print(f\"Shapiro-Wilk Test (Normal): W={shapiro_normal[0]:.3f}, p={shapiro_normal[1]:.3f}\")\n",
    "\n",
    "ks_attack = kstest(attack_sessions, 'norm', args=(attack_sessions.mean(), attack_sessions.std()))\n",
    "ks_normal = kstest(normal_sessions, 'norm', args=(normal_sessions.mean(), normal_sessions.std()))\n",
    "print(f\"KS-Test (Attack): Statistic={ks_attack.statistic:.3f}, p-value={ks_attack.pvalue:.3f}\")\n",
    "print(f\"KS-Test (Normal): Statistic={ks_normal.statistic:.3f}, p-value={ks_normal.pvalue:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the distributions **are not normally distributed**, we will be doing the `KS-test`.\n",
    "\n",
    "- Good thing with non-parametric hypothesis testing is that we don't need to make any assumption,making it much easier to do hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, ks_2samp\n",
    "\n",
    "# KS-test (non-parametric)\n",
    "ks_stat, p_value_ks = ks_2samp(attack_sessions, normal_sessions)\n",
    "print(f\"KS-Test: statistic={ks_stat:.3f}, p-value={p_value_ks:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `KS-test` passes the alpha value threshold of 0.05, hence **we reject the null hypotehsis that the mean session duration for attack and normal traffic is the same.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Do attack/no atack follow the same distribution in ip reputation score?\n",
    "> We can do T-test or KS-test dependening on normality.\n",
    "> * Hypothesis: \n",
    ">   * Null Hypothesis (H‚ÇÄ): The distributions are the same.\n",
    ">   * Alternative Hypothesis (H‚ÇÅ): Attack sessions follow a different distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we want to check teh normality assumption first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_sessions = df[df['attack_detected'] == 1]['ip_reputation_score']\n",
    "normal_sessions = df[df['attack_detected'] == 0]['ip_reputation_score']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(attack_sessions, kde=True, label='Attack', color='red', bins=30)\n",
    "sns.histplot(normal_sessions, kde=True, label='Normal', color='blue', bins=30)\n",
    "plt.legend()\n",
    "plt.title('Distribution of IP Reputation Score (Attack vs Normal)')\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "probplot(attack_sessions, dist=\"norm\", plot=axes[0])\n",
    "axes[0].set_title(\"QQ-Plot for Attack Session Reputation Score\")\n",
    "\n",
    "probplot(normal_sessions, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title(\"QQ-Plot for Normal Session Reputation Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "shapiro_attack = shapiro(attack_sessions.sample(min(5000, len(attack_sessions))))\n",
    "shapiro_normal = shapiro(normal_sessions.sample(min(5000, len(normal_sessions))))\n",
    "print(f\"Shapiro-Wilk Test (Attack): W={shapiro_attack[0]:.3f}, p={shapiro_attack[1]:.3f}\")\n",
    "print(f\"Shapiro-Wilk Test (Normal): W={shapiro_normal[0]:.3f}, p={shapiro_normal[1]:.3f}\")\n",
    "\n",
    "ks_attack = kstest(attack_sessions, 'norm', args=(attack_sessions.mean(), attack_sessions.std()))\n",
    "ks_normal = kstest(normal_sessions, 'norm', args=(normal_sessions.mean(), normal_sessions.std()))\n",
    "print(f\"KS-Test (Attack): Statistic={ks_attack.statistic:.3f}, p-value={ks_attack.pvalue:.3f}\")\n",
    "print(f\"KS-Test (Normal): Statistic={ks_normal.statistic:.3f}, p-value={ks_normal.pvalue:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be much more normally distributed comparing to `session_duration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_sessions = df[df['attack_detected'] == 1]['ip_reputation_score']\n",
    "normal_sessions = df[df['attack_detected'] == 0]['ip_reputation_score']\n",
    "\n",
    "t_stat, p_value_ttest = ttest_ind(attack_sessions, normal_sessions, equal_var=False)\n",
    "print(f\"T-test: t-statistic={t_stat:.3f}, p-value={p_value_ttest:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `T-test` passes the alpha value threshold of 0.05, hence **we reject the null hypotehsis that the ip reputation score for attack and normal traffic is the same.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Are attack sessions randomly distributed throughout the dataset?\n",
    "> We can do Wald-Wolfowitz Runs Test for randomness\n",
    "> * Hypothesis:\n",
    ">   * Null Hypothesis (H‚ÇÄ): Attack sessions are randomly distributed.\n",
    ">   * Alternative Hypothesis (H‚ÇÅ): Attack sessions follow a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.sandbox.stats.runs import runstest_1samp\n",
    "\n",
    "runs_p_value = runstest_1samp(df['attack_detected'])\n",
    "print(f\"Wald-Wolfowitz Runs Test p-value: {runs_p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about some **ordinal test** for some of the ordinal columns we have, such as `failed_login_attempts`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Do attack sessions have significantly different failed login attempts compared to normal sessions?\n",
    "> * We can do the **Mann-Whitney U Test** (Wilcoxon Rank-Sum Test) as we want a non-parametric test for comparing ordinal data distributions between two independent groups.\n",
    "> * Hypothesis:\n",
    ">   * H‚ÇÄ: Attack and normal sessions have the same median failed logins.\n",
    ">   * H‚ÇÅ: Attack and normal sessions differ in failed logins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can check underlaying distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(df['failed_logins'], fill=True, bw_adjust=0.5, color='blue')\n",
    "plt.title('Kernel Density Estimation (KDE) of Failed Login Attempts')\n",
    "plt.xlabel('Failed Login Attempts')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, normallity assumption does not hold and we need to do **non-parametric testing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "attack_failed_logins = df[df['attack_detected'] == 1]['failed_logins']\n",
    "normal_failed_logins = df[df['attack_detected'] == 0]['failed_logins']\n",
    "\n",
    "mw_stat, mw_p = mannwhitneyu(attack_failed_logins, normal_failed_logins, alternative=\"two-sided\")\n",
    "print(f\"Mann-Whitney U Test: Statistic={mw_stat:.3f}, p-value={mw_p:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Mann-Whitney U Test` passes the alpha value threshold of 0.05, hence **we reject the null hypotehsis that the failed login attempts for attack and normal traffic is the same.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we have found many **covariate** that effects the responses greatly can we have a more systematic way of doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis üöÄ\n",
    "\n",
    "Since `attack_detected` is a **binary response variable** (0 = normal, 1 = attack), logistic regression is the ideal statistical method for modeling attack detection probabilities.\n",
    "\n",
    "*We will be using both packages coming from `sklearn` and `smf` since they serve good purpos at different part of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Logistic Regression\n",
    "\n",
    "Let's fit a baseline logistic regression model frst to examine the effects of each of the covariates. Notice that we will be using `smf.logit` here. Notice that we introduced one hot encoding in earlier sections for later models to wor better, but for baseline model, we want to interpret all of the coefficients, so we will maintain the original data typ (i.e categorical) and use `smf.logit` to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.read_csv(\"data/cybersecurity_intrusion_data.csv\")\n",
    "to_category_columns = ['protocol_type', 'encryption_used', 'browser_type']\n",
    "df_base[to_category_columns] = df_base[to_category_columns].astype('category')\n",
    "X = df_base.drop(columns=['attack_detected', 'session_id'])\n",
    "formula = 'attack_detected ~ ' + (\" + \".join(list(X.columns)))\n",
    "logit_model = smf.logit(formula, data=df_base).fit()\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratios = np.exp(logit_model.params)\n",
    "print(\"\\nOdds Ratios:\\n\", odds_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA ü§î ‚û°Ô∏è ‚ùå\n",
    "We want to start with **Analysis of Variance**, seeing if adding extra variabl actually improves the explanability of the varaince. However, we cannot do so as ANOVA is made for linear regressions and assumption need to be held for linear regression cases instead of the logistic function that we are using. But maybe we can use alternative methods to test the importance of covaraites, namely **forward/backward selection** + **shrinking method with cross validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward/Backward Stepwise Selection\n",
    "\n",
    "We uses the Akaike Information Criterion (AIC) to meausure the **goodness of fit** of a model, adjusted for the number of predictors. They are both based on the (log-)likelihood of the model and the number of predictors. Remember the log-likelihood of the model is given by\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = \\sum_{i=1}^{n} \\log f(y_i | x_i, \\beta)\n",
    "$$\n",
    "\n",
    "The **AIC** is defined as\n",
    "\n",
    "$$\n",
    "AIC = -2\\ell(\\beta) + 2p\n",
    "$$\n",
    "\n",
    "The key idea is that:\n",
    "- AIC tells you given teh previous predictor, how much does the new one explain teh variance.\n",
    "- Smaller AIC  resembles better model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward selection based on AIC\n",
    "def forward_selection(data, target, predictors):\n",
    "    selected_predictors = []\n",
    "    remaining_predictors = list(predictors)\n",
    "    best_aic = float(\"inf\")\n",
    "    \n",
    "    while remaining_predictors:\n",
    "        best_candidate = None\n",
    "        for predictor in remaining_predictors:\n",
    "            formula = f\"{target} ~ {' + '.join(selected_predictors + [predictor])}\"\n",
    "            model = smf.logit(formula, data=data).fit(disp=0)\n",
    "            if model.aic < best_aic:\n",
    "                best_aic = model.aic\n",
    "                best_candidate = predictor\n",
    "        \n",
    "        if best_candidate:\n",
    "            remaining_predictors.remove(best_candidate)\n",
    "            selected_predictors.append(best_candidate)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return selected_predictors\n",
    "\n",
    "# backward elimination based on AIC\n",
    "def backward_elimination(data, target, predictors):\n",
    "    selected_predictors = list(predictors)\n",
    "    best_aic = float(\"inf\")\n",
    "    \n",
    "    while len(selected_predictors) > 1:\n",
    "        worst_candidate = None\n",
    "        for predictor in selected_predictors:\n",
    "            reduced_predictors = selected_predictors.copy()\n",
    "            reduced_predictors.remove(predictor)\n",
    "            formula = f\"{target} ~ {' + '.join(reduced_predictors)}\"\n",
    "            model = smf.logit(formula, data=data).fit(disp=0)\n",
    "            if model.aic < best_aic:\n",
    "                best_aic = model.aic\n",
    "                worst_candidate = predictor\n",
    "        \n",
    "        if worst_candidate:\n",
    "            selected_predictors.remove(worst_candidate)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return selected_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = df.drop(columns=['attack_detected']).columns\n",
    "\n",
    "selected_features_forward = forward_selection(df, 'attack_detected', covariates)\n",
    "print(\"Selected Features (Forward Selection):\", selected_features_forward)\n",
    "\n",
    "selected_features_backward = backward_elimination(df, 'attack_detected', covariates)\n",
    "print(\"Selected Features (Backward Elimination):\", selected_features_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the **forward** and **backward** method does find different fearures that effect AIC, but they seem to be similar at least.\n",
    "\n",
    "We will do similar things by using **shrinking method in the below section**, which wil be the actual fina feature selection that we give into our final logistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinking Method\n",
    "\n",
    "Notice in this section we use `sklearn` here because they are better to work with when trying to do regularziations then `smf.logit` as well as that they seem to be a littl bit faster to run, which is good since we are doing grid search cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['attack_detected']) \n",
    "y = df['attack_detected'] \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "alphas = np.logspace(-1, 5, 200)  # log-spaced values for better resolution\n",
    "\n",
    "lasso_coefs = []\n",
    "for alpha in alphas:\n",
    "    lasso = LogisticRegression(penalty='l1', solver='liblinear', C=1/alpha, max_iter=500)\n",
    "    lasso.fit(X_scaled, y)\n",
    "    lasso_coefs.append(lasso.coef_[0])\n",
    "\n",
    "lasso_coefs = pd.DataFrame(lasso_coefs, index=alphas, columns=df.drop(columns=['attack_detected']).columns)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for column in lasso_coefs.columns:\n",
    "    ax.plot(alphas, lasso_coefs[column], label=column)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Alpha (Regularization Strength)')\n",
    "ax.set_ylabel('Coefficient Value')\n",
    "ax.set_title('Lasso Regularization Path for Logistic Regression')\n",
    "ax.legend(loc='best', fontsize=8, frameon=True)\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remanber that from the previous forward/backward selection,we ahve the following features:\n",
    "\n",
    "- Forward Selection: `failed_logins`, `login_attempts`, `ip_reputation_score`, `browser_type`, `session_duration`, and `encryption_used`.\n",
    "\n",
    "- Backward Elimination: `login_attempts`, `session_duration`, `encryption_used`, `ip_reputation_score`, `failed_logins`, and `browser_type`.\n",
    "\n",
    "From the shrinking method, seems like `failed_logins`, `login_attempts`, `ip_reputation_score`, `browser_type`,`session_duration`, and `encryption_used` seems  to be coefficient that are relatively more important. Note that this is consistent with teh forward selction model, which we will be using them in our final logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha values (inverse of C)\n",
    "alphas = np.logspace(-3, 3, 300) \n",
    "cv_errors = np.zeros_like(alphas)\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    lasso = LogisticRegression(penalty='l1', solver='liblinear', C=1/alpha, max_iter=500)\n",
    "    cv_scores = cross_val_score(lasso, X_scaled, y, cv=5, scoring='neg_log_loss')  # Log loss as error metric\n",
    "    cv_errors[i] = -np.mean(cv_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(alphas, cv_errors, label=\"CV Error\")\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('Cross-Validation Error')\n",
    "plt.title('Lasso Cross-Validation Error vs. Regularization Strength')\n",
    "\n",
    "optimal_alpha_idx = np.argmin(cv_errors)\n",
    "optimal_alpha = alphas[optimal_alpha_idx]\n",
    "plt.scatter(optimal_alpha, cv_errors[optimal_alpha_idx], color='red', label=f'Optimal Alpha: {optimal_alpha:.3f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f'Optimal alpha: {optimal_alpha}, CV Error: {cv_errors[optimal_alpha_idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretating Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lasso = LogisticRegression(penalty='l1', solver='liblinear', C=1/optimal_alpha, max_iter=500)\n",
    "final_lasso.fit(X_scaled, y)\n",
    "\n",
    "final_coefs = pd.Series(final_lasso.coef_[0], index=df.drop(columns=['attack_detected']).columns)\n",
    "\n",
    "print(\"\\nFinal Selected Important Features & Coefficients:\")\n",
    "print(final_coefs[final_coefs != 0])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "final_coefs[final_coefs != 0].sort_values().plot(kind='barh', color='blue')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Final Lasso Model: Selected Features & Importance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Models\n",
    "\n",
    "Again, we choose to use `smf.logit` here, which is different from previous section where we uses `sklearn`, this is because that `smf.logit` is simply easier to work with interaction terms comparing to `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "if 'attack_detected' in numeric_features:\n",
    "    numeric_features.remove('attack_detected')\n",
    "    \n",
    "interaction_results = {}\n",
    "\n",
    "for feature in numeric_features:\n",
    "    formula = f\"attack_detected ~ {feature}\"\n",
    "\n",
    "    interaction_model = smf.logit(formula, data=df).fit(disp=0) \n",
    "    \n",
    "    y_scores = interaction_model.predict(df)\n",
    "    fpr, tpr, _ = roc_curve(df['attack_detected'], y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    interaction_results[feature] = roc_auc\n",
    "\n",
    "interaction_results_df = pd.DataFrame.from_dict(interaction_results, orient='index', columns=['AUC Score'])\n",
    "interaction_results_df = interaction_results_df.sort_values(by='AUC Score', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "interaction_results_df.head(10).sort_values(by='AUC Score').plot(kind='barh', legend=False, color='blue')\n",
    "plt.xlabel('AUC Score')\n",
    "plt.ylabel('Interaction Term')\n",
    "plt.title('Top 10 Interaction Terms by AUC Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "numeric_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "if 'attack_detected' in numeric_features:\n",
    "    numeric_features.remove('attack_detected')\n",
    "    \n",
    "interaction_results = {}\n",
    "\n",
    "for feature1, feature2 in combinations(numeric_features, 2):\n",
    "    interaction_term = f\"{feature1} * {feature2}\"\n",
    "    formula = f\"attack_detected ~ {interaction_term}\"\n",
    "\n",
    "    interaction_model = smf.logit(formula, data=df).fit(disp=0) \n",
    "    \n",
    "    y_scores = interaction_model.predict(df)\n",
    "    fpr, tpr, _ = roc_curve(df['attack_detected'], y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    interaction_results[interaction_term] = roc_auc\n",
    "\n",
    "interaction_results_df = pd.DataFrame.from_dict(interaction_results, orient='index', columns=['AUC Score'])\n",
    "interaction_results_df = interaction_results_df.sort_values(by='AUC Score', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "interaction_results_df.head(10).sort_values(by='AUC Score').plot(kind='barh', legend=False, color='blue')\n",
    "plt.xlabel('AUC Score')\n",
    "plt.ylabel('Interaction Term')\n",
    "plt.title('Top 10 Interaction Terms by AUC Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then fit the model again with this interaction model (note that the AUC score will change for the true model from the one observed above as teh above only uses one covaraite to fit the logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_interaction_terms = [\n",
    "    ('login_attempts', 'failed_logins'),\n",
    "    ('ip_reputation_score', 'failed_logins'),\n",
    "    ('login_attempts', 'ip_reputation_score'),\n",
    "    ('failed_logins', 'browser_type'),\n",
    "    ('encryption_used', 'failed_logins'),\n",
    "    ('network_packet_size', 'failed_logins'),\n",
    "    ('session_duration', 'failed_logins'),\n",
    "    ('protocol_type', 'failed_logins'),\n",
    "    ('failed_logins', 'unusual_time_access'),\n",
    "    ('session_id', 'failed_logins')\n",
    "]\n",
    "\n",
    "for feature1, feature2 in top_interaction_terms:\n",
    "    interaction_feature = f\"{feature1} * {feature2}\"\n",
    "    df[interaction_feature] = df[feature1] * df[feature2]\n",
    "\n",
    "X_interact = df.drop(columns=['attack_detected'])\n",
    "y = df['attack_detected']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled_interact = scaler.fit_transform(X_interact)\n",
    "\n",
    "final_lasso_interact = LogisticRegression(penalty='l1', solver='liblinear', C=1/optimal_alpha, max_iter=500)\n",
    "final_lasso_interact.fit(X_scaled_interact, y)\n",
    "final_coefs = pd.Series(final_lasso_interact.coef_[0], index=X_interact.columns)\n",
    "\n",
    "print(\"\\nFinal Selected Important Features & Coefficients:\")\n",
    "important_features = final_coefs[abs(final_coefs) >= 0.05]\n",
    "print(important_features)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "important_features.sort_values().plot(kind='barh', color='blue')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Final Lasso Model: Selected Features & Importance (Including Interactions)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Logistic Regression\n",
    "\n",
    "Since we use interaction term, we will see that `VIF` (Variance Inflation Factor) may have extremely high values, which is insufficient for us to interpret the coefficient and fitting the model. We can fit a **Principal Component Regression**. Though this will also make the coefficient uninterpretable, we can have a model that is not consisting of multi-colinear covaraites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['attack_detected'])\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled_pca = scaler.fit_transform(df.drop(columns=['attack_detected']))\n",
    "y = df['attack_detected']\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_interact)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA: Explained Variance vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# choose number of components to capture ~95% variance\n",
    "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "logistic_pca = Pipeline([('pca', pca), ('logistic', LogisticRegression(penalty='l1', solver='liblinear', max_iter=500))])\n",
    "\n",
    "cv_scores = cross_val_score(logistic_pca, X_scaled_pca, y, cv=5, scoring='accuracy')\n",
    "\n",
    "logistic_pca.fit(X_scaled_pca, y)\n",
    "\n",
    "print(f\"Optimal Number of Components: {n_components}\")\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here there might be a bug if you run the VIF code for PCA because i we only find one principal component (above 95% variance), then this code may have a problem. Usually this wouldn't happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_optimal = pca.fit_transform(X_scaled_pca)\n",
    "logistic_pca_optimal = Pipeline([('pca', pca), ('logistic', LogisticRegression(penalty='l1', solver='liblinear', max_iter=500))])\n",
    "logistic_pca_optimal.fit(X_pca_optimal, y)\n",
    "\n",
    "pca_df = pd.DataFrame(X_pca_optimal, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = pca_df.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_pca_optimal, i) for i in range(n_components)]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit of Model\n",
    "We will be examining fit of model via a **ROC + AUC** metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Lsasso Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = final_lasso.predict(X_scaled)\n",
    "fpr, tpr, _ = roc_curve(y, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--') \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model AUC: {roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Interaction Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = final_lasso_interact.predict(X_scaled_interact)\n",
    "fpr, tpr, _ = roc_curve(y, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--') \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model AUC: {roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principla Components Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logistic_pca_optimal.predict(X_pca_optimal)\n",
    "fpr, tpr, _ = roc_curve(y, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--') \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model AUC: {roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Statistical Learning: Conformal Inference\n",
    "\n",
    "We will upgrade from linear methods from statistical learning theory to non-linear methods such as nerual network. Furthermore, we want to conduct `conformal inferences` to quantify the uncertainties that we see in our **blackbox** models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Fully Connected Neural Network\n",
    "\n",
    "We will fit just a fully connected neural network in this section with use of 3 layered network, **Cross Entropy Loss**, and a **Sigmoid** function as the last output head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "model = NeuralNet(input_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "epochs = 250\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will evaluate the AUC/ROC curve just as our previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    p_test = model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, p_test)\n",
    "auc_test = roc_auc_score(y_test, p_test)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=f\"Neural Network (AUC = {auc_test:.4f})\", color=\"blue\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Classifier\")  # Baseline\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conformal Inference\n",
    "\n",
    "We will follow the following conformal inference procedure:\n",
    "\n",
    "> #### Define a heuristic (non)-conformity measure\n",
    ">\n",
    "> Define a (non)-conformity measure $s(y, x)$ which measures how well the prediction $\\hat{y}_i$ fits the actual value $y_i$.\n",
    "> \n",
    "> **Note:** this must take into account the nature of the model $f$, for example:\n",
    ">\n",
    "> Sicne $\\mathcal{Y} = \\{0,1\\}$ and $f(x) = (p_0, p_1)$ is a Logistic Regression, then we would have the scoring function as the following:\n",
    ">\n",
    ">\n",
    "> $$\n",
    "> s(y, x) = y \\log(p_1) + (1 - y) \\log(p_0)\n",
    "> $$\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Compute non-conformity scores and quantile\n",
    ">\n",
    "> For each $x_i$ in the calibration set, compute the **non-conformity score $s(y_i, \\hat{y}_i)$** that quantifies how \"unusual\" the true label is relative to the model‚Äôs prediction, i.e.,\n",
    ">\n",
    "> $$\n",
    "> s_i = s(y_i, \\hat{y}_i) \\quad \\text{for} \\quad (x_i, y_i) \\in \\mathcal{D}_{\\text{cal}}\n",
    "> $$\n",
    ">\n",
    "> For the **desired coverage** $\\alpha$, to construct a confidence set, we must determine the threshold, which ensures that at least $1 - \\alpha$ fraction of future predictions fall within this threshold. Then we just compute the $\\lfloor (1 - \\alpha) \\cdot n_{\\text{cal}} \\rfloor / n_{\\text{cal}}$-th quantile of the scores $\\{s_i\\}$, i.e.,\n",
    ">\n",
    "> $$\n",
    "> \\hat{q} = \\text{quantile} \\left( \\frac{\\lfloor (1 - \\alpha) \\cdot n_{\\text{cal}} \\rfloor}{n_{\\text{cal}}} ; \\{s_i\\} \\right)\n",
    "> $$\n",
    ">\n",
    "> However, we will  be using an different version comparing to the standard version:\n",
    ">\n",
    "> $$\n",
    "> \\hat{q}(x) = \\text{quantile} \\left( \\frac{\\lfloor (1 - \\alpha) \\cdot n_{\\text{cal}} \\rfloor}{n_{\\text{cal}}} ; \\{s_i\\} \\right) + \\lambda \\cdot (0.5 - |p(x) - 0.5|)\n",
    "> $$\n",
    ">\n",
    "> This is a more adaptive to uncertainty wheer we give **wider intervals for uncertain points (near 0.5 predicted proabbility)** since standard conformal inference assumes uniform uncertainty, which is not always true.\n",
    ">\n",
    "\n",
    "<br>\n",
    "\n",
    "> #### Compute the Prediction Set\n",
    ">\n",
    "> Given $\\hat{q}$ from step 5, the prediction set for a new observation $x$ is the ones that are beneath this quantile:\n",
    ">\n",
    "> $$\n",
    "> \\Gamma_{\\alpha}(x) = \\{ y \\in \\mathcal{Y} \\mid s(y, \\hat{f}(x)) \\leq \\hat{q} \\}\n",
    "> $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    p_train = model(X_train_tensor).numpy().flatten()\n",
    "    p_test = model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "eps = 1e-8\n",
    "nonconformity_scores = - (y_train * np.log(p_train + eps) + (1 - y_train) * np.log(1 - p_train + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conformal_intervals(alpha_base=0.1, lambda_factor=0.2):\n",
    "    \"\"\"\n",
    "    Plots NN predictions with interactive adaptive conformal intervals.\n",
    "    \"\"\"\n",
    "    # adaptive alpha based on uncertainty\n",
    "    alpha_dynamic = alpha_base + lambda_factor * (0.5 - np.abs(p_test - 0.5))\n",
    "    \n",
    "    # conformal quantile for each sample\n",
    "    q_alpha_dynamic = np.array([\n",
    "        np.percentile(nonconformity_scores, (1 - alpha) * 100) for alpha in alpha_dynamic\n",
    "    ])\n",
    "\n",
    "    # conformal prediction intervals\n",
    "    lower_bound = p_test - q_alpha_dynamic\n",
    "    upper_bound = p_test + q_alpha_dynamic\n",
    "    lower_bound = np.clip(lower_bound, 0, 1)\n",
    "    upper_bound = np.clip(upper_bound, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(range(len(p_test[:100])), p_test[:100], label=\"NN Predictions\", color='blue', alpha=0.6)\n",
    "    plt.fill_between(range(len(p_test[:100])), lower_bound[:100], upper_bound[:100], color='gray', alpha=0.3, label=\"Adaptive Conformal Interval\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Predicted Probability\")\n",
    "    plt.title(f\"NN Predictions with Adaptive Conformal Intervals\\nBase Alpha: {alpha_base}, Lambda: {lambda_factor}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the interactive code to illustrate that bigger the $\\alpha$, lower $(1 - \\alpha)$, lower confidence level of the predictions, more narrower interval, more \"precise toone prediction\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_conformal_intervals, \n",
    "         alpha_base=widgets.FloatSlider(min=0.01, max=0.5, step=0.01, value=0.1, description=\"Base Alpha\"),\n",
    "         lambda_factor=widgets.FloatSlider(min=0, max=0.5, step=0.01, value=0.2, description=\"Lambda Factor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same sense, we can check again with our pure logistic model and observe similar effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = sm.Logit(y_train, sm.add_constant(X_train)).fit()\n",
    "p_train = logit_model.predict(sm.add_constant(X_train))\n",
    "p_test = logit_model.predict(sm.add_constant(X_test))\n",
    "eps = 1e-8\n",
    "nonconformity_scores = - (y_train * np.log(p_train + eps) + (1 - y_train) * np.log(1 - p_train + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conformal_intervals(alpha_base=0.1, lambda_factor=0.2):\n",
    "    \"\"\"\n",
    "    Plots logistic regression predictions with interactive adaptive conformal intervals.\n",
    "    \"\"\"\n",
    "    alpha_dynamic = alpha_base + lambda_factor * (0.5 - np.abs(p_test - 0.5))\n",
    "    \n",
    "    q_alpha_dynamic = np.array([\n",
    "        np.percentile(nonconformity_scores, (1 - alpha) * 100) for alpha in alpha_dynamic\n",
    "    ])\n",
    "\n",
    "    lower_bound = p_test - q_alpha_dynamic\n",
    "    upper_bound = p_test + q_alpha_dynamic\n",
    "    lower_bound = np.clip(lower_bound, 0, 1)\n",
    "    upper_bound = np.clip(upper_bound, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(range(len(p_test[:100])), p_test[:100], label=\"Logistic Predictions\", color='blue', alpha=0.6)\n",
    "    plt.fill_between(range(len(p_test[:100])), lower_bound[:100], upper_bound[:100], color='gray', alpha=0.3, label=\"Adaptive Conformal Interval\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Predicted Probability\")\n",
    "    plt.title(f\"Logistic Regression Predictions with Adaptive Conformal Intervals\\nBase Alpha: {alpha}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_conformal_intervals, \n",
    "         alpha_base=widgets.FloatSlider(min=0.01, max=0.5, step=0.01, value=0.4, description=\"Base Alpha\"),\n",
    "         lambda_factor=widgets.FloatSlider(min=0, max=0.5, step=0.01, value=0.2, description=\"Lambda Factor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math189",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
