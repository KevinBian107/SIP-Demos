{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d499b8cf",
   "metadata": {},
   "source": [
    "# Math 189 Week 4 Summary\n",
    "> NAME: $\\color{blue}{\\text{Kaiwen Bian}}$\n",
    "> \n",
    "> PID: $\\color{blue}{\\text{A17316568}}$\n",
    ">\n",
    "> \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce0cf8",
   "metadata": {},
   "source": [
    "I certify that the following write-up is my own work, and have abided by the UCSD Academic Integrity Guidelines.\n",
    "\n",
    "- [x] Yes\n",
    "- [ ] No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c41b1",
   "metadata": {},
   "source": [
    "% # %load tex-macros\n",
    "<div hidden>\n",
    "\\newcommand{\\require}[1]{}\n",
    "\n",
    "$\\require{begingroup}\\require{newcommand}$\n",
    "$\\long\\def \\forcecommand #1{\\providecommand{#1}{}\\renewcommand{#1}}$\n",
    "$\\forcecommand{\\defeq}{\\stackrel{\\small\\bullet}{=}}$\n",
    "$\\forcecommand{\\ra}{\\rangle}$\n",
    "$\\forcecommand{\\la}{\\langle}$\n",
    "$\\forcecommand{\\pr}{{\\mathbb P}}$\n",
    "$\\forcecommand{\\qr}{{\\mathbb Q}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\av}{{\\boldsymbol{a}}}$\n",
    "$\\forcecommand{\\bv}{{\\boldsymbol{b}}}$\n",
    "$\\forcecommand{\\cv}{{\\boldsymbol{c}}}$\n",
    "$\\forcecommand{\\dv}{{\\boldsymbol{d}}}$\n",
    "$\\forcecommand{\\ev}{{\\boldsymbol{e}}}$\n",
    "$\\forcecommand{\\fv}{{\\boldsymbol{f}}}$\n",
    "$\\forcecommand{\\gv}{{\\boldsymbol{g}}}$\n",
    "$\\forcecommand{\\hv}{{\\boldsymbol{h}}}$\n",
    "$\\forcecommand{\\nv}{{\\boldsymbol{n}}}$\n",
    "$\\forcecommand{\\sv}{{\\boldsymbol{s}}}$\n",
    "$\\forcecommand{\\tv}{{\\boldsymbol{t}}}$\n",
    "$\\forcecommand{\\uv}{{\\boldsymbol{u}}}$\n",
    "$\\forcecommand{\\vv}{{\\boldsymbol{v}}}$\n",
    "$\\forcecommand{\\wv}{{\\boldsymbol{w}}}$\n",
    "$\\forcecommand{\\zerov}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\onev}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\phiv}{{\\boldsymbol{\\phi}}}$\n",
    "$\\forcecommand{\\cc}{{\\check{C}}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\Xv}{{\\boldsymbol{X}\\!}}$\n",
    "$\\forcecommand{\\yv}{{\\boldsymbol{y}}}$\n",
    "$\\forcecommand{\\Yv}{{\\boldsymbol{Y}}}$\n",
    "$\\forcecommand{\\zv}{{\\boldsymbol{z}}}$\n",
    "$\\forcecommand{\\Zv}{{\\boldsymbol{Z}}}$\n",
    "$\\forcecommand{\\Iv}{{\\boldsymbol{I}}}$\n",
    "$\\forcecommand{\\Jv}{{\\boldsymbol{J}}}$\n",
    "$\\forcecommand{\\Cv}{{\\boldsymbol{C}}}$\n",
    "$\\forcecommand{\\Ev}{{\\boldsymbol{E}}}$\n",
    "$\\forcecommand{\\Fv}{{\\boldsymbol{F}}}$\n",
    "$\\forcecommand{\\Gv}{{\\boldsymbol{G}}}$\n",
    "$\\forcecommand{\\Hv}{{\\boldsymbol{H}}}$\n",
    "$\\forcecommand{\\alphav}{{\\boldsymbol{\\alpha}}}$\n",
    "$\\forcecommand{\\epsilonv}{{\\boldsymbol{\\epsilon}}}$\n",
    "$\\forcecommand{\\betav}{{\\boldsymbol{\\beta}}}$\n",
    "$\\forcecommand{\\deltav}{{\\boldsymbol{\\delta}}}$\n",
    "$\\forcecommand{\\gammav}{{\\boldsymbol{\\gamma}}}$\n",
    "$\\forcecommand{\\etav}{{\\boldsymbol{\\eta}}}$\n",
    "$\\forcecommand{\\piv}{{\\boldsymbol{\\pi}}}$\n",
    "$\\forcecommand{\\thetav}{{\\boldsymbol{\\theta}}}$\n",
    "$\\forcecommand{\\tauv}{{\\boldsymbol{\\tau}}}$\n",
    "$\\forcecommand{\\muv}{{\\boldsymbol{\\mu}}}$\n",
    "$%$\n",
    "$\\forcecommand{\\sd}{\\text{SD}}$\n",
    "$\\forcecommand{\\se}{\\text{SE}}$\n",
    "$\\forcecommand{\\med}{\\text{median}}$\n",
    "$\\forcecommand{\\median}{\\text{median}}$\n",
    "$%$\n",
    "$\\forcecommand{\\supp}{\\text{supp}}$\n",
    "$\\forcecommand{\\E}{\\mathbb{E}}$\n",
    "$\\forcecommand{\\var}{\\text{Var}}$\n",
    "$\\forcecommand{\\Ber}{{\\text{Ber}}}$\n",
    "$\\forcecommand{\\Bin}{{\\text{Bin}}}$\n",
    "$\\forcecommand{\\Geo}{{\\text{Geo}}}$\n",
    "$\\forcecommand{\\Unif}{{\\text{Unif}}}$\n",
    "$\\forcecommand{\\Poi}{{\\text{Poi}}}$\n",
    "$\\forcecommand{\\Exp}{{\\text{Exp}}}$\n",
    "$\\forcecommand{\\Chisq}{{\\chi^2}}$\n",
    "$\\forcecommand{\\N}{\\mathbb{N}}$\n",
    "$\\forcecommand{\\iid}{{\\stackrel{iid}{\\sim}}}$\n",
    "$\\forcecommand{\\px}{p_{X}}$\n",
    "$\\forcecommand{\\fx}{f_{X}}$\n",
    "$\\forcecommand{\\Fx}{F_{X}}$\n",
    "$\\forcecommand{\\py}{p_{Y}}$\n",
    "$\\forcecommand{\\pxy}{p_{X,Y}}$\n",
    "$\\forcecommand{\\po}{{p_0}}$\n",
    "$\\forcecommand{\\pa}{{p_a}}$\n",
    "$\\forcecommand{\\Xbar}{\\overline{X}}$\n",
    "$\\forcecommand{\\Ybar}{\\overline{Y}}$\n",
    "$\\forcecommand{\\Zbar}{\\overline{Z}}$\n",
    "$\\forcecommand{\\nXbar}{n \\cdot \\overline{X}}$\n",
    "$\\forcecommand{\\nYbar}{n \\cdot \\overline{Y}}$\n",
    "$\\forcecommand{\\nZbar}{n \\cdot \\overline{Z}}$\n",
    "$\\forcecommand{\\Xn}{X_1, X_2, \\dots, X_n}$\n",
    "$\\forcecommand{\\Xm}{{X_1, X_2, \\dots, X_m}}$\n",
    "$\\forcecommand{\\Yn}{Y_1, Y_2, \\dots, Y_n}$\n",
    "$\\forcecommand{\\Ym}{{Y_1, Y_2, \\dots, Y_m}}$\n",
    "$\\forcecommand{\\sumXn}{X_1 + X_2 + \\dots + X_n}$\n",
    "$\\forcecommand{\\sumym}{Y_1 + Y_2 + \\dots + Y_m}$\n",
    "$\\forcecommand{\\la}{\\ell_\\alpha}$\n",
    "$\\forcecommand{\\ua}{u_\\alpha}$\n",
    "$\\forcecommand{\\at}{{\\alpha/2}}$\n",
    "$\\forcecommand{\\mux}{\\mu_{X}}$\n",
    "$\\forcecommand{\\muy}{\\mu_{Y}}$\n",
    "$\\forcecommand{\\sx}{\\sigma_{X}}$\n",
    "$\\forcecommand{\\sy}{\\sigma_{Y}}$\n",
    "$\\forcecommand{\\ci}{\\text{CI}}$\n",
    "$\\forcecommand{\\pvalue}{$p$-value}$\n",
    "$\\forcecommand{\\Ho}{H_{0}}$\n",
    "$\\forcecommand{\\Ha}{H_{a}}$\n",
    "\n",
    "\\vskip-\\parskip\n",
    "\\vskip-\\baselineskip\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb0120db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "matplotlib.rcParams['figure.figsize'] = 7, 7\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3e8eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def plot_X(X, ax, type='pmf', **kwargs):\n",
    "    ax.set_xlabel('Support')\n",
    "    ax.set_title(f'{X.dist.name}{X.args}')\n",
    "    \n",
    "    min_X, max_X = X.ppf((1e-3, 1-1e-3))\n",
    "    supp_X = np.linspace(min_X-1, max_X + 1, 200)\n",
    "    \n",
    "    if type == 'pmf':\n",
    "        supp_X = np.arange(min_X-1, max_X + 1)\n",
    "        ax.bar(supp_X, X.pmf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PMF')\n",
    "    elif type == 'pdf':\n",
    "        ax.plot(supp_X, X.pdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PDF')\n",
    "    elif type == 'cdf':\n",
    "        ax.plot(supp_X, X.cdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('CDF')\n",
    "    else:\n",
    "        raise ValueError('type must be pmf or cdf')\n",
    "\n",
    "def decision(pvalue, alpha):\n",
    "    if pvalue < alpha:\n",
    "        print(f'reject H0: pvalue={pvalue} < {alpha}')  \n",
    "    else: \n",
    "        print(f'fail to reject H0: pvalue={pvalue} â‰¥ {alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfec7ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6dc53",
   "metadata": {},
   "source": [
    "# Key Takeaways from Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc992f31",
   "metadata": {},
   "source": [
    "#### Tuesday: \n",
    "\n",
    "We discussed key properties of goodness of fit test, which is an instance of using categorical random variable to conduct hypothesis testing. We discussed that many categorical distribution may form a multinomial distribution and we can design test statistics precisely such that we can use our observation data to validate it.Thenw e discussed partially about test for independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69de7b",
   "metadata": {},
   "source": [
    "#### Thursday:\n",
    "\n",
    "We finished the discussion about test for independence and extended similar logics into homogenous test (for subgroups). We then move on from the assumption of using specific type of test statistics to use something more general with MLE and likelihood ratio test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaadbca8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d864d",
   "metadata": {},
   "source": [
    "# Categorical Random Variable Inference ðŸ”¢\n",
    "\n",
    "We develop away from just looking at the confidence interval of test statistics of continuous random variable. We will take a look into the `categorical` random variablesa nds ee what we can tell much about them.\n",
    "\n",
    "> Key Idea:\n",
    "> - Statistics is a domain of **testing matcheness of theory**. Essentially we are testing if what we are seeing in **observation** matches teh theoritical **expectation** derived from probability theory.\n",
    "> - The key intuition is that the **test statistics** is designed very interestingly such that is would approach to an theoritical distribution in some sense, then we can compare the empirical distribution and the theoritical distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443cd7c",
   "metadata": {},
   "source": [
    "# Goodness of Fit Test\n",
    "\n",
    "Suppose we have a categorical variable $X$ with $k$ categories. We want to test if **the observed frequencies of the categories are consistent with some expected frequencies**.\n",
    "\n",
    "| Category | Observed Frequency | Expected Frequency |\n",
    "|:--------|:------------------:|:------------------:|\n",
    "| 1       | $O_1$             | $E_1$             |\n",
    "| 2       | $O_2$             | $E_2$             |\n",
    "| $\\vdots$ | $\\vdots$         | $\\vdots$          |\n",
    "| $k$     | $O_k$             | $E_k$             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df37832",
   "metadata": {},
   "source": [
    "### Categorical distribution\n",
    "\n",
    "Categorical random variable is a **generalization** to $k$ different random variable.\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Categorical}(\\pi)\n",
    "$$\n",
    "\n",
    "has a categorical distribution with parameters\n",
    "\n",
    "$$\n",
    "\\pi = (p_1, p_2, \\dots, p_k)\n",
    "$$\n",
    "\n",
    "and that \n",
    "\n",
    "$$\n",
    "P(X = i) = p_i \\quad \\text{for } i = 1, 2, \\dots, k\n",
    "$$\n",
    "\n",
    "Notice that each $p_i$ need to be seperately defined (it is a parameter) and the parameters satisfy the following conditions:\n",
    "\n",
    "$$\n",
    "0 \\leq p_i \\leq 1 \\quad \\text{for } i = 1, 2, \\dots, k\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k} p_i = 1\n",
    "$$\n",
    "\n",
    "In addition, the sum of **random variable** is another **random vector**.\n",
    "- sum of berunulli r.v. is binomial r.v.\n",
    "- sum of categorical r.v. is multinomial r.v."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021d2b6",
   "metadata": {},
   "source": [
    "### Multinomial distribution\n",
    "\n",
    "A random vector that is composed of $n$ observations from the categorical random varaibles.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = (X_1, X_2, \\dots, X_k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\sim \\text{Multinomial}(n, \\pi).\n",
    "$$\n",
    "\n",
    "follows a **multinomial distribution** with parameters \n",
    "\n",
    "$$\n",
    "n \\quad \\text{and} \\quad \\pi = (p_1, p_2, \\dots, p_k),\n",
    "$$\n",
    "\n",
    "The probability mass function (PMF) of the multinomial distribution is given by:\n",
    "\n",
    "$$\n",
    "P(X_1 = O_1, X_2 = O_2, \\dots, X_k = O_k) = \\frac{n!}{O_1! \\times \\dots \\times O_k!} \\times p_1^{O_1} \\times \\dots \\times p_k^{O_k}\n",
    "$$\n",
    "\n",
    "where the **total number of observations** is:\n",
    "\n",
    "$$\n",
    "n = O_1 + O_2 + \\dots + O_k.\n",
    "$$\n",
    "\n",
    "- When $n=1$, then multinomial distribution = categorical distribution.\n",
    "- When $n=2$, the multinumomial distribution = bernulli distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afe601",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(n = (1, 10, 1), p=(0.1, 0.9, 0.1))\n",
    "def binom_multinom_comparison(n, p):\n",
    "    Mult_2 = stats.multinomial(n, [p, 1-p])\n",
    "    Binom = stats.binom(n, p)\n",
    "\n",
    "    x_support = np.arange(n+1)\n",
    "    x_mult_support = np.array([[i, n-i] for i in x_support])\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].bar(x_support, Binom.pmf(x_support), label='Binomial', color='red', alpha=0.5)\n",
    "    axs[1].bar(x_support, Mult_2.pmf(x_mult_support), label='Multinomial', alpha=0.5)\n",
    "    axs[0].set_title('PMF for Binomial'); axs[1].set_title('PMF for Multinomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05677128",
   "metadata": {},
   "source": [
    "## Hypotheses For Goodness of Fit Test\n",
    "\n",
    "We will model the frequency of our observations based on one instance of the **multinomial** distribution.\n",
    "\n",
    "Suppose we have a categorical variable $X$ with $k$ categories. We want to test if the observed frequencies of the categories are consistent with some expected frequencies.\n",
    "\n",
    "| Category | Observed Frequency | Expected Frequency | Observed Proportion | Expected Proportion |\n",
    "|----------|------------------|------------------|-------------------|-------------------|\n",
    "| 1        | $O_1$            | $E_1$            | $\\hat{p}_1 = O_1/n$ | $p_1 = E_1/n$  |\n",
    "| 2        | $O_2$            | $E_2$            | $\\hat{p}_2 = O_2/n$ | $p_2 = E_2/n$  |\n",
    "| $\\vdots$ | $\\vdots$         | $\\vdots$         | $\\vdots$         | $\\vdots$         |\n",
    "| $k$      | $O_k$            | $E_k$            | $\\hat{p}_k = O_k/n$ | $p_k = E_k/n$  |\n",
    "\n",
    "We try to test if the 2 proportions are equal to each other:\n",
    "- $H_0$: The observed proportions are consistent with the expected proportions.\n",
    "- $H_a$: The observed proportions are not consistent with the expected proportions.\n",
    "\n",
    "Let's use test statistics as **deviation measures** of how far the **observation** is from the **theoricical expectations**:\n",
    "- Notice that we only detect if there is a deviation, but not where the deviation is at in the observations.\n",
    "\n",
    "$$\n",
    "T = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= n \\times \\sum_{i=1}^{k} \\frac{(\\hat{p}_i - p_i)^2}{p_i} \\sim \\chi^2_{k-1}\n",
    "$$\n",
    "\n",
    "We will lose **one degree of freedom** since there is the **constraint** that they need to sum up to one. Rejection Region would be\n",
    "\n",
    "$$\n",
    "R(\\chi^2, \\alpha) = (\\chi^2_{\\alpha}, \\infty)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (\\chi^2_{k-1, \\alpha}, \\infty)\n",
    "$$\n",
    "\n",
    "where $x_{\\alpha} = \\chi^2_{k-1, \\alpha}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a508e7",
   "metadata": {},
   "source": [
    "| Anatomy of the hypothesis test | Answer |\n",
    "|:------------------------------:|:--------:|\n",
    "| **Assumption** | $(X_1, X_2, \\dots, X_k) \\sim \\text{Multinomial}(n, p_1, p_2, \\dots, p_k)$ |\n",
    "| **Population parameter** | $\\theta = (p_1, p_2, \\dots, p_k)$ |\n",
    "| **Sample statistic** | $\\hat{\\theta} = (\\hat{p}_1, \\hat{p}_2, \\dots, \\hat{p}_k)$ |\n",
    "| **Test statistic** | $$T = n \\times \\sum_{i=1}^{k} \\frac{(\\hat{p}_i - p_i)^2}{p_i} \\sim \\chi^2_{k-1}$$ |\n",
    "| **Null hypothesis** | $H_0: \\theta = (p_1, p_2, \\dots, p_k)$ |\n",
    "| **Alternate hypothesis** | $H_a: \\theta \\neq (p_1, p_2, \\dots, p_k)$ |\n",
    "| **Rejection region shape** | $(x_{\\alpha}, \\infty)$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af988ef7",
   "metadata": {},
   "source": [
    "> #### Example Question \n",
    "\n",
    "Are the ethical beliefs of the college students uniformly representative, or are they skewed towards one particular belief?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Political.csv'\n",
    "df = pd.read_csv(url, index_col=0)\n",
    "df.Ethics.value_counts() / 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e21456",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chi-squared goodness of fit statistic\n",
    "n = df.shape[0]\n",
    "k = df['Ethics'].nunique()\n",
    "\n",
    "p_hat = df['Ethics'].value_counts() / n\n",
    "p = np.repeat(1/4, 4) # p = (0.5, 0.5, 0.0, 0.0)\n",
    "\n",
    "t_hat = n * np.sum([\n",
    "    (x[0] - x[1])**2 / x[1] for x in zip(p_hat, p)\n",
    "])\n",
    "t_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb07016",
   "metadata": {},
   "outputs": [],
   "source": [
    "dof = k-1\n",
    "T = stats.chi2(dof)\n",
    "p_value = 1 - T.cdf(t_hat)\n",
    "\n",
    "alpha = 0.01\n",
    "decision(p_value, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cefd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pvalue = stats.chisquare(f_obs=df['Ethics'].value_counts())\n",
    "decision(p_value, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1c9be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59464ccd",
   "metadata": {},
   "source": [
    "# Test For Independence\n",
    "The test for independence is to see if 2 **observation distribution are jointed** from any point of perspective.\n",
    "\n",
    "Suppose we have two categorical variables $X$ and $Y$ with $k$ and $m$ categories, respectively. We want to test if the observed frequencies of the categories are independent of each other.\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Categorical}(\\pi_X)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y \\sim \\text{Categorical}(\\pi_Y)\n",
    "$$\n",
    "\n",
    "The joint distribution of $X$ and $Y$ is the product of the marginal distributions, given by:\n",
    "\n",
    "$$\n",
    "P(X = i, Y = j) = \\pi_{ij}, \\quad \\text{for } i = 1, 2, \\dots, k \\text{ and } j = 1, 2, \\dots, m.\n",
    "$$\n",
    "\n",
    "and the parameters satisfy the following conditions:\n",
    "\n",
    "$$\n",
    "0 \\leq \\pi_{ij} \\leq 1, \\quad \\text{for } i = 1, 2, \\dots, k \\text{ and } j = 1, 2, \\dots, m\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{k} \\sum_{j=1}^{m} \\pi_{ij} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08d9a1",
   "metadata": {},
   "source": [
    "This table is known as a **contingency table**.\n",
    "\n",
    "|             | Y Category 1 | Y Category 2 | $\\dots$ | Y Category $m$ |\n",
    "|------------|-------------|-------------|---------|--------------|\n",
    "| X Category 1 | $O_{11}$ | $O_{12}$ | $\\dots$ | $O_{1m}$ |\n",
    "| X Category 2 | $O_{21}$ | $O_{22}$ | $\\dots$ | $O_{2m}$ |\n",
    "| $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\ddots$ | $\\vdots$ |\n",
    "| X Category $k$ | $O_{k1}$ | $O_{k2}$ | $\\dots$ | $O_{km}$ |\n",
    "\n",
    "Here, $O_{ij}$ is the observed frequency of the $i$-th category of $X$ and the $j$-th category of $Y$. The total number of observations is:\n",
    "\n",
    "$$\n",
    "n = O_{11} + O_{12} + \\dots + O_{1m} + O_{21} + \\dots + O_{km}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70087b0e",
   "metadata": {},
   "source": [
    ">  If $X$ and $Y$ are independent:\n",
    "\n",
    "If $X$ and $Y$ are independent, then the joint distribution of $X$ and $Y$ is the product of the marginal distributions, i.e., \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pr(X = i, Y = j) &= \\pr(X = i) \\times \\pr(Y = j)\\\\ &= \\piv_{X, i} \\times \\piv_{Y, j}\\quad \\text{i.e.,}\\\\ \\\\\n",
    "\\quad p_{ij} &= \\piv_{X, i} \\times \\piv_{Y, j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "then  effectively you're saying that $k \\times m$ probability parameters can be faithfully encoded in just $k+m$ different probability parameters\n",
    "\n",
    "where $\\piv_{X, i}$ and $\\piv_{Y, j}$ are the marginal probabilities of $X$ and $Y$, respectively.\n",
    "\n",
    "> If $X$ and $Y$ are not independent:\n",
    "\n",
    "If $X$ and $Y$ are not independent, then the joint distribution of $X$ and $Y$ is not the product of the marginal distributions, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_{ij} &\\neq \\piv_{X, i} \\times \\piv_{Y, j}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279878e",
   "metadata": {},
   "source": [
    "The dataset in terms of the parameters and statistics:\n",
    "\n",
    "|               | Y Category 1        | Y Category 2        | $\\dots$ | Y Category $m$       |\n",
    "|--------------|-------------------|-------------------|---------|-------------------|\n",
    "| X Category 1 | $\\hat{p}_{11} = O_{11}/n$ | $\\hat{p}_{12} = O_{12}/n$ | $\\dots$ | $\\hat{p}_{1m} = O_{1m}/n$ |\n",
    "| X Category 2 | $\\hat{p}_{21} = O_{21}/n$ | $\\hat{p}_{22} = O_{22}/n$ | $\\dots$ | $\\hat{p}_{2m} = O_{2m}/n$ |\n",
    "| $\\vdots$    | $\\vdots$          | $\\vdots$          | $\\ddots$ | $\\vdots$          |\n",
    "| X Category $k$ | $\\hat{p}_{k1} = O_{k1}/n$ | $\\hat{p}_{k2} = O_{k2}/n$ | $\\dots$ | $\\hat{p}_{km} = O_{km}/n$ |\n",
    "\n",
    "The **marginal probabilities** for this observation would be calculated by:\n",
    "\n",
    "$$\n",
    "\\hat{\\pi}_{X,i} = \\sum_{j=1}^{m} \\hat{p}_{ij} \\quad \\text{for } i = 1, 2, \\dots, k\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\pi}_{Y,j} = \\sum_{i=1}^{k} \\hat{p}_{ij} \\quad \\text{for } j = 1, 2, \\dots, m\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37671a67",
   "metadata": {},
   "source": [
    "| Anatomy of the hypothesis test |  Answer  |\n",
    "|:------------------------------:|:--------:|\n",
    "| Assumption                     | $$(X, Y) \\sim \\text{Multinomial}(n, p_{11}, p_{12}, \\dots, p_{km})$$ |\n",
    "| Population parameter           | $$\\theta = (p_{11}, p_{12}, \\dots, p_{km}, \\piv_X, \\piv_Y)$$ |\n",
    "| Sample statistic               | $$\\hat\\theta = (\\hat p_{11}, \\hat p_{12}, \\dots, \\hat p_{km}, \\hat \\piv_X, \\hat \\piv_Y)$$ |\n",
    "| Test statistic                 | $$\\displaystyle T = n \\times  \\sum_{i=1}^k \\sum_{j=1}^m \\frac{(\\hat p_{ij} - \\hat\\piv_{X, i} \\cdot \\hat\\piv_{Y, j})^2}{\\hat\\piv_{X, i} \\cdot \\hat\\piv_{Y, j}} \\sim \\Chisq_{(k-1) \\times (m-1)}$$ |\n",
    "| Null hypothesis                | $$H_0: p_{ij} = \\piv_{X, i} \\times \\piv_{Y, j}$$ for all $i$ and $j$ |\n",
    "| Alternate hypothesis           | $$H_a: p_{ij} \\neq \\piv_{X, i} \\times \\piv_{Y, j}$$ for some $i$ and $j$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0715ea",
   "metadata": {},
   "source": [
    "Here is the code to perform the test:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf35a01",
   "metadata": {},
   "source": [
    "> #### Example Questions\n",
    "\n",
    "Let's look at the `mtcars` dataset again. Are the number of cylinders and the number of gears independent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fbd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['cyl'], df['gear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7209a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, k, m = df.shape[0], df['cyl'].nunique(), df['gear'].nunique()\n",
    "\n",
    "p_hat = pd.crosstab(df['cyl'], df['gear']).values / n\n",
    "\n",
    "pi_X = p_hat.sum(axis=0)\n",
    "pi_Y = p_hat.sum(axis=1)\n",
    "\n",
    "t_hat = n * np.sum([[(p_hat[i, j] - pi_X[j] * pi_Y[i]) ** 2 / (pi_X[j] * pi_Y[i]) for j in range(m)] for i in range(k)])\n",
    "t_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = stats.chi2((k-1)*(m-1))\n",
    "p_value = 1 - T.cdf(t_hat)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27d2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.chi2_contingency(pd.crosstab(df['cyl'], df['gear']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd84a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742f035",
   "metadata": {},
   "source": [
    "# Test For Homogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68946a95",
   "metadata": {},
   "source": [
    "Suppose we have $r$ subsets of the population, each of which is categorical and we have $k$ categorical probability from each of the subgroups.\n",
    "\n",
    "$$\n",
    "\\text{Categorical}(\\pi_1) = (p_{11}, p_{12}, \\dots, p_{1k})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Categorical}(\\pi_2) = (p_{21}, p_{22}, \\dots, p_{2k})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Categorical}(\\pi_r) = (p_{r1}, p_{r2}, \\dots, p_{rk})\n",
    "$$\n",
    "\n",
    "We want to test if the observed frequencies of the categories are **consistent** with each other.\n",
    "\n",
    "|        | Category 1 | Category 2 | $\\dots$ | Category $k$ | Total |\n",
    "|--------|------------|------------|---------|--------------|-------|\n",
    "| $X_1$  | $O_{11}$  | $O_{12}$  | $\\dots$ | $O_{1k}$  | $n_1$ |\n",
    "| $X_2$  | $O_{21}$  | $O_{22}$  | $\\dots$ | $O_{2k}$  | $n_2$ |\n",
    "| $\\vdots$  | $\\vdots$  | $\\vdots$  | $\\ddots$ | $\\vdots$  | $\\vdots$ |\n",
    "| $X_r$  | $O_{r1}$  | $O_{r2}$  | $\\dots$ | $O_{rk}$  | $n_r$ |\n",
    "| **Total** | $T_1$ | $T_2$ | $\\dots$ | $T_k$ | $n$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d62418",
   "metadata": {},
   "source": [
    "**Sample Statistic:**\n",
    "\n",
    "|        | Category 1                   | Category 2                   | $\\dots$ | Category $k$                   | Total |\n",
    "|--------|-----------------------------|-----------------------------|---------|-----------------------------|-------|\n",
    "| $X_1$  | $\\hat{p}_{11} = O_{11}/n_1$ | $\\hat{p}_{12} = O_{12}/n_1$ | $\\dots$ | $\\hat{p}_{1k} = O_{1k}/n_1$ | $n_1$ |\n",
    "| $X_2$  | $\\hat{p}_{21} = O_{21}/n_2$ | $\\hat{p}_{22} = O_{22}/n_2$ | $\\dots$ | $\\hat{p}_{2k} = O_{2k}/n_2$ | $n_2$ |\n",
    "| $\\vdots$  | $\\vdots$                   | $\\vdots$                   | $\\ddots$ | $\\vdots$                   | $\\vdots$ |\n",
    "| $X_r$  | $\\hat{p}_{r1} = O_{r1}/n_r$ | $\\hat{p}_{r2} = O_{r2}/n_r$ | $\\dots$ | $\\hat{p}_{rk} = O_{rk}/n_r$ | $n_r$ |\n",
    "| **Total** | $\\hat{p}_1 = T_1/n$ | $\\hat{p}_2 = T_2/n$ | $\\dots$ | $\\hat{p}_k = T_k/n$ | $n$ |\n",
    "\n",
    "\n",
    "**Assumption:**\n",
    "\n",
    "The individual sub-populations:\n",
    "* $X_1 \\sim \\text{Multinomial}(n_1, p_{11}, p_{12}, \\dots, p_{1k})$\n",
    "* $X_2 \\sim \\text{Multinomial}(n_1, p_{21}, p_{22}, \\dots, p_{2k})$\n",
    "* $\\dots$\n",
    "* $X_r \\sim \\text{Multinomial}(n_1, p_{r1}, p_{r2}, \\dots, p_{rk})$\n",
    "\n",
    "The reference population:\n",
    "* $Z \\sim \\text{Multinomial}(n, p_{1}, p_{2}, \\dots, p_{k})$\n",
    "\n",
    "\n",
    "**Population Parameters:**\n",
    "\n",
    "$\\theta = (p_{11}, p_{12}, \\dots, p_{1k}, \\quad \\quad p_{r1}, p_{r2}, \\dots, p_{rk}, \\quad \\quad p_1, p_2, \\dots, p_k)$\n",
    "\n",
    "**Hypotheses:**\n",
    "\n",
    "- $H_0$: The observed proportions are **consistent** with each other.\n",
    "- $H_a$: The observed proportions are **not** consistent with each other.\n",
    "\n",
    "Then the hypotheses become:\n",
    "\n",
    "$$\n",
    "H_0: p_{1i} = p_{2i} = p_i \\quad \\text{for all } i = 1, 2, \\dots, k\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_a: p_{1i} \\neq p_{2i} \\quad \\text{for some } i = 1, 2, \\dots, k\n",
    "$$\n",
    "\n",
    "\n",
    "**Test Statistic**:\n",
    "\n",
    "$$\n",
    "T = \\sum_{i=1}^{r} n_i \\times \\left( \\sum_{j=1}^{k} \\frac{(\\hat{p}_{ij} - \\hat{p}_j)^2}{\\hat{p}_j} \\right) \\sim \\chi^2_{(r-1) \\times (k-1)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\hat{p}_{ij}$ is the **observed proportion** for the $i$-th subgroup in category $j$.\n",
    "- $\\hat{p}_j$ is the **expected proportion** for category $j$.\n",
    "- The test statistic follows a chi-squared distribution with $(r-1) \\times (k-1)$ degrees of freedom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612f00e",
   "metadata": {},
   "source": [
    "| Anatomy of the hypothesis test |  Answer  |\n",
    "|:------------------------------:|:--------:|\n",
    "| **Assumption**                     | The sub-populations $X_1, X_2, \\dots, X_r$ are each drawn from a Multinomial distribution with parameters corresponding to each group, and the reference population $Z$ is drawn from a Multinomial distribution with aggregated parameters. |\n",
    "| **Population parameter**           | $\\theta = (p_{11}, p_{12}, \\dots, p_{1k}, \\quad p_{r1}, p_{r2}, \\dots, p_{rk}, \\quad p_1, p_2, \\dots, p_k)$ |\n",
    "| **Sample statistic**               | $\\hat{p}_{ij} = O_{ij}/n_i$ for each subgroup and category. $\\hat{p}_j = T_j / n$ is the expected proportion for each category. |\n",
    "| **Test statistic**                 | $T = \\sum_{i=1}^{r} n_i \\times \\left( \\sum_{j=1}^{k} \\frac{(\\hat{p}_{ij} - \\hat{p}_j)^2}{\\hat{p}_j} \\right) \\sim \\chi^2_{(r-1) \\times (k-1)}$ |\n",
    "| **Null hypothesis**                | $H_0: p_{1i} = p_{2i} = p_i \\quad \\text{for all } i = 1, 2, \\dots, k$ |\n",
    "| **Alternate hypothesis**           | $H_a: p_{1i} \\neq p_{2i} \\quad \\text{for some } i = 1, 2, \\dots, k$ |\n",
    "| **Rejection region shape**         | The rejection region is in the upper tail of the $\\chi^2$ distribution with $(r-1) \\times (k-1)$ degrees of freedom. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2425b31",
   "metadata": {},
   "source": [
    "Let's have a look at the `adult` dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/adult)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ucimlrepo\n",
    "\n",
    "adult = ucimlrepo.fetch_ucirepo(id=2)\n",
    "df = adult.data.features\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32cee75",
   "metadata": {},
   "source": [
    "Let's look at only the following education levels: \n",
    "\n",
    "* `HS-grad, Bachelors, Masters` and `Doctorate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f74adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[\n",
    "        df['education'].isin(['HS-grad', 'Bachelors', 'Masters', 'Doctorate'])\n",
    "    ]\n",
    "\n",
    "df.value_counts('education')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e5ad9",
   "metadata": {},
   "source": [
    "> #### Question\n",
    "\n",
    "Is there a difference in the distribution of education levels between Males and Females\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab( df['sex'], df['education'])\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15473cf",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca58ffe",
   "metadata": {},
   "source": [
    "# Beyond Means & Standard Deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c50c1",
   "metadata": {},
   "source": [
    "What we have down earlier is that:\n",
    "\n",
    "Suppose we have a collection $X_1, X_2, \\dots, X_n \\sim D(\\theta)$, observed iid from a distribution $D$ with unknown population parameter $\\theta$.\n",
    "\n",
    "So far, we have considered the cases when:\n",
    "- $D(\\theta) = \\mathcal{N}(\\mu, \\sigma^2)$, or\n",
    "- $D(\\theta) = \\text{Ber}(p)$\n",
    "\n",
    "And, we have considered the following types of hypotheses:\n",
    "\n",
    "- $H_0: \\theta = \\theta_0$ vs $H_a: \\theta \\leq \\theta_0$\n",
    "- $H_0: \\theta = \\theta_0$ vs $H_a: \\theta \\neq \\theta_0$\n",
    "\n",
    "We have also looked at their \"two-sample\" variants, i.e.,\n",
    "\n",
    "- $X_1, X_2, \\dots, X_n \\sim D(\\theta_X)$ and $Y_1, Y_2, \\dots, Y_m \\sim D(\\theta_Y)$\n",
    "- $H_0: \\theta_X = \\theta_Y$ vs $H_a: \\theta_X \\leq \\theta_Y$\n",
    "- $H_0: \\theta_X = \\theta_Y$ vs $H_a: \\theta_X \\neq \\theta_Y$\n",
    "\n",
    "In most of these cases, the **statistics** has always been somewhat the **sample mean**, **proportions**, or maybe **differences in means**. Let's look at data with more general assumptions, let's **go beyond population means and deviations**. Maybe we have some very complicated function to fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdccc31",
   "metadata": {},
   "source": [
    "## Likelihoods\n",
    "\n",
    "Suppose you're given an observation $X \\sim \\mathcal{N}(\\theta, 1)$ where the mean $\\theta$ is **unknown**.\n",
    "- With only the specific observation given (an single sample), how can we estimate the most likely $\\theta$ that generates the sample (*What is the most likely value of* $\\theta$ *which generated* $X$*?*)\n",
    "\n",
    "$X$ is a random variable, so we are looking at:\n",
    "\n",
    "$$\n",
    "\\pr_{\\theta} (X= \\text{certain value})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabef29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_theta = 2.3\n",
    "x = stats.norm(true_theta).rvs()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(theta=(-4.0, 4.0, 0.1))\n",
    "def _(theta=0.0):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "    X = stats.norm(theta, 1)\n",
    "    xs = np.arange(-5, 5, 0.1)\n",
    "    ax.plot(xs, X.pdf(xs), color='dodgerblue')\n",
    "    ax.scatter(x, 0, color='red')\n",
    "    ax.vlines([x], 0, X.pdf(x), 'r')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b106c1",
   "metadata": {},
   "source": [
    "Now, suppose you're given three observations $X_1, X_2, X_3 \\sim \\mathcal{N}(\\theta, 1)$ where the mean $\\theta$ is **unknown**.\n",
    "- Now how do I fit an $\\theta$ that gives teh most likely fit of all the observations that we see?\n",
    "- Essentially the **PDF** of a random avraible controls how likely we are able to see the observation (an probabilistic value), w  are trying to find a $\\theta$ that maximizes the height of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e12bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_more = stats.norm(true_theta).rvs(20)\n",
    "\n",
    "@interact(theta=(-3.0, 5.0, 0.1))\n",
    "def _(theta=0.0):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "    X = stats.norm(theta, 1)\n",
    "    xs = np.arange(-5, 5, 0.1)\n",
    "    ax.plot(xs, X.pdf(xs), color='dodgerblue')\n",
    "    ax.scatter(x_more, np.repeat(0, x_more.shape[0]), color='red')\n",
    "    for i in range(x_more.shape[0]):\n",
    "        ax.vlines([x_more[i]], 0, X.pdf(x_more[i]), 'r')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193fff6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d967582",
   "metadata": {},
   "source": [
    "## Intuition for Maximum Likelihood Estimation\n",
    "\n",
    "Given a collection $X_1, X_2, \\dots, X_n \\sim D(\\theta)$, where $D(\\theta)$ is a distribution with probability density function $f(x; \\theta)$, i.e.,\n",
    "\n",
    "$$\n",
    "X_i \\sim f(x_i; \\theta)\n",
    "$$\n",
    "\n",
    "the **likelihood function** is defined as:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^{n} f(x_i; \\theta)\n",
    "$$\n",
    "\n",
    "> *Intuitively, the likelihood function is a measure of* **how likely the observed data is, given the parameter** $\\theta$.\n",
    "\n",
    "The **Maximum Likelihood Estimator (MLE)** is the value of $\\theta$ that maximizes the likelihood function, i.e.,\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\arg \\max_{\\theta} L(\\theta)\n",
    "$$\n",
    "\n",
    "Equivalently, the **log-likelihood function** is given by:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} \\log f(x_i; \\theta)\n",
    "$$\n",
    "\n",
    "and, the **MLE** is the value of $\\theta$ that maximizes the log-likelihood function, i.e.,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e413b",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Suppose we have a collection $X_1, X_2, \\dots, X_n \\sim \\text{Exp}(\\lambda)$, where $\\text{Exp}(\\lambda)$ is the exponential distribution with probability density function\n",
    "\n",
    "$$\n",
    "f(x; \\lambda) = \\lambda e^{-\\lambda x}, \\quad \\text{for } x \\geq 0, \\lambda > 0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873678f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_true = 2.8\n",
    "X = stats.expon(scale=1/lambda_true)\n",
    "data = X.rvs(3000)\n",
    "sns.histplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee36a2",
   "metadata": {},
   "source": [
    "The likelihood function is\n",
    "\n",
    "$$\n",
    "L(\\lambda) = \\prod_{i=1}^{n} f(x; \\lambda)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\prod_{i=1}^{n} \\lambda e^{-\\lambda x_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\lambda^n \\exp \\left( -\\lambda \\sum_{i=1}^{n} x_i \\right)\n",
    "$$\n",
    "\n",
    "The log-likelihood function is\n",
    "\n",
    "$$\n",
    "\\ell(\\lambda) = \\sum_{i=1}^{n} \\log f(x_i; \\lambda)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{i=1}^{n} \\log(\\lambda) - \\lambda x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "= n \\log \\lambda - \\lambda \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "= n \\log \\lambda - \\lambda n \\cdot \\bar{x}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f2100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood function and log-likelihood function\n",
    "def likelihood(lambda_, data):\n",
    "    return np.prod(\n",
    "        stats.expon(scale=1/lambda_).pdf(data)\n",
    "    )\n",
    "\n",
    "def log_likelihood(lambda_, data):\n",
    "    return np.sum(\n",
    "        stats.expon(scale=1/lambda_).logpdf(data)\n",
    "    )\n",
    "    \n",
    "@interact(lambda_ = (0.1, 5, 0.001))\n",
    "def plot_likelihood(lambda_):\n",
    "    params = np.arange(0.1, 3, 0.01)\n",
    "    likelihoods = [likelihood(lambda_, data) for lambda_ in params]\n",
    "    log_likelihoods = [log_likelihood(lambda_, data) for lambda_ in params]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax[0].plot(params, likelihoods)\n",
    "    ax[0].vlines(lambda_, np.min(likelihoods), np.max(likelihoods), color='r', label='True value')\n",
    "    ax[0].set_title('Likelihood function'); ax[0].set_xlabel('lambda'); ax[0].set_ylabel('likelihood')\n",
    "\n",
    "    ax[1].plot(params, log_likelihoods)\n",
    "    ax[1].vlines(lambda_, np.min(log_likelihoods), np.max(log_likelihoods), color='r', label='True value')\n",
    "    ax[1].set_title('Log-likelihood function'); ax[1].set_xlabel('lambda');ax[1].set_ylabel('log-likelihood')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda9704",
   "metadata": {},
   "source": [
    "> How do you do this automatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "nll_function = lambda x: -log_likelihood(x, data)\n",
    "lambda_init = 5.5\n",
    "res = minimize(\n",
    "    nll_function, \n",
    "    lambda_init, \n",
    "    bounds=[(0, np.inf)]\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd43cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_lambda = res.x[0]\n",
    "mle_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0e177",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe96c24",
   "metadata": {},
   "source": [
    "# Likelihood Ratio Statistic\n",
    "\n",
    "One of the most important procedure in the hsitory of statitsics, most 90th century statistics was devoted to this.\n",
    "\n",
    "- All of the past test we looked at is just an `instance` of  this test.\n",
    "- We try to make this test **agonist of the test statistics**, we want to **move on from using one specific test statistics**.\n",
    "- We **make two hypothesis**, which would let us **fit two different distribution** using MLE, then we consider how different these two distributions are.\n",
    "\n",
    "The (log-)likelihood ratio statistic is defined as:\n",
    "\n",
    "$$\n",
    "\\Lambda = -2 \\log \\frac{L(\\hat{\\theta_0})}{L(\\hat{\\theta})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2 \\times (\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta_0}))\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\hat{\\theta_0}$ is the **Maximum Likelihood Estimator (MLE)** under the **null hypothesis** $H_0$.\n",
    "- $\\hat{\\theta}$ is the **MLE** under the **alternative hypothesis** $H_a$.\n",
    "\n",
    "This statistic is often used in likelihood ratio tests (LRT), which compare the likelihood of the data under two competing hypotheses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80575618",
   "metadata": {},
   "source": [
    "> #### Wilk's Theorem\n",
    ">\n",
    "> Under some mild conditions on $f(x;\\theta)$, the likelihood ratio statistic $\\Lambda$ is $\\Chisq$-distributed with degrees of freedom equal to the difference in the number of parameters under the null and alternative hypotheses.\n",
    "\n",
    "\n",
    "$$\n",
    "\\Lambda \\sim \\Chisq(df) = \\Chisq(dim(\\Theta_1) - dim(\\Theta_0))\n",
    "$$\n",
    "\n",
    "Then we approach this empirical distribution to a tehoritical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5a40e",
   "metadata": {},
   "source": [
    "Given $X_1, X_2, \\dots, X_n \\sim \\Exp(\\lambda)$. Suppose we want to test:\n",
    "\n",
    "$$\n",
    "H_0: \\lambda = 2 \\quad \\quad \\text{vs} \\quad \\quad H_a: \\lambda > 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ea9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_alternative = -minimize(\n",
    "    nll_function, \n",
    "    lambda_init, \n",
    "    bounds=[(2, np.inf)]\n",
    ").fun\n",
    "\n",
    "l_null = -minimize(\n",
    "    nll_function, \n",
    "    lambda_init, \n",
    "    bounds=[(2, 2)]\n",
    ").fun\n",
    "\n",
    "l_alternative, l_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eca625",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "pvalue = 1 - stats.chi2(1).cdf( 2*(l_alternative - l_null)   )\n",
    "decision(pvalue, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f477a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ratio = 2 * (l_alternative - l_null)\n",
    "l_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = 1 - stats.chi2(1).cdf(l_ratio)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c83d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(n=(10, 1000, 1))\n",
    "\n",
    "def lr_test(n):\n",
    "    lambda_true = 2.5\n",
    "    X = stats.expon(scale=1/lambda_true)\n",
    "    data = X.rvs(n)\n",
    "    nll_function = lambda x: -log_likelihood(x, data)\n",
    "    l_alternative = -minimize(nll_function, lambda_init, bounds=[(2, np.inf)]).fun\n",
    "    l_null = -minimize(nll_function, lambda_init, bounds=[(0, 2)]).fun\n",
    "    l_ratio = 2*(l_alternative - l_null)\n",
    "    p_value = 1 - stats.chi2(1).cdf(l_ratio)\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2054701",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
