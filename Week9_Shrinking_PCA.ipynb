{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d499b8cf",
   "metadata": {},
   "source": [
    "# Math 189 Week 9 Summary\n",
    "> NAME: $\\color{blue}{\\text{Kaiwen Bian}}$\n",
    "> \n",
    "> PID: $\\color{blue}{\\text{A17316568}}$\n",
    ">\n",
    "> \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce0cf8",
   "metadata": {},
   "source": [
    "I certify that the following write-up is my own work, and have abided by the UCSD Academic Integrity Guidelines.\n",
    "\n",
    "- [x] Yes\n",
    "- [ ] No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c41b1",
   "metadata": {},
   "source": [
    "% # %load tex-macros\n",
    "<div hidden>\n",
    "\\newcommand{\\require}[1]{}\n",
    "\n",
    "$\\require{begingroup}\\require{newcommand}$\n",
    "$\\long\\def \\forcecommand #1{\\providecommand{#1}{}\\renewcommand{#1}}$\n",
    "$\\forcecommand{\\defeq}{\\stackrel{\\small\\bullet}{=}}$\n",
    "$\\forcecommand{\\ra}{\\rangle}$\n",
    "$\\forcecommand{\\la}{\\langle}$\n",
    "$\\forcecommand{\\pr}{{\\mathbb P}}$\n",
    "$\\forcecommand{\\qr}{{\\mathbb Q}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\av}{{\\boldsymbol{a}}}$\n",
    "$\\forcecommand{\\bv}{{\\boldsymbol{b}}}$\n",
    "$\\forcecommand{\\cv}{{\\boldsymbol{c}}}$\n",
    "$\\forcecommand{\\dv}{{\\boldsymbol{d}}}$\n",
    "$\\forcecommand{\\ev}{{\\boldsymbol{e}}}$\n",
    "$\\forcecommand{\\fv}{{\\boldsymbol{f}}}$\n",
    "$\\forcecommand{\\gv}{{\\boldsymbol{g}}}$\n",
    "$\\forcecommand{\\hv}{{\\boldsymbol{h}}}$\n",
    "$\\forcecommand{\\nv}{{\\boldsymbol{n}}}$\n",
    "$\\forcecommand{\\sv}{{\\boldsymbol{s}}}$\n",
    "$\\forcecommand{\\tv}{{\\boldsymbol{t}}}$\n",
    "$\\forcecommand{\\uv}{{\\boldsymbol{u}}}$\n",
    "$\\forcecommand{\\vv}{{\\boldsymbol{v}}}$\n",
    "$\\forcecommand{\\wv}{{\\boldsymbol{w}}}$\n",
    "$\\forcecommand{\\zerov}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\onev}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\phiv}{{\\boldsymbol{\\phi}}}$\n",
    "$\\forcecommand{\\cc}{{\\check{C}}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\Xv}{{\\boldsymbol{X}\\!}}$\n",
    "$\\forcecommand{\\yv}{{\\boldsymbol{y}}}$\n",
    "$\\forcecommand{\\Yv}{{\\boldsymbol{Y}}}$\n",
    "$\\forcecommand{\\zv}{{\\boldsymbol{z}}}$\n",
    "$\\forcecommand{\\Zv}{{\\boldsymbol{Z}}}$\n",
    "$\\forcecommand{\\Iv}{{\\boldsymbol{I}}}$\n",
    "$\\forcecommand{\\Jv}{{\\boldsymbol{J}}}$\n",
    "$\\forcecommand{\\Cv}{{\\boldsymbol{C}}}$\n",
    "$\\forcecommand{\\Ev}{{\\boldsymbol{E}}}$\n",
    "$\\forcecommand{\\Fv}{{\\boldsymbol{F}}}$\n",
    "$\\forcecommand{\\Gv}{{\\boldsymbol{G}}}$\n",
    "$\\forcecommand{\\Hv}{{\\boldsymbol{H}}}$\n",
    "$\\forcecommand{\\alphav}{{\\boldsymbol{\\alpha}}}$\n",
    "$\\forcecommand{\\epsilonv}{{\\boldsymbol{\\epsilon}}}$\n",
    "$\\forcecommand{\\betav}{{\\boldsymbol{\\beta}}}$\n",
    "$\\forcecommand{\\deltav}{{\\boldsymbol{\\delta}}}$\n",
    "$\\forcecommand{\\gammav}{{\\boldsymbol{\\gamma}}}$\n",
    "$\\forcecommand{\\etav}{{\\boldsymbol{\\eta}}}$\n",
    "$\\forcecommand{\\piv}{{\\boldsymbol{\\pi}}}$\n",
    "$\\forcecommand{\\thetav}{{\\boldsymbol{\\theta}}}$\n",
    "$\\forcecommand{\\tauv}{{\\boldsymbol{\\tau}}}$\n",
    "$\\forcecommand{\\muv}{{\\boldsymbol{\\mu}}}$\n",
    "$%$\n",
    "$\\forcecommand{\\sd}{\\text{SD}}$\n",
    "$\\forcecommand{\\se}{\\text{SE}}$\n",
    "$\\forcecommand{\\med}{\\text{median}}$\n",
    "$\\forcecommand{\\median}{\\text{median}}$\n",
    "$%$\n",
    "$\\forcecommand{\\supp}{\\text{supp}}$\n",
    "$\\forcecommand{\\E}{\\mathbb{E}}$\n",
    "$\\forcecommand{\\var}{\\text{Var}}$\n",
    "$\\forcecommand{\\Ber}{{\\text{Ber}}}$\n",
    "$\\forcecommand{\\Bin}{{\\text{Bin}}}$\n",
    "$\\forcecommand{\\Geo}{{\\text{Geo}}}$\n",
    "$\\forcecommand{\\Unif}{{\\text{Unif}}}$\n",
    "$\\forcecommand{\\Poi}{{\\text{Poi}}}$\n",
    "$\\forcecommand{\\Exp}{{\\text{Exp}}}$\n",
    "$\\forcecommand{\\Chisq}{{\\chi^2}}$\n",
    "$\\forcecommand{\\N}{\\mathbb{N}}$\n",
    "$\\forcecommand{\\iid}{{\\stackrel{iid}{\\sim}}}$\n",
    "$\\forcecommand{\\px}{p_{X}}$\n",
    "$\\forcecommand{\\fx}{f_{X}}$\n",
    "$\\forcecommand{\\Fx}{F_{X}}$\n",
    "$\\forcecommand{\\py}{p_{Y}}$\n",
    "$\\forcecommand{\\pxy}{p_{X,Y}}$\n",
    "$\\forcecommand{\\po}{{p_0}}$\n",
    "$\\forcecommand{\\pa}{{p_a}}$\n",
    "$\\forcecommand{\\Xbar}{\\overline{X}}$\n",
    "$\\forcecommand{\\Ybar}{\\overline{Y}}$\n",
    "$\\forcecommand{\\Zbar}{\\overline{Z}}$\n",
    "$\\forcecommand{\\nXbar}{n \\cdot \\overline{X}}$\n",
    "$\\forcecommand{\\nYbar}{n \\cdot \\overline{Y}}$\n",
    "$\\forcecommand{\\nZbar}{n \\cdot \\overline{Z}}$\n",
    "$\\forcecommand{\\Xn}{X_1, X_2, \\dots, X_n}$\n",
    "$\\forcecommand{\\Xm}{{X_1, X_2, \\dots, X_m}}$\n",
    "$\\forcecommand{\\Yn}{Y_1, Y_2, \\dots, Y_n}$\n",
    "$\\forcecommand{\\Ym}{{Y_1, Y_2, \\dots, Y_m}}$\n",
    "$\\forcecommand{\\sumXn}{X_1 + X_2 + \\dots + X_n}$\n",
    "$\\forcecommand{\\sumym}{Y_1 + Y_2 + \\dots + Y_m}$\n",
    "$\\forcecommand{\\la}{\\ell_\\alpha}$\n",
    "$\\forcecommand{\\ua}{u_\\alpha}$\n",
    "$\\forcecommand{\\at}{{\\alpha/2}}$\n",
    "$\\forcecommand{\\mux}{\\mu_{X}}$\n",
    "$\\forcecommand{\\muy}{\\mu_{Y}}$\n",
    "$\\forcecommand{\\sx}{\\sigma_{X}}$\n",
    "$\\forcecommand{\\sy}{\\sigma_{Y}}$\n",
    "$\\forcecommand{\\ci}{\\text{CI}}$\n",
    "$\\forcecommand{\\pvalue}{$p$-value}$\n",
    "$\\forcecommand{\\Ho}{H_{0}}$\n",
    "$\\forcecommand{\\Ha}{H_{a}}$\n",
    "\n",
    "\\vskip-\\parskip\n",
    "\\vskip-\\baselineskip\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9025a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "# pio.renderers.default='notebook'\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# Optional \n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "matplotlib.rcParams['figure.figsize'] = 7, 7\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4b4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def plot_X(X, ax, type='pmf', **kwargs):\n",
    "    ax.set_xlabel('Support')\n",
    "    ax.set_title(f'{X.dist.name}{X.args}')\n",
    "    \n",
    "    min_X, max_X = X.ppf((1e-3, 1-1e-3))\n",
    "    supp_X = np.linspace(min_X-1, max_X + 1, 200)\n",
    "    \n",
    "    if type == 'pmf':\n",
    "        supp_X = np.arange(min_X-1, max_X + 1)\n",
    "        ax.bar(supp_X, X.pmf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PMF')\n",
    "    elif type == 'pdf':\n",
    "        ax.plot(supp_X, X.pdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PDF')\n",
    "    elif type == 'cdf':\n",
    "        ax.plot(supp_X, X.cdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('CDF')\n",
    "    else:\n",
    "        raise ValueError('type must be pmf or cdf')\n",
    "\n",
    "def decision(pvalue, alpha):\n",
    "    if pvalue < alpha:\n",
    "        print(f'reject H0: pvalue={pvalue} < {alpha}')  \n",
    "    else: \n",
    "        print(f'fail to reject H0: pvalue={pvalue} â‰¥ {alpha}')\n",
    "\n",
    "def standardize(X):\n",
    "    return (X - X.mean()) / X.std()\n",
    "\n",
    "\n",
    "def make_data(errors):\n",
    "    n = len(errors)\n",
    "    x1 = np.linspace(0, 1, n)\n",
    "    x2 = np.random.rand(n)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'x1': x1, \n",
    "        'x2': x2, \n",
    "        'y': 2 + 3*x1 + 4*x2 + errors\n",
    "    })\n",
    "\n",
    "def plot_regression(data, fit, residuals=True):\n",
    "    b = fit.params\n",
    "    b0, b1, b2 = *b, *np.zeros(3 - len(b))\n",
    "    y, x1, x2 = data.y, data.x1, data.x2\n",
    "    fig = px.scatter_3d(x=x1, y=x2, z=y)\n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "            xaxis_title='X1',\n",
    "            yaxis_title='X2',\n",
    "            zaxis_title='Y'),\n",
    "            margin=dict(l=0, r=0, b=0, t=0\n",
    "        )\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    \n",
    "    x1_grid, x2_grid = np.meshgrid(x1, x2)\n",
    "    yhat = b0 + (b1 * x1_grid) + (b2 * x2_grid)\n",
    "    fig.add_trace(\n",
    "        go.Surface(x=x1_grid, y=x2_grid, z=yhat, opacity=0.5,colorscale='Gray')\n",
    "    )\n",
    "    if residuals:\n",
    "        for i in range(len(x1)):\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(x=[x1[i], x1[i]], y=[x2[i], x2[i]], z=[b0 + b1*x1[i] + b2*x2[i], y[i]], mode='lines', line=dict(color='black', width=2))\n",
    "            )\n",
    "    fig.update_layout(showlegend=False, scene_camera=dict(eye=dict(x=2.0, y=0.5, z=0.1)))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfec7ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6dc53",
   "metadata": {},
   "source": [
    "## Key Takeaways from Week 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc992f31",
   "metadata": {},
   "source": [
    "#### Tuesday: \n",
    "\n",
    "We finished off the shrinking method and how we can find the optimal alpha value, then we started on the mathematical formulation of PCA and different characteristics of it (i.e. decorrolation ability in multicolinearity cases), as well as how to pick the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69de7b",
   "metadata": {},
   "source": [
    "#### Thursday\n",
    "\n",
    "We continued discuss the characteristics of PCA and then moved on the quantifying uncertainty in the prediction directly, involving confidence interval for  prediction in expectations and prediction interval for single predictions. We then moved on to statsitcial learning theory and try to develop an uncertainty quantification method that is more general for black box models: conformal inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaadbca8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2f50d",
   "metadata": {},
   "source": [
    "# Regularization & Shrinking Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd014f40",
   "metadata": {},
   "source": [
    "Choosing `alpha` via:\n",
    "1. alpha for each covariate graph\n",
    "2. cross validation error comparing with alpha graph (shows bias variance tradeoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d368fda",
   "metadata": {},
   "source": [
    "### Intuition for Ridge and LASSO\n",
    "\n",
    "* In the absence of $p_\\lambda(\\beta_1, \\beta_2, \\dots, \\beta_p)$ the objective is to minimize the residual sum of squares loss function, i.e., \n",
    "\n",
    "$$\n",
    "\\mathop{\\min} L(\\beta_0, \\beta_2, \\dots, \\beta_p)\n",
    "$$\n",
    "\n",
    "\n",
    "* After adding the penalty term, the objective can equivalently be written as\n",
    "\n",
    "$$\n",
    "\\mathop{\\min} L(\\beta_0, \\beta_2, \\dots, \\beta_p)\\\\ \\\\\n",
    "s.t. \\quad p(\\beta_1, \\dots, \\beta_p) \\leq t\n",
    "$$\n",
    "\n",
    "where the value of **$t$ is inversely related to $\\lambda$**\n",
    "- if $t$ is small then $\\lambda$ in $p_\\lambda(\\cdot)$ is large and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07326344",
   "metadata": {},
   "source": [
    "Consider the following example:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n",
    "$$\n",
    "\n",
    "Suppose the values $(\\hat\\beta_1, \\hat\\beta_2)$ which minimize the loss function are:\n",
    "\n",
    "$$\n",
    "(\\hat\\beta_1, \\hat\\beta_2) = (1, 1)\n",
    "$$\n",
    "\n",
    "The loss function \n",
    "\n",
    "$$\n",
    "L(\\beta_1, \\beta_2) = \\sum_{i=1}^n (y_i - \\beta_1x_{1i} - \\beta_2 x_{2i})^2\n",
    "$$\n",
    "\n",
    "define ellipsoidal regions in the $\\beta_1, \\beta_2$ plane with the minimum at $(\\hat\\beta_1, \\hat\\beta_2) = (1, 1)$.\n",
    "\n",
    "$$\n",
    "(\\beta_1^2 a) + (\\beta_2^2 c) + (\\beta_1 \\cdot \\beta_2 \\cdot d) + \\dots = 0\n",
    "$$\n",
    "\n",
    "This is a `conic` section equation, which is deternmined by actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9b46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (1, 1)\n",
    "\n",
    "@interact(t=(0.01, 1.5, 0.01), l1wt=(0, 1, 0.1))\n",
    "def plot(t, l1wt):\n",
    "    x1seq = np.linspace(-3, 3, 500)\n",
    "    x2seq = np.linspace(-3, 3, 500)\n",
    "    X1, X2 = np.meshgrid(x1seq, x2seq)\n",
    "    d = np.array([X1.flatten(), X2.flatten()]).T\n",
    "\n",
    "    z = [ l1wt * (np.abs(x[0]) + np.abs(x[1])) + (1-l1wt) * (x[0]**2 + x[1]**2) < t for x in d]\n",
    "    Z = np.array(z).reshape(X1.shape)\n",
    "\n",
    "    y = [(x[0]-b[0])**2 / 3 + (x[1] - b[1])**2 /0.5 + 1 * (x[0] - b[0]) * (x[1] - b[1]) for x in d]\n",
    "    Y = np.array(y).reshape(X1.shape)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.axhline(0, color='black', lw=2)\n",
    "    ax.axvline(0, color='black', lw=2)\n",
    "    ax.contourf(X1, X2, Z, alpha=0.5)\n",
    "    # line contours of X1, X2, Y with colors representing the value of Y on each contour with magma theme\n",
    "    # reverse the color map\n",
    "    ax.contour(X1, X2, Y, levels=np.linspace(0, 2, 10), cmap='inferno_r', linestyles='dashed')\n",
    "    ax.set_xlabel('b1')\n",
    "    ax.set_ylabel('b2')\n",
    "    ax.set_title(f'Contour plot of $\\\\lambda$ = {l1wt: .3f} and $t$ = {t: .3f}')\n",
    "    ax.plot(b[0], b[1], 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854a0c1",
   "metadata": {},
   "source": [
    "Dependending on the specific formulation, regularization is a projection that requires one to **move on the objective/loss surface while living in certain region**\n",
    "\n",
    "- For Ridge, it is a circle and we can find somewhere on the curve\n",
    "\n",
    "- For LASSO, it is a dimond and the only point it would take place would be **on the axis**, forcing all covaraitae but one to be zero -> giving much sparser covariate coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2370c5",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection Via Cross-validation\n",
    "\n",
    "We need a systematic way to select out the good varaibels that we actually need. To this end, we would also need to have a really good `alpha` value  to help us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "num_features = 50\n",
    "\n",
    "# mean and covariance for multivariate normal distribution\n",
    "mean = np.zeros(num_features)\n",
    "cov = np.eye(num_features) * 0.5 + 0.5 * np.random.rand(num_features, num_features)\n",
    "cov = (cov + cov.T) / 2  # Ensure symmetry\n",
    "\n",
    "# generate synthetic high-dimensional data\n",
    "data_high_dim = np.random.multivariate_normal(mean, cov, size=num_samples)\n",
    "\n",
    "true_p = [2, 3, 13, 29, 42]\n",
    "\n",
    "# generate target variable y as a linear combination of selected features with some noise\n",
    "weights = np.random.uniform(0.5, 2.0, len(true_p))  # Random weights for selected features\n",
    "y = np.dot(data_high_dim[:, true_p], weights) + np.random.normal(scale=1.0, size=num_samples)\n",
    "\n",
    "df = pd.DataFrame(data_high_dim, columns=[f'x{i}' for i in range(num_features)])\n",
    "df['y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575c69b",
   "metadata": {},
   "source": [
    "We can do the asiest visualization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ab54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_formula = f\" y ~ {'+'.join(df.columns.drop('y'))}\"\n",
    "\n",
    "elastic_fit = smf.ols(full_model_formula, data=df).\\\n",
    "                fit_regularized(method='elastic_net', alpha=1.0, L1_wt=1.0)\n",
    "\n",
    "l1wt = 1.0\n",
    "alphas = np.linspace(1e-3, 10.5, 200)\n",
    "elastic_coefs = []\n",
    "for alpha in alphas:\n",
    "    elastic_fit = smf.ols(full_model_formula, data=df).\\\n",
    "                fit_regularized(method='elastic_net', alpha=alpha, L1_wt=l1wt)\n",
    "    elastic_coefs.append(elastic_fit.params[1:])\n",
    "\n",
    "\n",
    "# Dataframe of coefficients\n",
    "elastic_coefs = pd.DataFrame(elastic_coefs, index=alphas)\n",
    "elastic_coefs\n",
    "\n",
    "\n",
    "# plot each coefficient\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "for column in elastic_coefs.columns:\n",
    "    ax.plot(alphas, elastic_coefs[column], label=column)\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a242fe",
   "metadata": {},
   "source": [
    "Or `corss validation selection` method and then plotting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d07a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.25, random_state = 0)\n",
    "train.shape, test.shape\n",
    "\n",
    "def cv_error(alpha, l1wt, train, test):\n",
    "    model = smf.ols(full_model_formula, data=train).\\\n",
    "                fit_regularized(method='elastic_net', alpha=alpha, L1_wt=l1wt)\n",
    "    yhat_test = model.predict(test)\n",
    "    error = np.sum((test['y'] - yhat_test) ** 2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92826c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1wt = 1.0\n",
    "alphas = np.linspace(1e-3, 0.5, 200)\n",
    "cv_errors = np.zeros_like(alphas)\n",
    "\n",
    "for (i, alpha) in enumerate(alphas):\n",
    "    cv_errors[i] = cv_error(alpha, l1wt, train, test)\n",
    "\n",
    "plt.plot(alphas, cv_errors)\n",
    "np.argmin(cv_errors), alphas[np.argmin(cv_errors)]\n",
    "print(f'Optimal alpha: {alphas[np.argmin(cv_errors)]}, CV Error: {cv_errors[np.argmin(cv_errors)]}')\n",
    "plt.scatter(alphas[np.argmin(cv_errors)], cv_errors[np.argmin(cv_errors)], color='red')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('CV Error');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4654fabe",
   "metadata": {},
   "source": [
    "Same boston housing dataset, we can see if there are similar patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://vincentarelbundock.github.io/Rdatasets/csv/MASS/Boston.csv\"\n",
    "boston = pd.read_csv(url)\n",
    "df = boston\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5999bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = boston\n",
    "\n",
    "response = 'medv'\n",
    "full_model_formula = f\" {response} ~ {'+'.join(df.columns.drop(response))}\"\n",
    "\n",
    "l1wt = 1.0\n",
    "alphas = np.linspace(1e-3, 0.5, 200)\n",
    "elastic_coefs = []\n",
    "for alpha in alphas:\n",
    "    elastic_fit = smf.ols(full_model_formula, data=df).\\\n",
    "                fit_regularized(method='elastic_net', alpha=alpha, L1_wt=l1wt)\n",
    "    elastic_coefs.append(elastic_fit.params[1:])\n",
    "\n",
    "\n",
    "# Dataframe of coefficients\n",
    "elastic_coefs = pd.DataFrame(elastic_coefs, index=alphas)\n",
    "elastic_coefs\n",
    "\n",
    "# plot each coefficient\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "for column in elastic_coefs.columns:\n",
    "    ax.plot(alphas, elastic_coefs[column], label=column)\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.3, random_state = 0)\n",
    "train.shape, test.shape\n",
    "\n",
    "def cv_error(alpha, l1wt, train, test):\n",
    "    model = smf.ols(full_model_formula, data=train).\\\n",
    "                fit_regularized(method='elastic_net', alpha=alpha, L1_wt=l1wt)\n",
    "    yhat_test = model.predict(test)\n",
    "    error = np.sum((test[response] - yhat_test) ** 2)\n",
    "    return error\n",
    "l1wt = 1.0\n",
    "alphas = np.linspace(1e-3, 0.5, 200)\n",
    "cv_errors = np.zeros_like(alphas)\n",
    "\n",
    "for (i, alpha) in enumerate(alphas):\n",
    "    cv_errors[i] = cv_error(alpha, l1wt, train, test)\n",
    "\n",
    "plt.plot(alphas, cv_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f002aa",
   "metadata": {},
   "source": [
    "### Desiderata\n",
    "\n",
    "Again, we want an ideal penalty function to do the following\n",
    "\n",
    "\n",
    "1. We want $p_\\lambda(\\beta_1, \\beta_2, \\dots, \\beta_p)$ to be be **large** when too many predictors are included in the model. \n",
    "\n",
    "2. We want $p_\\lambda(\\beta_1, \\beta_2, \\dots, \\beta_p)$ to be **small** when only a few predictors are included in the model.\n",
    "\n",
    "3. Ideally, we want $p_\\lambda(\\beta_1, \\beta_2, \\dots, \\beta_p)$ to be **zero** when only the true predictors are included in the model.\n",
    "    * Unfortunately, this ideal is never achieved in practice! \n",
    "\n",
    "> We canpropose a new penalty function:\n",
    ">\n",
    "> $$\n",
    "> P(x) = \\lambda \\frac{x^2}{1 + \\alpha |x|}\n",
    "> $$\n",
    ">\n",
    "> This penalty function incorporates the characteristics of the previous penalty function that we have discussed:\n",
    "> - Quadratic for small $ x $ (L2-like behavior), promoting smoothness.\n",
    "> - Reduces to nearly L1 for large values, encouraging sparsity.\n",
    "> - Can be used for feature selection while ensuring numerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69fb562",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a37685",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "Into unsupervised, given variables $(X_1, X_2, \\dots, X_p)$, PCA produces a low-dimensional representation of the dataset, i.e.,\n",
    "\n",
    "\\begin{aligned}\n",
    "\\text{observation } 1 &: \\quad (X_{1,1}, X_{2,1}, \\dots, X_{p,1}) \\quad \\longrightarrow \\quad (Z_{1,1}, Z_{2,1}) \\\\\n",
    "\\text{observation } 2 &: \\quad (X_{1,2}, X_{2,2}, \\dots, X_{p,2}) \\quad \\longrightarrow \\quad (Z_{1,2}, Z_{2,2}) \\\\\n",
    "&\\quad \\vdots \\hspace{4cm} \\vdots \\\\\n",
    "\\text{observation } n &: \\quad (X_{1,n}, X_{2,n}, \\dots, X_{p,n}) \\quad \\longrightarrow \\quad (Z_{1,n}, Z_{2,n})\n",
    "\\end{aligned}\n",
    "\n",
    "> #### Goal of PCA:\n",
    ">\n",
    "> **Input:**\n",
    ">\n",
    "> Variables $X_1, X_2, \\dots, X_p$\n",
    ">\n",
    "> **Output:**\n",
    ">\n",
    "> A new set of variables $Z_1, Z_2, \\dots, Z_q$ where $q \\ll p$\n",
    ">\n",
    "> **Constraints:**\n",
    ">\n",
    "> 1. $Z_1, Z_2, \\dots, Z_q$ are linear combinations of $X_1, X_2, \\dots, X_p$\n",
    "> 2. $\\text{Var}(Z_1, Z_2, \\dots, Z_q) \\approx \\text{Var}(X_1, X_2, \\dots, X_p)$ (we want variance to be the same)\n",
    "> 3. $Z_1, Z_2, \\dots, Z_q$ are **uncorrelated**\n",
    ">\n",
    "> Where $V = (v_1, v_2, ..., v_p)$ is factor loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0927c8a",
   "metadata": {},
   "source": [
    "## Procedure of PCA\n",
    "\n",
    "This is different from the PCA perspective that we get from DSC 140B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a10ac6",
   "metadata": {},
   "source": [
    "The first **principal component** $Z_1$ is the *normalized* linear combination of the features:\n",
    "\n",
    "$$\n",
    "Z_1 = v_{11} X_1 + v_{21} X_2 + \\dots + v_{p1} X_p\n",
    "$$\n",
    "\n",
    "such that:\n",
    "1. $Z_1$ has the **largest possible variance**\n",
    "2. The weights satisfy the **constraint of unit vector**: $\\sum_{j=1}^{p} v_{j1}^2 = 1$\n",
    "    - We want to avoid too trivial solution where the magnitude of some vector just makes the x xomponent directly zero, so we limit the vectors to a unit circle, they are all unit vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a27b7f",
   "metadata": {},
   "source": [
    "> #### Courant-Fisher-Weyl Min-Max Theorem (Paraphrased)\n",
    ">\n",
    "> Consider the optimization:\n",
    ">\n",
    "> $$\n",
    "> \\max v^T \\cdot \\text{cov}(X) \\cdot v\n",
    "> $$\n",
    ">\n",
    "> subject to:\n",
    ">\n",
    "> $$\n",
    "> v \\cdot v = 1\n",
    "> $$\n",
    ">\n",
    "> - **Max value** = Largest eigenvalue of $X$\n",
    "> - **Maximizer** $v^*$ = Eigenvector corresponding to the largest eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f8596",
   "metadata": {},
   "source": [
    "The **second principal component** $Z_2$ is the _normalized_ linear combination of the features:\n",
    "\n",
    "$$\n",
    "Z_2 = v_{12}X_1 + v_{22}X_2 + \\dots + v_{p2}X_p\n",
    "$$\n",
    "\n",
    "such that:\n",
    "\n",
    "- $V_2 \\perp V_1$ (orthogonal to the first principal component)\n",
    "- $Z_2$ has the largest possible variance\n",
    "- $\\sum_{i=1}^{p} v_{p2}^2 = 1$ (normalization constraint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0255e2a",
   "metadata": {},
   "source": [
    "The **qth principal component** $Z_q$ is the _normalized_ linear combination of the features:\n",
    "\n",
    "$$\n",
    "Z_q = v_{1q}X_1 + v_{2q}X_2 + \\dots + v_{pq}X_p\n",
    "$$\n",
    "\n",
    "such that:\n",
    "\n",
    "- $Z_q$ has the largest possible variance\n",
    "- $V_q \\perp \\text{span}(V_1, V_2, \\dots, V_{q-1})$ (orthogonal to the previous principal components)\n",
    "- $\\sum_{i=1}^{p} v_{p2}^2 = 1$ (normalization constraint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecf2c35",
   "metadata": {},
   "source": [
    "**Eigfenvectors are very useful as they are the direct solution to the optimization probelm that we care about, it links `eigen decomposition/singular value deconposition` to the `optimization problem` we want to solve in PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1d320",
   "metadata": {},
   "source": [
    "PCA was namely originally trying to address multicolinearioty (think back in deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2054701",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55632b42",
   "metadata": {},
   "source": [
    "# Characteristic of PCA\n",
    "\n",
    "We will look at an example to show the characteristics of PCA, let's generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "mean = [0, 0]  # Mean of the distribution\n",
    "cov = [[1, 0.8], [0.8, 1]]  # Covariance matrix (strong correlation)\n",
    "\n",
    "x1, x2 = np.random.multivariate_normal(mean, cov, 1000).T\n",
    "\n",
    "df = pd.DataFrame({'x1': x1, 'x2': x2})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x1'], df['x2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f564d",
   "metadata": {},
   "source": [
    "We can examine the varaince that this dataset has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4317a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(theta=(0, 2 * np.pi, 0.2))\n",
    "def plot_projection(theta=np.pi/4):\n",
    "    # fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    axs[0].scatter(df['x1'], df['x2'])\n",
    "    axs[0].set_aspect('equal'); axs[0].set_xlim(-4, 4); axs[0].set_ylim(-4, 4)\n",
    "    x = np.linspace(-4, 4, 100)\n",
    "    y = np.tan(theta) * x\n",
    "    axs[0].plot(x, y, 'r')\n",
    "    # plot projections\n",
    "    for i in range(len(df)):\n",
    "        x1, x2 = df.loc[i, ['x1', 'x2']]\n",
    "        x1_proj = (x1 + x2 * np.tan(theta)) / (1 + np.tan(theta)**2)\n",
    "        x2_proj = x1_proj * np.tan(theta)\n",
    "        axs[0].plot([x1, x1_proj], [x2, x2_proj], '--', c='orange', lw=0.5)\n",
    "        axs[0].plot(x1_proj, x2_proj, 'k.', ms=4)\n",
    "    projected = df['x1'] * np.cos(theta) + df['x2'] * np.sin(theta)\n",
    "    axs[1].hist(projected, color='black', bins=50, alpha=0.2)\n",
    "    axs[1].set_xlim(-5, 5)\n",
    "    axs[1].set_title(f'Variance: {np.var(projected): .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889957e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance matrix for x1 and x2\n",
    "C = np.cov(df['x1'], df['x2'])\n",
    "print(f'Covariance matrix:\\n {C}\\n')\n",
    "print(f'Total variance: {np.trace(C)}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6424f973",
   "metadata": {},
   "source": [
    "Graphing this overlap `covaraince matrix` on the original data graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x1'], df['x2'])\n",
    "\n",
    "v, w = np.linalg.eig(C)\n",
    "theta = np.arctan2(w[0, 1], w[0, 0])\n",
    "theta = 180 * theta / np.pi\n",
    "ell = matplotlib.patches.Ellipse(xy=(df['x1'].mean(), df['x2'].mean()), width=3 * np.sqrt(v[1])*2, height=3 * np.sqrt(v[0])*2, angle=theta, edgecolor='black', lw=1, facecolor='none')\n",
    "ax.add_patch(ell)\n",
    "ax.set_aspect('equal'); ax.set_xlim(-4, 4); ax.set_ylim(-4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323f640",
   "metadata": {},
   "source": [
    "Now let's try to get the actualprincipal components out by performing the eigen decomposition on the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631be800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance matrix for x1 and x2\n",
    "C = np.cov(df['x1'], df['x2'])\n",
    "print(f'Covariance matrix:\\n {C}\\n')\n",
    "print(f'Total variance: {np.trace(C)}\\n\\n')\n",
    "\n",
    "r = np.corrcoef(df['x1'], df['x2'])\n",
    "print(f'Correlation: {r[0, 1]}\\n\\n')\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "\n",
    "print(f' Eigenvalues:\\n {eigenvalues}')\n",
    "print(f' Eigenvectors:\\n {eigenvectors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x1'], df['x2'])\n",
    "ell = matplotlib.patches.Ellipse(xy=(df['x1'].mean(), df['x2'].mean()), width=3 * np.sqrt(v[1])*2, height=3 * np.sqrt(v[0])*2, angle=theta, edgecolor='black', lw=1, facecolor='none')\n",
    "ax.add_patch(ell)\n",
    "ax.plot([0, 3 * eigenvectors[0, 0]], [0, 3 * eigenvectors[1, 0]], color='red', lw=2)\n",
    "ax.plot([0, 3 * eigenvectors[0, 1]], [0, 3 * eigenvectors[1, 1]], color='pink', lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a5de2",
   "metadata": {},
   "source": [
    "Transformed coordinates to eigen space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated = df @ eigenvectors.T\n",
    "rotated.columns = ['x2_rotated', 'x1_rotated']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(rotated['x1_rotated'], rotated['x2_rotated'])\n",
    "ax.axhline(0, color='black', lw=2)\n",
    "ax.axvline(0, color='black', lw=2)\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d97cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.cov(df['x1'], df['x2'])\n",
    "print(f' Original covariance matrix:\\n {C}\\n')\n",
    "print(f' Original total variance: \\n{np.trace(C)}\\n')\n",
    "\n",
    "correlation = np.corrcoef(df['x1'], df['x2'])[0, 1]\n",
    "print(f' Original correlation: {correlation}\\n\\n------\\n\\n')\n",
    "\n",
    "rotated_C = np.cov(rotated['x1_rotated'], rotated['x2_rotated'])\n",
    "print(f' Rotated covariance matrix:\\n {rotated_C}\\n')\n",
    "print(f' Rotated total variance: \\n{np.trace(rotated_C)}\\n')\n",
    "\n",
    "\n",
    "new_correlation = np.corrcoef(rotated['x1_rotated'], rotated['x2_rotated'])[0, 1]\n",
    "print(f' New correlation: {new_correlation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca335489",
   "metadata": {},
   "source": [
    "There are a few propetirs that are critical for PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1de2d",
   "metadata": {},
   "source": [
    "> ### 1. Always rescale the data before PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06adf9",
   "metadata": {},
   "source": [
    "Consider the following dataset which consists of three variables \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X_1 &= \\text{height in cm}\\\\\n",
    "X_2 &= \\text{weight in kg}\\\\\n",
    "X_3 &= \\text{age in years}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d5b9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "num_samples = 1000\n",
    "height_cm = np.random.normal(loc=165, scale=10, size=num_samples)\n",
    "weight_kg = height_cm * 0.45 + np.random.normal(loc=0, scale=5, size=num_samples)\n",
    "age = np.random.normal(loc=28, scale=3, size=num_samples)\n",
    "age = np.clip(age, 12, 60)\n",
    "df1 = pd.DataFrame({'height_cm': height_cm, 'weight_kg': weight_kg, 'age': age})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59ff834",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(df1.T).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2348515",
   "metadata": {},
   "source": [
    "Notice  that the covaraiate is on very different scale, we will examine the effect on PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def princomp(X):\n",
    "    Cov = np.cov(X, rowvar=False)\n",
    "    evals, evecs = np.linalg.eigh(Cov)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evecs = evecs[:, idx]\n",
    "    evals = evals[idx]\n",
    "    return evecs.round(2), evals.round(2)\n",
    "\n",
    "princomp(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ea2f8e",
   "metadata": {},
   "source": [
    "If we change the units of $X_1$ from cm to in and the units of $X_2$ from kg to lbs, what happens to the PCA?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_in = round(df1['height_cm'] * 0.393701, 2)\n",
    "weight_lb = round(df1['weight_kg'] * 2.20462, 2)\n",
    "age = df1['age']\n",
    "\n",
    "df2 = pd.DataFrame({'height_in': height_in, 'weight_lb': weight_lb, 'age': age})\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b13da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(df2.T).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "princomp(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3ad95",
   "metadata": {},
   "source": [
    "This PCA results is very different, so scaling the original input would effect the eigen space, effecting teh results. Now, let's standardize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_standard = (df1 - df1.mean()) / df1.std()\n",
    "print(df1_standard.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_standard = (df2 - df2.mean()) / df2.std()\n",
    "print(df2_standard.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eabf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(df1_standard.T).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'principal compoenents for df1_standard:\\n\\n{princomp(df1_standard)}\\n\\n')\n",
    "print(f'principal compoenents for df2_standard:\\n\\n{princomp(df2_standard)}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358aa3e4",
   "metadata": {},
   "source": [
    "> ### 2. Performing PCA on the covariate matrix would be equivalent to performing PCA on the standarlized original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "princomp(df1_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65842b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def princomp_corr(X):\n",
    "    Cor = np.corrcoef(X, rowvar=False)\n",
    "    evals, evecs = np.linalg.eigh(Cor)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evecs = evecs[:, idx]\n",
    "    evals = evals[idx]\n",
    "    return evecs.round(2), evals.round(2)\n",
    "\n",
    "princomp_corr(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa446d",
   "metadata": {},
   "source": [
    "> ### 3. The principal components are guaranteed to be uncorrelated with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67140130",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 10\n",
    "X = stats.multivariate_normal(np.zeros(p), np.eye(p) + 0.75).rvs(1000)\n",
    "df = pd.DataFrame(X, columns=[f'x{i}' for i in range(1, p + 1)])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb90fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr().round(1), annot=True, cmap='coolwarm', center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, lam = princomp_corr(X)\n",
    "Z = X @ v\n",
    "\n",
    "df_z = pd.DataFrame(Z, columns=[f'z{i}' for i in range(1, p + 1)])\n",
    "df_z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_z.corr().round(1), annot=True, cmap='coolwarm', center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340df75",
   "metadata": {},
   "source": [
    "We can also use the `PCA` class in `sklearn`  to perform PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "num_samples = 1000\n",
    "num_features = 50\n",
    "\n",
    "mean = np.zeros(num_features)\n",
    "cov = np.eye(num_features) * 0.5 + 0.5 * np.random.rand(num_features, num_features)\n",
    "cov = (cov + cov.T) / 2\n",
    "\n",
    "data_high_dim = np.random.multivariate_normal(mean, cov, size=num_samples)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(data_high_dim)\n",
    "\n",
    "Z = pca.transform(data_high_dim)\n",
    "\n",
    "plt.scatter(*Z[:, 0:2].T)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d7b7a",
   "metadata": {},
   "source": [
    "We can try to deduce out the important `PC` by looking the the `cumulative explained varaince` curve or just the `explained variance` curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48871db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, data_high_dim.shape[1] + 1), pca.explained_variance_ratio_, marker='o')\n",
    "\n",
    "# or you can use the cumulative explained variance\n",
    "plt.plot(range(1, data_high_dim.shape[1] + 1), pca.explained_variance_ratio_.cumsum(), marker='o')\n",
    "plt.xticks(np.arange(0, 10, 1))\n",
    "plt.xlabel('Principal component')\n",
    "plt.ylabel('Proportion of variance explained');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db3826",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d105da5",
   "metadata": {},
   "source": [
    "## Principal Component Regression\n",
    "\n",
    "Principal component regression (PCR) is a regression technique that uses principal component analysis to reduce the number of predictor variables in a regression model. Instead of fitting the model:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\epsilon\n",
    "$$\n",
    "\n",
    "we fit the model:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 Z_1 + \\beta_2 Z_2 + \\dots + \\beta_q Z_q + \\epsilon\n",
    "$$\n",
    "\n",
    "where $Z_1, Z_2, \\dots, Z_q$ are the principal components of the predictors $X_1, X_2, \\dots, X_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebfc20e",
   "metadata": {},
   "source": [
    "### Adressing Multicolinearity With PCA\n",
    "\n",
    "PCA performes in a greedy fashion such that `PC1` would have teh greatest eigenvalue and followed by `PC2`, `PC3`, ...\n",
    "\n",
    "We will manually create a multicolinear dataset to illustrate this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4334ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_students = 1000\n",
    "\n",
    "# base scores with moderate correlation for some subjects\n",
    "base_scores = np.random.normal(loc=75, scale=10, size=num_students)\n",
    "\n",
    "# generate subject scores with partial multicollinearity\n",
    "math = base_scores + np.random.normal(scale=5, size=num_students)\n",
    "physics = base_scores * 0.9 + np.random.normal(scale=10, size=num_students)  # Strong correlation with base\n",
    "chemistry = base_scores * 0.85 + np.random.normal(scale=10, size=num_students)  # Moderate correlation\n",
    "history = np.random.normal(loc=65, scale=15, size=num_students)  # Independent\n",
    "geography = np.random.normal(loc=70, scale=10, size=num_students)  # Independent\n",
    "literature = base_scores * 1.1 + np.random.normal(scale=15, size=num_students)  # Strong correlation\n",
    "\n",
    "math = np.clip(math, 0, 100)\n",
    "physics = np.clip(physics, 0, 100)\n",
    "chemistry = np.clip(chemistry, 0, 100)\n",
    "history = np.clip(history, 0, 100)\n",
    "geography = np.clip(geography, 0, 100)\n",
    "literature = np.clip(literature, 0, 120)\n",
    "\n",
    "gpa = (0.2 * math + 0.2 * physics + 0.2 * chemistry + 0.1 * history + \n",
    "       0.1 * geography + 0.2 * literature) / 20 * 5\n",
    "gpa = np.clip(gpa, 0, 5)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'math': math,\n",
    "    'physics': physics,\n",
    "    'chemistry': chemistry,\n",
    "    'history': history,\n",
    "    'geography': geography,\n",
    "    'literature': literature,\n",
    "    'gpa': gpa\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be799e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'gpa'\n",
    "full_model_formula = f\" {response} ~ {'+'.join(df.columns.drop(response))}\"\n",
    "raw_model = smf.ols(full_model_formula, data=df).fit()\n",
    "print(raw_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff03856",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr().round(1), annot=True, cmap='coolwarm', center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = df.drop('gpa', axis=1)\n",
    "vif = pd.DataFrame([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns, columns=['VIF'])\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5834989",
   "metadata": {},
   "source": [
    "`VIF` above 10, much strong multicolinearity in the data. We can do PCA to decorrelate the data, however, notice that in this way we will `lose the interpretation` of the model.\n",
    "\n",
    "We can use the **street plot** or **cumulative sum plot** to choose the **appropriate number of PC**, trying to find the `elbo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a974ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "Z = pca.transform(X)\n",
    "\n",
    "plt.plot(range(1, X.shape[1] + 1), (pca.explained_variance_ratio_.cumsum()), marker='o')\n",
    "plt.xticks(np.arange(1, X.shape[1] + 1, 1))\n",
    "plt.xlabel('Principal component')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe00bbd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a6ed3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
