{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d499b8cf",
   "metadata": {},
   "source": [
    "# Math 189 Week 7 Summary\n",
    "> NAME: $\\color{blue}{\\text{Kaiwen Bian}}$\n",
    "> \n",
    "> PID: $\\color{blue}{\\text{A17316568}}$\n",
    ">\n",
    "> \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce0cf8",
   "metadata": {},
   "source": [
    "I certify that the following write-up is my own work, and have abided by the UCSD Academic Integrity Guidelines.\n",
    "\n",
    "- [x] Yes\n",
    "- [ ] No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c41b1",
   "metadata": {},
   "source": [
    "% # %load tex-macros\n",
    "<div hidden>\n",
    "\\newcommand{\\require}[1]{}\n",
    "\n",
    "$\\require{begingroup}\\require{newcommand}$\n",
    "$\\long\\def \\forcecommand #1{\\providecommand{#1}{}\\renewcommand{#1}}$\n",
    "$\\forcecommand{\\defeq}{\\stackrel{\\small\\bullet}{=}}$\n",
    "$\\forcecommand{\\ra}{\\rangle}$\n",
    "$\\forcecommand{\\la}{\\langle}$\n",
    "$\\forcecommand{\\pr}{{\\mathbb P}}$\n",
    "$\\forcecommand{\\qr}{{\\mathbb Q}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\av}{{\\boldsymbol{a}}}$\n",
    "$\\forcecommand{\\bv}{{\\boldsymbol{b}}}$\n",
    "$\\forcecommand{\\cv}{{\\boldsymbol{c}}}$\n",
    "$\\forcecommand{\\dv}{{\\boldsymbol{d}}}$\n",
    "$\\forcecommand{\\ev}{{\\boldsymbol{e}}}$\n",
    "$\\forcecommand{\\fv}{{\\boldsymbol{f}}}$\n",
    "$\\forcecommand{\\gv}{{\\boldsymbol{g}}}$\n",
    "$\\forcecommand{\\hv}{{\\boldsymbol{h}}}$\n",
    "$\\forcecommand{\\nv}{{\\boldsymbol{n}}}$\n",
    "$\\forcecommand{\\sv}{{\\boldsymbol{s}}}$\n",
    "$\\forcecommand{\\tv}{{\\boldsymbol{t}}}$\n",
    "$\\forcecommand{\\uv}{{\\boldsymbol{u}}}$\n",
    "$\\forcecommand{\\vv}{{\\boldsymbol{v}}}$\n",
    "$\\forcecommand{\\wv}{{\\boldsymbol{w}}}$\n",
    "$\\forcecommand{\\zerov}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\onev}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\phiv}{{\\boldsymbol{\\phi}}}$\n",
    "$\\forcecommand{\\cc}{{\\check{C}}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\Xv}{{\\boldsymbol{X}\\!}}$\n",
    "$\\forcecommand{\\yv}{{\\boldsymbol{y}}}$\n",
    "$\\forcecommand{\\Yv}{{\\boldsymbol{Y}}}$\n",
    "$\\forcecommand{\\zv}{{\\boldsymbol{z}}}$\n",
    "$\\forcecommand{\\Zv}{{\\boldsymbol{Z}}}$\n",
    "$\\forcecommand{\\Iv}{{\\boldsymbol{I}}}$\n",
    "$\\forcecommand{\\Jv}{{\\boldsymbol{J}}}$\n",
    "$\\forcecommand{\\Cv}{{\\boldsymbol{C}}}$\n",
    "$\\forcecommand{\\Ev}{{\\boldsymbol{E}}}$\n",
    "$\\forcecommand{\\Fv}{{\\boldsymbol{F}}}$\n",
    "$\\forcecommand{\\Gv}{{\\boldsymbol{G}}}$\n",
    "$\\forcecommand{\\Hv}{{\\boldsymbol{H}}}$\n",
    "$\\forcecommand{\\alphav}{{\\boldsymbol{\\alpha}}}$\n",
    "$\\forcecommand{\\epsilonv}{{\\boldsymbol{\\epsilon}}}$\n",
    "$\\forcecommand{\\betav}{{\\boldsymbol{\\beta}}}$\n",
    "$\\forcecommand{\\deltav}{{\\boldsymbol{\\delta}}}$\n",
    "$\\forcecommand{\\gammav}{{\\boldsymbol{\\gamma}}}$\n",
    "$\\forcecommand{\\etav}{{\\boldsymbol{\\eta}}}$\n",
    "$\\forcecommand{\\piv}{{\\boldsymbol{\\pi}}}$\n",
    "$\\forcecommand{\\thetav}{{\\boldsymbol{\\theta}}}$\n",
    "$\\forcecommand{\\tauv}{{\\boldsymbol{\\tau}}}$\n",
    "$\\forcecommand{\\muv}{{\\boldsymbol{\\mu}}}$\n",
    "$%$\n",
    "$\\forcecommand{\\sd}{\\text{SD}}$\n",
    "$\\forcecommand{\\se}{\\text{SE}}$\n",
    "$\\forcecommand{\\med}{\\text{median}}$\n",
    "$\\forcecommand{\\median}{\\text{median}}$\n",
    "$%$\n",
    "$\\forcecommand{\\supp}{\\text{supp}}$\n",
    "$\\forcecommand{\\E}{\\mathbb{E}}$\n",
    "$\\forcecommand{\\var}{\\text{Var}}$\n",
    "$\\forcecommand{\\Ber}{{\\text{Ber}}}$\n",
    "$\\forcecommand{\\Bin}{{\\text{Bin}}}$\n",
    "$\\forcecommand{\\Geo}{{\\text{Geo}}}$\n",
    "$\\forcecommand{\\Unif}{{\\text{Unif}}}$\n",
    "$\\forcecommand{\\Poi}{{\\text{Poi}}}$\n",
    "$\\forcecommand{\\Exp}{{\\text{Exp}}}$\n",
    "$\\forcecommand{\\Chisq}{{\\chi^2}}$\n",
    "$\\forcecommand{\\N}{\\mathbb{N}}$\n",
    "$\\forcecommand{\\iid}{{\\stackrel{iid}{\\sim}}}$\n",
    "$\\forcecommand{\\px}{p_{X}}$\n",
    "$\\forcecommand{\\fx}{f_{X}}$\n",
    "$\\forcecommand{\\Fx}{F_{X}}$\n",
    "$\\forcecommand{\\py}{p_{Y}}$\n",
    "$\\forcecommand{\\pxy}{p_{X,Y}}$\n",
    "$\\forcecommand{\\po}{{p_0}}$\n",
    "$\\forcecommand{\\pa}{{p_a}}$\n",
    "$\\forcecommand{\\Xbar}{\\overline{X}}$\n",
    "$\\forcecommand{\\Ybar}{\\overline{Y}}$\n",
    "$\\forcecommand{\\Zbar}{\\overline{Z}}$\n",
    "$\\forcecommand{\\nXbar}{n \\cdot \\overline{X}}$\n",
    "$\\forcecommand{\\nYbar}{n \\cdot \\overline{Y}}$\n",
    "$\\forcecommand{\\nZbar}{n \\cdot \\overline{Z}}$\n",
    "$\\forcecommand{\\Xn}{X_1, X_2, \\dots, X_n}$\n",
    "$\\forcecommand{\\Xm}{{X_1, X_2, \\dots, X_m}}$\n",
    "$\\forcecommand{\\Yn}{Y_1, Y_2, \\dots, Y_n}$\n",
    "$\\forcecommand{\\Ym}{{Y_1, Y_2, \\dots, Y_m}}$\n",
    "$\\forcecommand{\\sumXn}{X_1 + X_2 + \\dots + X_n}$\n",
    "$\\forcecommand{\\sumym}{Y_1 + Y_2 + \\dots + Y_m}$\n",
    "$\\forcecommand{\\la}{\\ell_\\alpha}$\n",
    "$\\forcecommand{\\ua}{u_\\alpha}$\n",
    "$\\forcecommand{\\at}{{\\alpha/2}}$\n",
    "$\\forcecommand{\\mux}{\\mu_{X}}$\n",
    "$\\forcecommand{\\muy}{\\mu_{Y}}$\n",
    "$\\forcecommand{\\sx}{\\sigma_{X}}$\n",
    "$\\forcecommand{\\sy}{\\sigma_{Y}}$\n",
    "$\\forcecommand{\\ci}{\\text{CI}}$\n",
    "$\\forcecommand{\\pvalue}{$p$-value}$\n",
    "$\\forcecommand{\\Ho}{H_{0}}$\n",
    "$\\forcecommand{\\Ha}{H_{a}}$\n",
    "\n",
    "\\vskip-\\parskip\n",
    "\\vskip-\\baselineskip\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bb0120db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "# pio.renderers.default='notebook'\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# Optional \n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "matplotlib.rcParams['figure.figsize'] = 7, 7\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "422800c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def plot_X(X, ax, type='pmf', **kwargs):\n",
    "    ax.set_xlabel('Support')\n",
    "    ax.set_title(f'{X.dist.name}{X.args}')\n",
    "    \n",
    "    min_X, max_X = X.ppf((1e-3, 1-1e-3))\n",
    "    supp_X = np.linspace(min_X-1, max_X + 1, 200)\n",
    "    \n",
    "    if type == 'pmf':\n",
    "        supp_X = np.arange(min_X-1, max_X + 1)\n",
    "        ax.bar(supp_X, X.pmf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PMF')\n",
    "    elif type == 'pdf':\n",
    "        ax.plot(supp_X, X.pdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PDF')\n",
    "    elif type == 'cdf':\n",
    "        ax.plot(supp_X, X.cdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('CDF')\n",
    "    else:\n",
    "        raise ValueError('type must be pmf or cdf')\n",
    "\n",
    "def decision(pvalue, alpha):\n",
    "    if pvalue < alpha:\n",
    "        print(f'reject H0: pvalue={pvalue} < {alpha}')  \n",
    "    else: \n",
    "        print(f'fail to reject H0: pvalue={pvalue} ≥ {alpha}')\n",
    "\n",
    "def standardize(X):\n",
    "    return (X - X.mean()) / X.std()\n",
    "\n",
    "\n",
    "def make_data(errors):\n",
    "    n = len(errors)\n",
    "    x1 = np.linspace(0, 1, n)\n",
    "    x2 = np.random.rand(n)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'x1': x1, \n",
    "        'x2': x2, \n",
    "        'y': 2 + 3*x1 + 4*x2 + errors\n",
    "    })\n",
    "\n",
    "def plot_regression(data, fit, residuals=True):\n",
    "    b = fit.params\n",
    "    b0, b1, b2 = *b, *np.zeros(3 - len(b))\n",
    "    y, x1, x2 = data.y, data.x1, data.x2\n",
    "    fig = px.scatter_3d(x=x1, y=x2, z=y)\n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "            xaxis_title='X1',\n",
    "            yaxis_title='X2',\n",
    "            zaxis_title='Y'),\n",
    "            margin=dict(l=0, r=0, b=0, t=0\n",
    "        )\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    \n",
    "    x1_grid, x2_grid = np.meshgrid(x1, x2)\n",
    "    yhat = b0 + (b1 * x1_grid) + (b2 * x2_grid)\n",
    "    fig.add_trace(\n",
    "        go.Surface(x=x1_grid, y=x2_grid, z=yhat, opacity=0.5,colorscale='Gray')\n",
    "    )\n",
    "    if residuals:\n",
    "        for i in range(len(x1)):\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(x=[x1[i], x1[i]], y=[x2[i], x2[i]], z=[b0 + b1*x1[i] + b2*x2[i], y[i]], mode='lines', line=dict(color='black', width=2))\n",
    "            )\n",
    "    fig.update_layout(showlegend=False, scene_camera=dict(eye=dict(x=2.0, y=0.5, z=0.1)))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfec7ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6dc53",
   "metadata": {},
   "source": [
    "## Key Takeaways from Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc992f31",
   "metadata": {},
   "source": [
    "#### Tuesday: \n",
    "We went over the three types of inference, the three types of uncertainty quantification that we cna do with a linear regression analysis, which includes hypothesis testing with the coefficient, general hypothesis tetsing, and analysis of varaicne. Specifcally, we dive very deep into how we can constructs smaller models form the big model to see if the bigger model actually help us explain more variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69de7b",
   "metadata": {},
   "source": [
    "#### Thursday\n",
    "We looked into the case when there exist non-linearity in our data and how we might solve this issue through using interaction terms. We dived into the mathamatical formulation of these interaction terms as well as their usages. Afterwards, we examined the effects of multicolinearity as well as how we cna use different techniques to try to spot them. At last, we went into dealing with cases of categorical responses and derived a new model to deal with it, the logistci regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaadbca8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abad0d9",
   "metadata": {},
   "source": [
    "# Inference from Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f47b9",
   "metadata": {},
   "source": [
    "Consider the following regression data and fitting to a model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abecc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/BudgetUK.csv\"\n",
    "df = pd.read_csv(url).drop(['rownames', 'totexp'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1743dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'income'\n",
    "covariates = df.columns.drop(response)\n",
    "model_formula = f\"np.log({response}) ~ wfood + wother\"\n",
    "model = smf.ols(model_formula, data=df).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3aaa91",
   "metadata": {},
   "source": [
    "There are qualitatively, **three different types of inference (`uncertainty quantification`)** that can be made from regression models:\n",
    "\n",
    "1. Hypothesis Testing\n",
    "2. Analysis of Variance\n",
    "3. General Linear Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313ab33",
   "metadata": {},
   "source": [
    "## Type 1: Examining Effects of Particular Variable\n",
    "\n",
    "The *p*-values for the coefficients are used to test the null hypothesis that the coefficient is zero, i.e.,\n",
    "\n",
    "$$\n",
    "H_0: \\beta_j = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_a: \\beta_j \\neq 0\n",
    "$$\n",
    "\n",
    "At level $\\alpha$, if the *p*-value for the $j$-th coefficient is less than $\\alpha$, then we reject the null hypothesis that the $j$-th coefficient is zero, i.e.,\n",
    "\n",
    "**We can look at the significance of all of the coefficients!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d26e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51426d",
   "metadata": {},
   "source": [
    "We will be using a t-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient = 'wother'\n",
    "t = (model.params[coefficient] - 0) / model.bse[coefficient]\n",
    "dof = model.df_resid\n",
    "\n",
    "t_distribution = stats.t(dof)\n",
    "t_p_value = 2 * (1 - t_distribution.cdf(np.abs(t)))\n",
    "t_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aafd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f52bb",
   "metadata": {},
   "source": [
    "## Type 2: Analysis of Variance\n",
    "Very complciated, usually there are a few cources that covers the detials of ANOVA.\n",
    "\n",
    "- We will only get some key `intuition` here for using ANOVA.\n",
    "- It is based from **sums of squares**.\n",
    "\n",
    "We need to go back to sum of squares and sum of residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39cbf6",
   "metadata": {},
   "source": [
    "### Sum of Squares\n",
    "\n",
    "We have three main quantities that are used to evaluate the goodness of fit of the regression model:\n",
    "\n",
    "| Quantity | Description |\n",
    "|----------|------------|\n",
    "| $y$     | Observed values of the dependent variable |\n",
    "| $\\hat{y}$ | Predicted values of the dependent variable |\n",
    "| $\\bar{y}$ | Mean of the observed values of the dependent variable |\n",
    "\n",
    "Your regression line has to path through the means of the data, the mean of $\\hat{y}$ will be thes ame with mean of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f9ec1",
   "metadata": {},
   "source": [
    "Based on this, we have three important **sums of squares** in the regression model:\n",
    "\n",
    "| Sum of squares | Equation | Intuition |\n",
    "|---------------|----------|-----------|\n",
    "| $SS_{\\text{Tot}}$ | $\\sum\\limits_{i=1}^{n} (y_i - \\bar{y})^2$ | How much variability is present in the observed values $y$ |\n",
    "| $SS_{\\text{Reg}}$ | $\\sum\\limits_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2$ | How much variability the regression model $\\hat{y}$ reproduces |\n",
    "| $SS_{\\text{Res}}$ | $\\sum\\limits_{i=1}^{n} (y_i - \\hat{y}_i)^2$ | How much variability in $y$ the regression model $\\hat{y}$ is unable to reproduce |\n",
    "\n",
    "- $SS_{\\text{Reg}}$ + $SS_{\\text{Res}}$ should be smaller than $SS_{\\text{Tot}}$. Think of a triangle where $SS_{\\text{Reg}}$ is the base.\n",
    "- Hence we want a **triangle with smaller base**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b94418c",
   "metadata": {},
   "source": [
    "### F-Statistic\n",
    "\n",
    "The *F*-statistic is a measure of how well the model explains the variance in the data. It is given by:\n",
    "\n",
    "$$\n",
    "\\hat{F} = \\frac{SS_{\\text{Reg}} / \\text{df}_{\\text{Reg}}}{SS_{\\text{Res}} / \\text{df}_{\\text{Res}}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\text{df}_{\\text{Reg}} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{df}_{\\text{Res}} = n - p - 1\n",
    "$$\n",
    "\n",
    "Intuitively, the *F*-statistic is the **ratio** of the explained variance to the unexplained variance in the data.\n",
    "\n",
    "The *F*-statistic is used to test the null hypothesis that the model is no better than the null model, i.e., the model with no predictors.\n",
    "\n",
    "$$\n",
    "H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_a: \\text{At least one } \\beta_j \\neq 0\n",
    "$$\n",
    "\n",
    "The *F*-statistic follows an $F(\\text{df}_{\\text{Reg}}, \\text{df}_{\\text{Res}})$ distribution under the null hypothesis, i.e.,\n",
    "\n",
    "$$\n",
    "\\hat{F} = \\frac{SS_{\\text{Reg}} / \\text{df}_{\\text{Reg}}}{SS_{\\text{Res}} / \\text{df}_{\\text{Res}}} \\sim F(\\text{df}_{\\text{Reg}}, \\text{df}_{\\text{Res}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3506af",
   "metadata": {},
   "source": [
    "> ANOVA is the doing like a `goodness of fit` test on all the variables of the F-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652264fa",
   "metadata": {},
   "source": [
    "### Analysis of Variance (ANOVA)\n",
    "\n",
    "ANOVA is a method used to test whether the inclusion of a set of variables in a model significantly improves the model fit (among other things...).\n",
    "\n",
    "### Setting:\n",
    "\n",
    "Suppose we have two models: **we have one full model, then a `subset` od the big model, we want to see whether there is actually a improvement in the fit of data.**\n",
    "\n",
    "**Small model:**\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_1 + \\beta_3 x_3 + \\beta_{p-1} x_{p-1} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "**Big model:**\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\dots + \\beta_p x_p + \\epsilon_i\n",
    "$$\n",
    "\n",
    "i.e.,\n",
    "\n",
    "- Small model: $(\\beta_0, \\beta_1, \\beta_3, \\beta_{p-1})$\n",
    "- Big model: $(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\dots, \\beta_p)$\n",
    "\n",
    "The idea is that **does having a bigger model give you more power to explain the variance (how much explanatory power does your big model has in relative to the smaller model)?** We look at it from three perspective\n",
    "- `Extra variance explained`\n",
    "- `Extra degree of freedom needed`\n",
    "- `Extra residual caused`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446fce4",
   "metadata": {},
   "source": [
    "### Method:\n",
    "\n",
    "- How much excess variance is explained by the inclusion of variables in the Big Model?\n",
    "\n",
    "  **Answer:**\n",
    "  $$\n",
    "  SS_{\\text{Reg}}(\\text{Small Model}) - SS_{\\text{Reg}}(\\text{Big Model})\n",
    "  $$\n",
    "\n",
    "- How many degrees of freedom are used up by the inclusion of variables in the Big Model?\n",
    "\n",
    "  **Answer:**\n",
    "  $$\n",
    "  \\Delta \\text{df} = \\text{df}_{\\text{Reg}}(\\text{Small Model}) - \\text{df}_{\\text{Reg}}(\\text{Big Model})\n",
    "  $$\n",
    "\n",
    "- How much residual variance is left after the inclusion of variables in the Big Model?\n",
    "\n",
    "  **Answer:**\n",
    "  $$\n",
    "  SS_{\\text{Res}}(\\text{Big Model})\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ef6e7",
   "metadata": {},
   "source": [
    "If the excess variance explained by the inclusion of $x_3, \\dots, x_p$ is large relative to the residual variance left after the inclusion of $x_3, \\dots, x_p$, then we conclude that the inclusion of $x_3, \\dots, x_p$ significantly improves the model fit, i.e.,\n",
    "\n",
    "$$\n",
    "\\hat F = {\\frac{\\text{SS}_{Reg}(\\text{Big Model}) - \\text{SS}_{Reg}(\\text{Small Model})}{\\Delta \\text{df}} \\Bigg / {\\frac{\\text{SS}_{Res}(\\text{Big Model})}{\\text{df}_{Res}(\\text{Big Model})}}}\n",
    "$$\n",
    "\n",
    "- Ths is a `generalization` of the F-test\n",
    "\n",
    "* If $\\hat F$ is $\\Large LARGE$ then including the variables of the big model significantly improves the model fit\n",
    "* If $\\hat F$ is $\\scriptsize small$ then including the variables of the big model does not significantly improve the model fit\n",
    "* The rejection region for the ANOVA *F*-test is: $(x_{\\alpha}, \\infty)$\n",
    "\n",
    "$$\n",
    "\\hat F \\sim F\\Big(\\Delta \\text{df}, \\ \\ \\ \\text{df}_{Res}(\\text{Big Model})\\Big)\n",
    "$$\n",
    "\n",
    "where $\\Delta \\text{df} = \\text{df}_{Reg}(\\text{Small Model}) - \\text{df}_{Reg}(\\text{Big Model})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2b5d7",
   "metadata": {},
   "source": [
    "Techniqually, with this setup, you can have a **combinatorial** composition of nested smaller models!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5c00fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_formula = f'np.log({response}) ~ wfood + wother + age + children'\n",
    "big_formula = f'np.log({response}) ~ wfood + wother + wcloth + wtrans +  age + children'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1bfcd759",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = smf.ols(small_formula, data=df).fit()\n",
    "big_model = smf.ols(big_formula, data=df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_table = sm.stats.anova_lm(small_model, big_model)\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f2b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_dist = stats.f(2, 1512)\n",
    "F_stat = anova_table['F'].values[1]\n",
    "p_value = 1 - F_dist.cdf(F_stat)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ead40",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision(anova_table['Pr(>F)'].values[1], alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f157045",
   "metadata": {},
   "source": [
    "## Type 3: How Regression Coefficients Relates to Each Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dabdbb",
   "metadata": {},
   "source": [
    "**Under the null hypotheiss, you have a belief that there are linear corrolation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339972a3",
   "metadata": {},
   "source": [
    "### General Linear Hypothesis Testing\n",
    "\n",
    "Suppose we want to test some general hypothesis about the coefficients, e.g.,\n",
    "\n",
    "$$\n",
    "H_0: 2\\beta_1 - 3\\beta_2 = 1 \\quad \\text{and} \\quad 3\\beta_0 + \\beta_1 + 4\\beta_2 = 100\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_a: 2\\beta_1 - 3\\beta_2 \\neq 1 \\quad \\text{OR} \\quad 3\\beta_0 + \\beta_1 + 4\\beta_2 \\neq 100\n",
    "$$\n",
    "\n",
    "This is called a **general linear hypothesis** and can be written in matrix form as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "0 & 2 & -3 \\\\ \n",
    "3 & 1 & 4 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "\\beta_0 \\\\ \n",
    "\\beta_1 \\\\ \n",
    "\\beta_2 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1 \\\\ \n",
    "100 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "More generally, suppose we want to test the hypothesis:\n",
    "\n",
    "$$\n",
    "H_0: R\\beta = q\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_a: R\\beta \\neq q\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ R $ is a $ c \\times (p + 1) $ matrix of constants\n",
    "- $ q $ is a $ c \\times 1 $ vector of constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_formula = f'np.log({response}) ~ wfood + wother + wfuel'\n",
    "alt_model = smf.ols(alt_formula, data=df).fit()\n",
    "alt_model.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592802d",
   "metadata": {},
   "source": [
    "Let's assume the following:\n",
    "$$\n",
    "R= \\begin{bmatrix} 0 & 2 & 0 & -1\\\\ 0 & 1 & -1 & -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* $q = (0, 0)^\\top$ \n",
    "\n",
    "what are we testing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([\n",
    "    [0, 2, 0, -1],\n",
    "    [0, 1, -1, -1]\n",
    "])\n",
    "q = np.array([0, 0])\n",
    "\n",
    "alt_model.f_test((R, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c5ed1",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb16e9c",
   "metadata": {},
   "source": [
    "# Interaction Terms: what happens when having nonlinear data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e111f",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "**Maybe we can use transformation (i.e. log transformation) to solve some easy condisitions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa4865",
   "metadata": {},
   "source": [
    "### Interaction: Categorical $\\times$ Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc085603",
   "metadata": {},
   "source": [
    "The objective of adding an interaction term is to capture different orders of derivative information (i.e. second order interaction term captures second order derivative terms). More interatcions, we can capture more non-linear interactions by seeing how `covariate` $x_i$ interact with the `responses` $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac371164",
   "metadata": {},
   "source": [
    "Assuming we want to use the following model using **dummy variables** $x_2$:\n",
    "\n",
    "$$\n",
    "E(y|x_1, x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\n",
    "$$\n",
    "\n",
    "For simplicity, suppose the term $ x_2 \\in \\{0,1\\} $ is a **categorical variable**. Then the interpretation of the coefficient $ \\beta_3 $ is as follows:\n",
    "\n",
    "> Case: categorical $ x_2 = 0 $\n",
    "$$\n",
    "E(y|x_1, x_2 = 0) = \\beta_0 + \\beta_1 x_1\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow \\frac{\\partial}{\\partial x_1} E(y|x_1, x_2 = 0) = \\beta_1\n",
    "$$\n",
    "\n",
    "We can take teh derivative to get one coefficient.\n",
    "\n",
    "> Case: categorical $ x_2 = 1 $\n",
    "$$\n",
    "E(y|x_1, x_2 = 1) = \\beta_0 + \\beta_1 x_1 + \\beta_2 + \\beta_3 x_1\n",
    "$$\n",
    "$$\n",
    "= (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3)x_1\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow \\frac{\\partial}{\\partial x_1} E(y|x_1, x_2 = 1) = \\beta_1 + \\beta_3\n",
    "$$\n",
    "\n",
    "Similar as above, we can take another derivative to get the other coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25ea3f",
   "metadata": {},
   "source": [
    "> Hence, we can say that:\n",
    "\n",
    "$$\n",
    "\\beta_3 = \\left[ \\frac{\\partial}{\\partial x_1} E(y|x_1, x_2 = 1) \\right] - \\left[ \\frac{\\partial}{\\partial x_1} E(y|x_1, x_2 = 0) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_3 = (\\beta_1 + \\beta_3) - \\beta_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_3 = \\frac{\\partial^2}{\\partial x_1 \\partial x_2} E(y | x_1, x_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_3 = \\text{the infinite-decimal change in the effect of } x_1 \\text{ on } y \\text{ when } x_2 \\text{ increases from 0 to 1.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb32e5",
   "metadata": {},
   "source": [
    "$ \\beta_3 $ represents the **interaction effect** between $ x_1 $ and $ x_2 $. It quantifies **how much the effect of $ x_1 $ on $ y $ changes when $ x_2 $ increases from 0 to 1**.\n",
    "\n",
    "- If $ \\beta_3 $ is significantly different from zero, it suggests that the relationship between $ x_1 $ and $ y $ depends on the value of $ x_2 $.\n",
    "\n",
    "- **In another word, it captures the gradient of $x_1$ with resect to changes in the $x_2$ variables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38ed4e",
   "metadata": {},
   "source": [
    "### Interaction: Numeric $\\times$ Numeric\n",
    "\n",
    "Now with continuous scale, we can essentially model the original changes with another **partial derivative**! When $ x_1 $ and $ x_2 $ are continuous, it has the effect of bringing in some curvature (nonlinear effect) into the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90fd52",
   "metadata": {},
   "source": [
    "More generally, the **interaction term $ \\beta_{i:j} $**, which is the coefficient of the term $ x_i x_j $ in the model, is interpreted as the second order partial derivatives:\n",
    "\n",
    "$$\n",
    "\\beta_{i:j} = \\frac{\\partial^2}{\\partial x_j \\partial x_i} E(y | x_i, x_j)\n",
    "$$\n",
    "\n",
    "Just by adding an interaction term the model can become more flexible and can capture more complex relationships between the predictors and the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503290f4",
   "metadata": {},
   "source": [
    "The interatcion between $x_1$ and $x_2$ is atrteing to including **curvatures** and **saddle point infromations**, you can capture mroe than just fistting a plain on it. Depends on the **order** of your interactions, you can capture differnt trends.\n",
    "\n",
    "- When linearrity is insufficient to capture, you can capture the relationships between $x$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2b98bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(data, model, residuals=False):\n",
    "    y, x1, x2 = data[\"y\"], data[\"x1\"], data[\"x2\"]\n",
    "    fig = px.scatter_3d(x=x1, y=x2, z=y)\n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "            xaxis_title='X1',\n",
    "            yaxis_title='X2',\n",
    "            zaxis_title='Y'),\n",
    "            margin=dict(l=0, r=0, b=0, t=0\n",
    "        )\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    \n",
    "    x1_grid, x2_grid = np.meshgrid(x1, x2)\n",
    "    df = pd.DataFrame({'x1': x1_grid.ravel(), 'x2': x2_grid.ravel()})\n",
    "    yhat = model.predict(df).values\n",
    "    fig.add_trace(\n",
    "        go.Surface(x=x1_grid, y=x2_grid, z=np.reshape(yhat, x1_grid.shape), opacity=0.01,colorscale='viridis')\n",
    "    )\n",
    "    if residuals:\n",
    "        for i in range(len(x1)):\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(x=[x1[i], x1[i]], y=[x2[i], x2[i]], z=[model.predict(df), y[i]], mode='lines', line=dict(color='black', width=2))\n",
    "            )\n",
    "    fig.update_layout(showlegend=False, scene_camera=dict(eye=dict(x=2.0, y=0.5, z=0.1)))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x1 = np.random.randn(n)\n",
    "x2 = np.random.randn(n)\n",
    "y = -1 + x1 +  x2 - x1 * x2 + np.random.randn(n)\n",
    "data = pd.DataFrame({\"y\": y, \"x1\": x1, \"x2\": x2})\n",
    "model = smf.ols(formula='y ~ x1 + x2 + x1:x2', data=data).fit()\n",
    "\n",
    "plot_regression(data, model, residuals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300\n",
    "x1 = np.random.randn(n)\n",
    "x2 = np.random.randn(n)\n",
    "y = -1 + x1 +  x2 + x1**2 * x2**2 + np.random.randn(n)\n",
    "data = pd.DataFrame({\"y\": y, \"x1\": x1, \"x2\": x2})\n",
    "model = smf.ols(formula='y ~ x1 + x2 + I(x1**2) + I(x2**2)', data=data).fit()\n",
    "\n",
    "plot_regression(data, model, residuals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15473cf",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b025551",
   "metadata": {},
   "source": [
    "# Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a2974",
   "metadata": {},
   "source": [
    "When you incorporate so many interaction terms to the point where you may have varaible interaction terms that are not independent any more, you face the issue of `multicollinearity`. Let's look at a sample daatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/BudgetUK.csv\"\n",
    "df = pd.read_csv(url).drop(['rownames', 'totexp'], axis=1)\n",
    "\n",
    "response = 'income'\n",
    "covariates = df.columns.drop(response)\n",
    "\n",
    "full_formula = f\"np.log({response}) ~ {' + '.join(covariates)}\"\n",
    "print(full_formula)\n",
    "\n",
    "partial_formula = f\"np.log({response}) ~ {' + '.join(covariates[3:])}\"\n",
    "print(partial_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e4f53",
   "metadata": {},
   "source": [
    "### Multicollinearity Intuition\n",
    "\n",
    "Intuitively, it is impossible to make an **infinite-decimal changes** in one of your covariate variable $x_i$ to cause changes in teh response variable $y$ while holding everything else constant (if one thing changes, the other covariate variable must also changes).\n",
    "\n",
    "- Making one unit change or any change in $x_1$ would cause chnage in $y$ as will as in $x_2$ and $x_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6cbdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collinear_illustration():\n",
    "    x1, x2, x3, y = widgets.IntSlider(description='x1'),\\\n",
    "    widgets.IntSlider(description='x2'),\\\n",
    "    widgets.IntSlider(description='x3'),\\\n",
    "    widgets.IntSlider(description='y')\n",
    "    dl = widgets.dlink((x1, 'value'), (y, 'value'))\n",
    "    dl = widgets.dlink((x2, 'value'), (y, 'value'))\n",
    "    dl = widgets.dlink((x3, 'value'), (y, 'value'))\n",
    "    dl = widgets.dlink((x1, 'value'), (x2, 'value'))\n",
    "    dl = widgets.dlink((x2, 'value'), (x3, 'value'))\n",
    "    dl = widgets.dlink((x3, 'value'), (x1, 'value'))\n",
    "    dl = widgets.dlink((x1, 'value'), (x3, 'value'))\n",
    "    dl = widgets.dlink((x2, 'value'), (x1, 'value'))\n",
    "    dl = widgets.dlink((x3, 'value'), (x2, 'value'))\n",
    "    display(x1, x2, x3, y)\n",
    "\n",
    "collinear_illustration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d263628",
   "metadata": {},
   "source": [
    "### Issues ⚠️\n",
    "In the regression model, the effect of multicollinearity would cause:\n",
    "\n",
    "- our model to not show significance response when there are actually significiance responses existing!\n",
    "\n",
    "- The **sensitivity** values is incorrect, the coefficient are meaningless from a regression model in this case!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd5c1e",
   "metadata": {},
   "source": [
    "Using ANOVA, we can see the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b97147",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = smf.ols(full_formula, data=df).fit()\n",
    "partial_model = smf.ols(partial_formula, data=df).fit()\n",
    "\n",
    "anova_table = sm.stats.anova_lm(partial_model, full_model)\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Full model table:\\n {full_model.summary().tables[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed6138",
   "metadata": {},
   "source": [
    "This coefficient term looks pretty weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a25756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Partial model table:\\n {partial_model.summary().tables[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad36263",
   "metadata": {},
   "source": [
    "Let's look at the corrolation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba22cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_table = df[covariates].corr()\n",
    "sns.heatmap(corr_table, annot=True, cmap='coolwarm', mask=np.tril(corr_table, k=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb305b",
   "metadata": {},
   "source": [
    "Theree seems to be multicollinearity taht exist!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6020e6",
   "metadata": {},
   "source": [
    "### Multicollinearity Detection\n",
    "\n",
    "We can identify it through `corrolation table` or `variance inflation factor`. Consider the regression model:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n",
    "$$\n",
    "\n",
    "but $ x_1 $ and $ x_2 $ are related via the relationship:\n",
    "\n",
    "$$\n",
    "x_1 = 10 - x_2 + \\eta \\quad \\text{where} \\quad \\eta \\sim N(0, \\tau^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.random.randn(1000)\n",
    "x1 = 1 - x2 + 0.01 * np.random.randn(1000)\n",
    "y = -2 + 1 * x1 + 1 * x2 + 1.1 * np.random.randn(1000)\n",
    "data = pd.DataFrame({\"y\": y, \"x1\": x1, \"x2\": x2})\n",
    "model = smf.ols(formula='y ~ x1 + x2', data=data).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80beb5",
   "metadata": {},
   "source": [
    "> You can diagnose multicollinearity by looking at the **Variance Inflation Factor (VIF)** of each predictor.\n",
    ">\n",
    "> *The idea of the VIF is to quantify how much the variance of the estimated coefficient* $ \\hat{\\beta}_j $ *is inflated due to multicollinearity.*  \n",
    "> *This is related to the matrix* $ (X^T X)^{-1} $ *we encountered for the regression coefficients* $ \\hat{\\beta} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "exog = full_model.model.exog\n",
    "names = full_model.params.index\n",
    "for i in range(1, exog.shape[1]):\n",
    "    print(f'VIF: {names[i]}: {variance_inflation_factor(exog, i): .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0843e20",
   "metadata": {},
   "source": [
    "Intuitively, we create another smaller `regression model` for **each of the covariate variable** as the **response** of every single else covaraite varaibles in the model. Then we calculate teh $R^2$ of the model.\n",
    "\n",
    "- If $R^2$ is high, this means that the rest covaraite variable is very corrolated with this covaraite varaible, the VIF is very high, showing multicollinearity.\n",
    "    - If VIF is greater than 5, it pretty much shows multicolineaity\n",
    "    - VIF is always greater or equal to 1\n",
    "\n",
    "- Then we just delete the highest VIF variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1de9c",
   "metadata": {},
   "source": [
    "Now modifying the model by removing the multicolinearity varaoble, we see the parameter as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ceb038",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_modified = covariates.drop(['wtrans'])\n",
    "full_formula_modified = f\"np.log({response}) ~ {' + '.join(covariates_modified)}\"\n",
    "print(full_formula_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5133f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_modified = smf.ols(full_formula_modified, data=df).fit()\n",
    "print(full_model_modified.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog = full_model_modified.model.exog\n",
    "names = full_model_modified.params.index\n",
    "for i in range(1, exog.shape[1]):\n",
    "    print(f'VIF: {names[i]}: {variance_inflation_factor(exog, i): .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee525856",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab4e41",
   "metadata": {},
   "source": [
    "# Regression For Categorical Response\n",
    "\n",
    "Previously we are all talking about things on the covariate scale, now we wnat to see on the response scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a998da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "bank_marketing = fetch_ucirepo(id=222) \n",
    "\n",
    "df = bank_marketing.data.features\n",
    "df['outcome'] = bank_marketing.data.targets['y'].transform(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "to_drop = ['contact', 'poutcome', 'pdays', 'pdays']\n",
    "df = df.drop(columns=to_drop)\n",
    "df = df.dropna()\n",
    "\n",
    "categorical_vars = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'month', 'day_of_week']\n",
    "\n",
    "for var in categorical_vars:\n",
    "    df[var] = df[var].astype('category')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d65de",
   "metadata": {},
   "source": [
    "> #### Issue with treating $y$ as a standard quantiative response\n",
    "The linear regression model will give us a predicted value for the response variable for any given value of the predictor variable, but this predicted value is **not a probability**. The predicted value can take on any value between 0 and 1, but it doesn’t necessarily represent the probability of the response variable being a 1.\n",
    "\n",
    "**We need to have a model that give in faithful of our response varaible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f63cc7",
   "metadata": {},
   "source": [
    "> #### Remedy for the case when $y$ is binary categorical\n",
    "\n",
    "We need a model that can give us the probability of the response variable being a 1 for any given value of the predictor variables. That is, we want a model which, when given a set of independent variables $x_1, x_2, \\dots, x_p$, will give us:\n",
    "\n",
    "$$\n",
    "p(x_1, x_2, \\dots, x_p)\n",
    "$$\n",
    "\n",
    "the probability that the dependent variable $y$ is a 1.\n",
    "\n",
    "$$\n",
    "(x_1, x_2, \\dots, x_p) \\mapsto p(x_1, x_2, \\dots, x_p)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "p(x_1, x_2, \\dots, x_p) = \\mathbb{P}(y = 1 \\mid x_1, x_2, \\dots, x_p)\n",
    "$$\n",
    "\n",
    "Let's look like the world of `Odds`, it is a different way of looking at probability -> it is a very different but a more intuitive paradim compared to probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ce914",
   "metadata": {},
   "source": [
    "> #### Odds\n",
    "\n",
    "Consider the stock price of a famous GPU manufacturer whose valuation recently hit $2$ trillion.\n",
    "\n",
    "Suppose there’s a $0.7$ probability that the stock price will go down tomorrow, and a $0.3$ probability that the stock price will go up tomorrow.\n",
    "\n",
    "We have the following basic table:\n",
    "\n",
    "| Outcome | Probability |\n",
    "|---------|------------|\n",
    "| Up      | 0.3        |\n",
    "| Down    | 0.7        |\n",
    "\n",
    "The odds of the stock price going up tomorrow is:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathbb{P}(\\text{Up})}{\\mathbb{P}(\\text{Down})} = \\frac{0.3}{0.7} = \\frac{3}{7}\n",
    "$$\n",
    "\n",
    "In general, given an event $A$, which occurs with probability $\\mathbb{P}(A)$:\n",
    "\n",
    "| Outcome | Probability        |\n",
    "|---------|--------------------|\n",
    "| $A$     | $\\mathbb{P}(A)$    |\n",
    "| $A^c$   | $1 - \\mathbb{P}(A)$ |\n",
    "\n",
    "The odds of $A$ is:\n",
    "\n",
    "$$\n",
    "\\text{odds}(A) = \\frac{\\mathbb{P}(A)}{\\mathbb{P}(A^c)} = \\frac{\\mathbb{P}(A)}{1 - \\mathbb{P}(A)}\n",
    "$$\n",
    "\n",
    "Notice that it is like **changing the absoluteness of probability into a relative standard**，odds takes in\n",
    "\n",
    "$$\n",
    "(0, \\infty)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65350648",
   "metadata": {},
   "source": [
    "> #### Log-Odds\n",
    "\n",
    "Because $\\mathbb{P}(A) \\in (0,1)$, it follows that\n",
    "\n",
    "$$\\text{odds}(A) \\in (0, \\infty)$$\n",
    "\n",
    "$$\\Rightarrow \\quad \\boxed{\\text{log-odds}(A) \\in (-\\infty, \\infty)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc15fd5",
   "metadata": {},
   "source": [
    "> #### Logistic Function\n",
    "\n",
    "Notice that this is how `Statistics` dervied out logistic regression!!! Remanber in `Deep Learning` logistic coems out from Bayes + Gaussian assumption.\n",
    "\n",
    "\n",
    "Given the independent variables $x_1, x_2, \\dots, x_p$, consider the event:\n",
    "\n",
    "$$\n",
    "A = \\{ y = 1 \\mid x_1, x_2, \\dots, x_p \\}.\n",
    "$$\n",
    "\n",
    "The log-odds of $A$ is:\n",
    "\n",
    "$$\n",
    "\\text{log-odds}(\\{ y = 1 \\mid x_1, x_2, \\dots, x_p \\}) = \\log \\frac{\\mathbb{P}(A)}{\\mathbb{P}(A^c)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\log \\left( \\frac{\\mathbb{P}(y = 1 \\mid x_1, x_2, \\dots, x_p)}{\\mathbb{P}(y = 0 \\mid x_1, x_2, \\dots, x_p)} \\right)\n",
    "$$\n",
    "\n",
    "Instead of modeling the probability of the response variable directly:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(y = 1 \\mid x_1, x_2, \\dots, x_p) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p \\text{  ❌}\n",
    "$$\n",
    "\n",
    "logistic regression models the **log-odds** of the response variable:\n",
    "\n",
    "$$\n",
    "\\text{log-odds}(y = 1 \\mid x_1, x_2, \\dots, x_p) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p \\text{ ✅}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3268b640",
   "metadata": {},
   "source": [
    "Since we use this log-odds definition, we can derive back the probability and get a very familier expression:\n",
    "\n",
    "$$\n",
    "\\text{log-odds}(y = 1 \\mid x_1, x_2, \\dots, x_p) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\text{odds}(y = 1 \\mid x_1, x_2, \\dots, x_p) = \\exp \\left( \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\frac{\\mathbb{P}(y = 1 \\mid x_1, x_2, \\dots, x_p)}{\\mathbb{P}(y = 0 \\mid x_1, x_2, \\dots, x_p)} = \\exp \\left( \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\frac{\\mathbb{P}(y = 1 \\mid x_1, x_2, \\dots, x_p)}{1 - \\mathbb{P}(y = 1 \\mid x_1, x_2, \\dots, x_p)} = \\exp \\left( \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ad326",
   "metadata": {},
   "source": [
    "By solving for $\\mathbb{P}(y = 1 \\mid x_1, x_2, \\dots, x_p)$, we get:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(y = 1 \\mid x_1, x_2, \\dots, x_p) =\n",
    "\\frac{\\exp \\left( \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p \\right)}\n",
    "{1 + \\exp \\left( \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p \\right)}\n",
    "$$\n",
    "\n",
    "Assuming $z = \\left( \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p \\right)$, we can then say that:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{\\exp(z)}{1 + \\exp(z)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4fd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = lambda x: np.exp(x) / (1 + np.exp(x))\n",
    "x = np.linspace(-10, 10, 100)\n",
    "plt.plot(x, sigma(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4159cf",
   "metadata": {},
   "source": [
    "> #### Logistic Regression\n",
    "The way logistic regression remedies the issue of standard regression for a categorical response is to use `Odds` and assuming data coming from a `bernulli` distribution.\n",
    "\n",
    "- Logistic regression models the **odds** of the response variable being a 1 as a linear function of the predictor variables.\n",
    "\n",
    "  $$\n",
    "  \\text{log-odds}(y = 1 \\mid x_1, x_2, \\dots, x_p) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\n",
    "  $$\n",
    "\n",
    "- The probability of the response variable being a 1 is given by the **logistic function** of the linear combination of the predictor variables.\n",
    "\n",
    "  $$\n",
    "  \\mathbb{P}(y = 1 \\mid x_1, x_2, \\dots, x_p) = \\sigma \\left( \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p \\right)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\sigma(z) = \\frac{\\exp(z)}{1 + \\exp(z)}\n",
    "  $$\n",
    "\n",
    "- The function which bridges the gap between the log-odds and the probability is called the **logistic function**, or the **sigmoid function**.\n",
    "\n",
    "- All of these is about how you fit to what data distribution you have! This `CDF` function may changes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc645f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = smf.ols('outcome ~ duration + balance', data=df).fit()\n",
    "ols_model.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = smf.logit('outcome ~ duration + balance + education', data=df).fit()\n",
    "print(logistic_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customer = df[df.index == 24148]\n",
    "ols_model.predict(new_customer), logistic_model.predict(new_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2054701",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
