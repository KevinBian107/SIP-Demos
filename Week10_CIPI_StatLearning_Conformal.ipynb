{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d499b8cf",
   "metadata": {},
   "source": [
    "# Math 189 Week 10 Summary\n",
    "> NAME: $\\color{blue}{\\text{Kaiwen Bian}}$\n",
    "> \n",
    "> PID: $\\color{blue}{\\text{A17316568}}$\n",
    ">\n",
    "> \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce0cf8",
   "metadata": {},
   "source": [
    "I certify that the following write-up is my own work, and have abided by the UCSD Academic Integrity Guidelines.\n",
    "\n",
    "- [x] Yes\n",
    "- [ ] No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c41b1",
   "metadata": {},
   "source": [
    "% # %load tex-macros\n",
    "<div hidden>\n",
    "\\newcommand{\\require}[1]{}\n",
    "\n",
    "$\\require{begingroup}\\require{newcommand}$\n",
    "$\\long\\def \\forcecommand #1{\\providecommand{#1}{}\\renewcommand{#1}}$\n",
    "$\\forcecommand{\\defeq}{\\stackrel{\\small\\bullet}{=}}$\n",
    "$\\forcecommand{\\ra}{\\rangle}$\n",
    "$\\forcecommand{\\la}{\\langle}$\n",
    "$\\forcecommand{\\pr}{{\\mathbb P}}$\n",
    "$\\forcecommand{\\qr}{{\\mathbb Q}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\av}{{\\boldsymbol{a}}}$\n",
    "$\\forcecommand{\\bv}{{\\boldsymbol{b}}}$\n",
    "$\\forcecommand{\\cv}{{\\boldsymbol{c}}}$\n",
    "$\\forcecommand{\\dv}{{\\boldsymbol{d}}}$\n",
    "$\\forcecommand{\\ev}{{\\boldsymbol{e}}}$\n",
    "$\\forcecommand{\\fv}{{\\boldsymbol{f}}}$\n",
    "$\\forcecommand{\\gv}{{\\boldsymbol{g}}}$\n",
    "$\\forcecommand{\\hv}{{\\boldsymbol{h}}}$\n",
    "$\\forcecommand{\\nv}{{\\boldsymbol{n}}}$\n",
    "$\\forcecommand{\\sv}{{\\boldsymbol{s}}}$\n",
    "$\\forcecommand{\\tv}{{\\boldsymbol{t}}}$\n",
    "$\\forcecommand{\\uv}{{\\boldsymbol{u}}}$\n",
    "$\\forcecommand{\\vv}{{\\boldsymbol{v}}}$\n",
    "$\\forcecommand{\\wv}{{\\boldsymbol{w}}}$\n",
    "$\\forcecommand{\\zerov}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\onev}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\phiv}{{\\boldsymbol{\\phi}}}$\n",
    "$\\forcecommand{\\cc}{{\\check{C}}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\Xv}{{\\boldsymbol{X}\\!}}$\n",
    "$\\forcecommand{\\yv}{{\\boldsymbol{y}}}$\n",
    "$\\forcecommand{\\Yv}{{\\boldsymbol{Y}}}$\n",
    "$\\forcecommand{\\zv}{{\\boldsymbol{z}}}$\n",
    "$\\forcecommand{\\Zv}{{\\boldsymbol{Z}}}$\n",
    "$\\forcecommand{\\Iv}{{\\boldsymbol{I}}}$\n",
    "$\\forcecommand{\\Jv}{{\\boldsymbol{J}}}$\n",
    "$\\forcecommand{\\Cv}{{\\boldsymbol{C}}}$\n",
    "$\\forcecommand{\\Ev}{{\\boldsymbol{E}}}$\n",
    "$\\forcecommand{\\Fv}{{\\boldsymbol{F}}}$\n",
    "$\\forcecommand{\\Gv}{{\\boldsymbol{G}}}$\n",
    "$\\forcecommand{\\Hv}{{\\boldsymbol{H}}}$\n",
    "$\\forcecommand{\\alphav}{{\\boldsymbol{\\alpha}}}$\n",
    "$\\forcecommand{\\epsilonv}{{\\boldsymbol{\\epsilon}}}$\n",
    "$\\forcecommand{\\betav}{{\\boldsymbol{\\beta}}}$\n",
    "$\\forcecommand{\\deltav}{{\\boldsymbol{\\delta}}}$\n",
    "$\\forcecommand{\\gammav}{{\\boldsymbol{\\gamma}}}$\n",
    "$\\forcecommand{\\etav}{{\\boldsymbol{\\eta}}}$\n",
    "$\\forcecommand{\\piv}{{\\boldsymbol{\\pi}}}$\n",
    "$\\forcecommand{\\thetav}{{\\boldsymbol{\\theta}}}$\n",
    "$\\forcecommand{\\tauv}{{\\boldsymbol{\\tau}}}$\n",
    "$\\forcecommand{\\muv}{{\\boldsymbol{\\mu}}}$\n",
    "$%$\n",
    "$\\forcecommand{\\sd}{\\text{SD}}$\n",
    "$\\forcecommand{\\se}{\\text{SE}}$\n",
    "$\\forcecommand{\\med}{\\text{median}}$\n",
    "$\\forcecommand{\\median}{\\text{median}}$\n",
    "$%$\n",
    "$\\forcecommand{\\supp}{\\text{supp}}$\n",
    "$\\forcecommand{\\E}{\\mathbb{E}}$\n",
    "$\\forcecommand{\\var}{\\text{Var}}$\n",
    "$\\forcecommand{\\Ber}{{\\text{Ber}}}$\n",
    "$\\forcecommand{\\Bin}{{\\text{Bin}}}$\n",
    "$\\forcecommand{\\Geo}{{\\text{Geo}}}$\n",
    "$\\forcecommand{\\Unif}{{\\text{Unif}}}$\n",
    "$\\forcecommand{\\Poi}{{\\text{Poi}}}$\n",
    "$\\forcecommand{\\Exp}{{\\text{Exp}}}$\n",
    "$\\forcecommand{\\Chisq}{{\\chi^2}}$\n",
    "$\\forcecommand{\\N}{\\mathbb{N}}$\n",
    "$\\forcecommand{\\iid}{{\\stackrel{iid}{\\sim}}}$\n",
    "$\\forcecommand{\\px}{p_{X}}$\n",
    "$\\forcecommand{\\fx}{f_{X}}$\n",
    "$\\forcecommand{\\Fx}{F_{X}}$\n",
    "$\\forcecommand{\\py}{p_{Y}}$\n",
    "$\\forcecommand{\\pxy}{p_{X,Y}}$\n",
    "$\\forcecommand{\\po}{{p_0}}$\n",
    "$\\forcecommand{\\pa}{{p_a}}$\n",
    "$\\forcecommand{\\Xbar}{\\overline{X}}$\n",
    "$\\forcecommand{\\Ybar}{\\overline{Y}}$\n",
    "$\\forcecommand{\\Zbar}{\\overline{Z}}$\n",
    "$\\forcecommand{\\nXbar}{n \\cdot \\overline{X}}$\n",
    "$\\forcecommand{\\nYbar}{n \\cdot \\overline{Y}}$\n",
    "$\\forcecommand{\\nZbar}{n \\cdot \\overline{Z}}$\n",
    "$\\forcecommand{\\Xn}{X_1, X_2, \\dots, X_n}$\n",
    "$\\forcecommand{\\Xm}{{X_1, X_2, \\dots, X_m}}$\n",
    "$\\forcecommand{\\Yn}{Y_1, Y_2, \\dots, Y_n}$\n",
    "$\\forcecommand{\\Ym}{{Y_1, Y_2, \\dots, Y_m}}$\n",
    "$\\forcecommand{\\sumXn}{X_1 + X_2 + \\dots + X_n}$\n",
    "$\\forcecommand{\\sumym}{Y_1 + Y_2 + \\dots + Y_m}$\n",
    "$\\forcecommand{\\la}{\\ell_\\alpha}$\n",
    "$\\forcecommand{\\ua}{u_\\alpha}$\n",
    "$\\forcecommand{\\at}{{\\alpha/2}}$\n",
    "$\\forcecommand{\\mux}{\\mu_{X}}$\n",
    "$\\forcecommand{\\muy}{\\mu_{Y}}$\n",
    "$\\forcecommand{\\sx}{\\sigma_{X}}$\n",
    "$\\forcecommand{\\sy}{\\sigma_{Y}}$\n",
    "$\\forcecommand{\\ci}{\\text{CI}}$\n",
    "$\\forcecommand{\\pvalue}{$p$-value}$\n",
    "$\\forcecommand{\\Ho}{H_{0}}$\n",
    "$\\forcecommand{\\Ha}{H_{a}}$\n",
    "\n",
    "\\vskip-\\parskip\n",
    "\\vskip-\\baselineskip\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f9025a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "# pio.renderers.default='notebook'\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# Optional \n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "matplotlib.rcParams['figure.figsize'] = 7, 7\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4b4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def plot_X(X, ax, type='pmf', **kwargs):\n",
    "    ax.set_xlabel('Support')\n",
    "    ax.set_title(f'{X.dist.name}{X.args}')\n",
    "    \n",
    "    min_X, max_X = X.ppf((1e-3, 1-1e-3))\n",
    "    supp_X = np.linspace(min_X-1, max_X + 1, 200)\n",
    "    \n",
    "    if type == 'pmf':\n",
    "        supp_X = np.arange(min_X-1, max_X + 1)\n",
    "        ax.bar(supp_X, X.pmf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PMF')\n",
    "    elif type == 'pdf':\n",
    "        ax.plot(supp_X, X.pdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PDF')\n",
    "    elif type == 'cdf':\n",
    "        ax.plot(supp_X, X.cdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('CDF')\n",
    "    else:\n",
    "        raise ValueError('type must be pmf or cdf')\n",
    "\n",
    "def decision(pvalue, alpha):\n",
    "    if pvalue < alpha:\n",
    "        print(f'reject H0: pvalue={pvalue} < {alpha}')  \n",
    "    else: \n",
    "        print(f'fail to reject H0: pvalue={pvalue} â‰¥ {alpha}')\n",
    "\n",
    "def standardize(X):\n",
    "    return (X - X.mean()) / X.std()\n",
    "\n",
    "\n",
    "def make_data(errors):\n",
    "    n = len(errors)\n",
    "    x1 = np.linspace(0, 1, n)\n",
    "    x2 = np.random.rand(n)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'x1': x1, \n",
    "        'x2': x2, \n",
    "        'y': 2 + 3*x1 + 4*x2 + errors\n",
    "    })\n",
    "\n",
    "def plot_regression(data, fit, residuals=True):\n",
    "    b = fit.params\n",
    "    b0, b1, b2 = *b, *np.zeros(3 - len(b))\n",
    "    y, x1, x2 = data.y, data.x1, data.x2\n",
    "    fig = px.scatter_3d(x=x1, y=x2, z=y)\n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "            xaxis_title='X1',\n",
    "            yaxis_title='X2',\n",
    "            zaxis_title='Y'),\n",
    "            margin=dict(l=0, r=0, b=0, t=0\n",
    "        )\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    \n",
    "    x1_grid, x2_grid = np.meshgrid(x1, x2)\n",
    "    yhat = b0 + (b1 * x1_grid) + (b2 * x2_grid)\n",
    "    fig.add_trace(\n",
    "        go.Surface(x=x1_grid, y=x2_grid, z=yhat, opacity=0.5,colorscale='Gray')\n",
    "    )\n",
    "    if residuals:\n",
    "        for i in range(len(x1)):\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(x=[x1[i], x1[i]], y=[x2[i], x2[i]], z=[b0 + b1*x1[i] + b2*x2[i], y[i]], mode='lines', line=dict(color='black', width=2))\n",
    "            )\n",
    "    fig.update_layout(showlegend=False, scene_camera=dict(eye=dict(x=2.0, y=0.5, z=0.1)))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6dc53",
   "metadata": {},
   "source": [
    "## Key Takeaways from Week 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69de7b",
   "metadata": {},
   "source": [
    "#### Thursday\n",
    "\n",
    "... insert your takeaway here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaadbca8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071d8aa",
   "metadata": {},
   "source": [
    "# Prediction Intervals\n",
    "Previously we ahve pretty much looked at simple models, models that we can perform **uncertainty quantification** on the parameter of the model:\n",
    "\n",
    "All of these model are `creating manifolds on the data` by using parameter, we test our hypothesis on the parameters of the model:\n",
    "\n",
    "- Confidence intervals for the coefficients\n",
    "- Hypothesis tests for the coefficients\n",
    "- Hypothesis tests for linear functions of the coefficients\n",
    "\n",
    "Can we **quantify the uncertainties in the predictions themselves?**: `predictions themeselves are uncertain!`\n",
    "\n",
    "- Confidence interval for the mean response\n",
    "- Prediction interval for individual response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11abc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(43)\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "x = np.random.rand(num_samples)\n",
    "\n",
    "# true linear relationship\n",
    "true_slope = 2\n",
    "true_intercept = 1\n",
    "y = true_slope * x + true_intercept + np.random.normal(scale=0.2, size=num_samples)\n",
    "\n",
    "# introduce some outliers\n",
    "num_outliers = 10\n",
    "outlier_indices = np.random.choice(num_samples, num_outliers, replace=False)\n",
    "y_outliers = y[outlier_indices] + np.random.normal(scale=1.0, size=num_outliers)\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df_new = pd.DataFrame({'x': x[outlier_indices], 'y': y_outliers})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.scatter(df_new['x'], df_new['y'], color='red')\n",
    "ax.plot(df['x'], true_intercept + true_slope * df['x'], color='green', ls='--', lw=0.5, label='True line')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a40cc",
   "metadata": {},
   "source": [
    "### Confidence intervals for response in expectation\n",
    "\n",
    "A confidence interval for the mean response $\\mathbb{E}(y_0 \\mid x_0)$ at a given point $x_0$ is an interval $[\\ell_{\\alpha}, u_{\\alpha}]$ such that\n",
    "\n",
    "$$\n",
    "P(\\ell_{\\alpha} \\leq \\mathbb{E}(y_0 \\mid x_0) \\leq u_{\\alpha}) = 1 - \\alpha\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the confidence level. **It try to give the expected value of the predictions, which accounts in the noises**ã€‚\n",
    "- It tries to capture the mean response as much as possible!\n",
    "- 0.9 CI is a random interval that cpatures 90% of the predictions\n",
    "- Quantify the unceratinty of the predictions you make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ecb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols('y~x', df).fit()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.scatter(df_new['x'], df_new['y'], color='red', label='New data')\n",
    "ax.plot(df['x'], 1 + 2*df['x'], color='green', lw=0.5, ls='--', label='True line')\n",
    "ax.plot(df['x'], model.fittedvalues, label='fitted line', color='black')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcae5a0",
   "metadata": {},
   "source": [
    "### Prediction intervals for individual responses\n",
    "\n",
    "A prediction interval for the response $y_0$ at a given point $x_0$ is an interval $[\\ell_{\\alpha}, u_{\\alpha}]$ such that\n",
    "\n",
    "$$\n",
    "P(\\ell_{\\alpha} \\leq y_0 \\leq u_{\\alpha}) = 1 - \\alpha\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the confidence level. **It try to give the noises in the predictions**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953bf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pi = df.copy()\n",
    "df_pi_new = df_new.copy()\n",
    "\n",
    "model = smf.ols('y ~ x', df_pi).fit()\n",
    "\n",
    "@interact(alpha=(0.01, 0.5, 0.01))\n",
    "def plot_intervals(alpha):\n",
    "    preds = model.get_prediction(df_pi).summary_frame(alpha=alpha)\n",
    "    # preds = model.get_prediction(df_pi_new).summary_frame(alpha=alpha)\n",
    "\n",
    "    # plot confidence interval\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.scatter(df_pi['x'], df_pi['y'])\n",
    "    ax.scatter(df_pi_new['x'], df_pi_new['y'], color='red', label='New data')\n",
    "    ax.plot(df_pi['x'], 1 + 2*df_pi['x'], color='green', lw=0.5, ls='--', label='True line')\n",
    "    ax.plot(df_pi['x'], model.fittedvalues, c='black', label='fitted line')\n",
    "\n",
    "#     ax.fill_between(df_pi['x'], preds['mean_ci_lower'], preds['mean_ci_upper'], color='blue', alpha=0.2)\n",
    "    ax.fill_between(df_pi['x'], preds['obs_ci_lower'], preds['obs_ci_upper'], color='red', alpha=0.2)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(0.5, 3.5)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d585bd2",
   "metadata": {},
   "source": [
    "Another more complicated example with more complicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 300\n",
    "x = np.linspace(-3, 3, num_samples)\n",
    "\n",
    "# true nonlinear function\n",
    "fx = lambda x: x**2 + 10 * np.sin(x**2) + np.exp(x)\n",
    "\n",
    "# generate y values with noise\n",
    "y = fx(x) + np.random.normal(scale=1.0, size=num_samples)\n",
    "\n",
    "# introduce some outliers\n",
    "num_outliers = 30\n",
    "outlier_indices = np.random.choice(num_samples, num_outliers, replace=False)\n",
    "y_outliers = y[outlier_indices] + np.random.normal(scale=5.0, size=num_outliers)\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y}).sort_values('x')\n",
    "df_new = pd.DataFrame({'x': x[outlier_indices], 'y': y_outliers}).sort_values('x')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.plot(df['x'], fx(df['x']), color='green', lw=1, ls='--', label='True function')\n",
    "ax.scatter(df_new['x'], df_new['y'], color='red')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48601b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols('y ~ x', df).fit()\n",
    "\n",
    "@interact(alpha=(0.01, 0.5, 0.01))\n",
    "def plot_intervals(alpha):\n",
    "    preds = model.get_prediction(df).summary_frame(alpha=alpha)\n",
    "    # preds = model.get_prediction(df_new).summary_frame(alpha=alpha)\n",
    "\n",
    "    # plot confidence interval\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.scatter(df['x'], df['y'])\n",
    "    ax.scatter(df_new['x'], df_new['y'], color='red', label='New data')\n",
    "    ax.plot(df['x'].sort_values(), fx(df['x'].sort_values()), color='green', lw=0.5, ls='--', label='True line')\n",
    "    ax.plot(df['x'], model.fittedvalues, c='black', label='fitted line')\n",
    "\n",
    "    # ax.fill_between(df['x'], preds['mean_ci_lower'], preds['mean_ci_upper'], color='blue', alpha=0.2)\n",
    "    ax.fill_between(df['x'], preds['obs_ci_lower'], preds['obs_ci_upper'], color='red', alpha=0.2)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlim(-3.5, 3.5)\n",
    "    # ax.set_ylim(0.5, 3.5)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd786a01",
   "metadata": {},
   "source": [
    "Obviously, this prediction interval doesn't seem to be that good, we need some alternative ways to try to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a6afc",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4918a6a0",
   "metadata": {},
   "source": [
    "# Statistical Leraning Theory\n",
    "We will be looking at a very traditional perspective on machine learning from the `statistical learning` paradigm. This is where ERM comes from as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9065d3d",
   "metadata": {},
   "source": [
    "> Goal:\n",
    "\n",
    "Find an appropriate model $f$ which fits $y$, i.e.,\n",
    "\n",
    "$$\n",
    "y = f(X_1, X_2, \\dots, X_p)\n",
    "$$\n",
    "\n",
    "> Statistical Learning:\n",
    "\n",
    "$$\n",
    "\\min_{f \\in \\mathcal{F}} \\sum_{i=1}^{n} L \\left( y_i, f(X_{i1}, \\dots, X_{ip}) \\right) + \\lambda p(f)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $L$ is the loss function,\n",
    "- $\\mathcal{F}$ is the set of possible models,\n",
    "- $\\lambda p(f)$ is a penalty term to prevent overfitting.\n",
    "\n",
    "\n",
    "**This is the `Empirical Risk Minimization` paradigm in statistical learning theory**. Actually, most of traditional machine elarning comes from paradigms in statistical learning (i.e. SVM tries to reproduce margin minimization on hillbert space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffbfa16",
   "metadata": {},
   "source": [
    "Much of the course has been focused on the following examples:\n",
    "\n",
    "| Problem                      | $y$                  | $f \\in \\mathcal{F}$                            | $\\mathcal{L}$                                            | $p$                                      |\n",
    "|------------------------------|----------------------|--------------------------------|------------------------------------------------|--------------------------------|\n",
    "| **Regression**               | $\\mathbb{R}$         | $\\beta_0 + x^T \\beta_1$       | $\\|y - f(X)\\|^2$                              | $0$                        |\n",
    "| **Logistic**                 | $\\{0,1\\}$           | $\\sigma(\\beta_0 + x^T \\beta_1)$ | $y \\log(f(X)) + (1 - y) \\log(1 - f(X))$      | $0$                        |\n",
    "| **Multinomial Logistic**     | $\\{C_1 \\dots C_k\\}$ | $\\text{softmax}(B_{0,1:k} + x^T B_{1,1:k})$ | $\\text{OneHot}(y) \\cdot \\log(f(X))$ | $0$                        |\n",
    "| **Ridge**                    | $\\mathbb{R}$         | $\\beta_0 + x^T \\beta_1$       | $\\|y - f(X)\\|^2$                              | $\\|\\beta_1\\|^2$              |\n",
    "| **LASSO**                    | $\\mathbb{R}$         | $\\beta_0 + x^T \\beta_1$       | $\\|y - f(X)\\|^2$                              | $\\|\\beta_1\\|_1$              |\n",
    "| **Elastic Net**              | $\\mathbb{R}$         | $\\beta_0 + x^T \\beta_1$       | $\\|y - f(X)\\|^2$                              | $\\gamma \\|\\beta_1\\|_1 + (1 - \\gamma) \\|\\beta_1\\|^2$ |\n",
    "| **Logistic + Elastic Net**   | $\\{0,1\\}$           | $\\sigma(\\beta_0 + x^T \\beta_1)$ | $y \\log(f(X)) + (1 - y) \\log(1 - f(X))$      | $\\gamma \\|\\beta_1\\|_1 + (1 - \\gamma) \\|\\beta_1\\|^2$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eaa162",
   "metadata": {},
   "source": [
    "Most supervised learning problems can be cast into the same framework:\n",
    "\n",
    "| Problem                  | $y$                  | $f \\in \\mathcal{F}$                                      | $\\mathcal{L}$                                        | $p$  |\n",
    "|--------------------------|----------------------|------------------------------------------------|------------------------------------------------|----|\n",
    "| **Regression**           | $\\mathbb{R}$         | $f(X) = \\beta_0 + x^T \\beta_1$                 | $\\|y - f(X)\\|^2$                              | $0$ |\n",
    "| **Logistic**             | $\\{0,1\\}$           | $f(X) = \\sigma(\\beta_0 + x^T \\beta_1)$         | $y \\log(f(X)) + (1 - y) \\log(1 - f(X))$      | $0$ |\n",
    "| **SVM Classification**   | $\\{-1,1\\}$          | $f(X) = \\sum_{i=1}^{n} K_{\\sigma}(\\cdot, X_i)$ | $\\max(0, 1 - y f(X))$                         | $\\|\\beta_1\\|^2$ |\n",
    "| **MLP**                 | $\\mathbb{R}$         | $f(X) = \\psi(W_2 \\psi(W_1 x + b_1) + b_2)$     | $\\|y - f(X)\\|^2$                              | $0$ |\n",
    "| **MLP Classification**   | $\\{0,1\\}$           | $f(X) = \\text{softmax}(\\psi(W_2 \\psi(W_1 x + b_1) + b_2))$ | $\\text{OneHot}(y) \\cdot \\log(f(X))$ | $0$ |\n",
    "| **DNNs**                | $\\mathbb{R}$         | $f(X) = \\psi(W_D \\circ \\dots \\circ (W_1 x + b_1)) + \\dots + b_D$ | $\\|y - f(X)\\|^2$ | $0$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9489b073",
   "metadata": {},
   "source": [
    "Let's look at an example of support vector machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3baabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df['x'].values.reshape(-1, 1)\n",
    "y = df['y']\n",
    "\n",
    "svr = make_pipeline(StandardScaler(), SVR(C=1e4, epsilon=1.0))\n",
    "svr.fit(X, y)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.scatter(df_new['x'], df_new['y'], color='red')\n",
    "ax.plot(df['x'], fx(df['x']), color='green', lw=1, ls='--', label='True function')\n",
    "ax.plot(df['x'], svr.predict(X), color='black', lw=1, label='SVM fit')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf50e10",
   "metadata": {},
   "source": [
    "Let's do the same with a neural network and visualzie the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "X = torch.tensor(df['x'].values).float().reshape(-1, 1)\n",
    "Y = torch.tensor(df['y'].values).float().reshape(-1, 1)\n",
    "\n",
    "model_linear = nn.Linear(1, 1)\n",
    "model_dnn = nn.Sequential(\n",
    "    nn.Linear(1, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 1)\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_linear = torch.optim.Adam(model_linear.parameters(), lr=0.05)\n",
    "optimizer_dnn = torch.optim.Adam(model_dnn.parameters(), lr=0.05)\n",
    "\n",
    "# Visualization setup\n",
    "epochs_to_plot = [0, 10, 50, 99, 150, 299]  # Chosen epochs to visualize\n",
    "predictions_linear = {}\n",
    "predictions_dnn = {}\n",
    "\n",
    "for epoch in range(300):\n",
    "    optimizer_linear.zero_grad()\n",
    "    optimizer_dnn.zero_grad()\n",
    "\n",
    "    output_linear = model_linear(X)\n",
    "    output_dnn = model_dnn(X)\n",
    "\n",
    "    loss_linear = criterion(output_linear, Y)\n",
    "    loss_dnn = criterion(output_dnn, Y)\n",
    "\n",
    "    print(f'Epoch {epoch}, Loss linear: {loss_linear.item():.4f}, Loss DNN: {loss_dnn.item():.4f}')\n",
    "\n",
    "    loss_linear.backward()\n",
    "    loss_dnn.backward()\n",
    "\n",
    "    optimizer_linear.step()\n",
    "    optimizer_dnn.step()\n",
    "\n",
    "    # Store predictions at specific epochs\n",
    "    if epoch in epochs_to_plot:\n",
    "        predictions_linear[epoch] = output_linear.detach().numpy()\n",
    "        predictions_dnn[epoch] = output_dnn.detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "for i, epoch in enumerate(epochs_to_plot):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(df['x'], df['y'], alpha=0.3, label='True Data')\n",
    "    ax.plot(df['x'], predictions_linear[epoch], color='blue', lw=1, label=f'Linear Model (Epoch {epoch})')\n",
    "    ax.plot(df['x'], predictions_dnn[epoch], color='red', lw=1, label=f'DNN Model (Epoch {epoch})')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'Epoch {epoch}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf69ccc",
   "metadata": {},
   "source": [
    "However, we bump into an issue. Some sort of inference can be done on the learned model, but it gets increasingly more difficult as the complexity of the model increases.\n",
    "\n",
    "Types of inference:\n",
    "- **Variable Importance**: Which variables are most important in predicting the response? (**No uncertainty quantification** ðŸ˜ž)\n",
    "- **Confidence Intervals**: How confident are we in our predictions?\n",
    "\n",
    "\n",
    "We lose out some of the **inferential** ability we have in the simpler models. However, in the past 5-6 years, there came out an new method of doing `uncertainty quantification`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ee6ee",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3dbaf3",
   "metadata": {},
   "source": [
    "# Conformal Prediction Overview\n",
    "\n",
    "Now, we don't make any assumptions on the underlaying model, but rather treat it as a `black-box` directly! Ideas dates back to the 90th!\n",
    "\n",
    "- Conformal inference is a framework for **constructing prediction intervals and confidence regions for black-box machine learning models**.\n",
    "\n",
    "- It doesnâ€™t rely on any assumptions about the underlying model, and can be used with any machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc33ad9",
   "metadata": {},
   "source": [
    "#### 1. Split the Data:\n",
    "- Split the dataset into two independent subsets:\n",
    "  - **Training Set** ($D_{\\text{train}}$): Used to fit the model.\n",
    "  - **Calibration Set** ($D_{\\text{cal}}$): Used to calibrate the prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b3d83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_cal, y_train, y_cal = train_test_split(df['x'], df['y'], test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abce656",
   "metadata": {},
   "source": [
    "#### 2. Fit the Model:\n",
    "- Use the training set $D_{\\text{train}}$ to fit your predictive model.\n",
    "- The choice of model depends on the problem (e.g., linear regression, decision tree, neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d119048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit SVM on training data\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_cal = X_cal.values.reshape(-1, 1)\n",
    "\n",
    "svr = make_pipeline(StandardScaler(), SVR(C=1e4, epsilon=1.0))\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e592326",
   "metadata": {},
   "source": [
    "#### 3. Generate Predictions:\n",
    "- Apply the fitted model to the calibration set $D_{\\text{cal}}$ to obtain predictions.\n",
    "- For each instance in the calibration set, compute the prediction error:\n",
    "  - **Error**: $e_i = | y_i - \\hat{y}_i |$\n",
    "  - Where $y_i$ is the actual value and $\\hat{y}_i$ is the predicted value for the $i$-th observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d94bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = svr.predict(X_cal)\n",
    "\n",
    "errors = y_cal - yhat\n",
    "\n",
    "plt.hist(errors, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87193ccf",
   "metadata": {},
   "source": [
    "#### 4. Calculate Conformity Scores:\n",
    "- Use a conformity measure to calculate a score for each prediction in the calibration set.\n",
    "- A common choice for the conformity score is the absolute prediction error, as calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4597ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort errors\n",
    "conf_score = np.sort(errors)\n",
    "\n",
    "# get the empirical 90% interval\n",
    "alpha = 0.1\n",
    "lower = conf_score[int(alpha/2 * len(conf_score))]\n",
    "upper = conf_score[int((1 - alpha/2) * len(conf_score))]\n",
    "lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96250ae7",
   "metadata": {},
   "source": [
    "#### 5. Determine the **Prediction Interval**:\n",
    "- Sort the conformity scores in ascending order.\n",
    "- Determine the $(1 - \\alpha)$-quantile of the conformity scores, where $\\alpha$ is the significance level (e.g., $0.05$ for $95\\%$ coverage).\n",
    "- The $(1 - \\alpha)$-quantile is the threshold value $q$.\n",
    "\n",
    "#### 6. Construct **Prediction Intervals**:\n",
    "- For a new data point $x$, after fitting the model on $D_{\\text{train}}$, predict the value $\\hat{y}(x)$.\n",
    "- Construct the prediction interval as $[\\hat{y} - q, \\hat{y} + q]$, ensuring that the interval will cover the true value of $y$ with probability at least $(1 - \\alpha)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = svr.predict(df['x'].sort_values().values.reshape(-1, 1))\n",
    "pred_upper = preds + upper\n",
    "pred_lower = preds + lower\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.plot(df['x'].sort_values(), preds, color='black', lw=1, label='SVM fit')\n",
    "ax.fill_between(df['x'].sort_values(), pred_lower, pred_upper, color='red', alpha=0.2, label='90% PI')\n",
    "ax.scatter(df_new['x'], df_new['y'], color='red', label='New Data')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8371eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(alpha=(0.01, 0.5, 0.01))\n",
    "def plot_intervals(alpha):\n",
    "    X_train, X_cal, y_train, y_cal = train_test_split(df['x'], df['y'], test_size=0.25, random_state=0)\n",
    "    X_train = X_train.values.reshape(-1, 1)\n",
    "    X_cal = X_cal.values.reshape(-1, 1)\n",
    "    svr = make_pipeline(StandardScaler(), SVR(C=1e3, epsilon=1.0))\n",
    "    svr.fit(X_train, y_train)\n",
    "    yhat = svr.predict(X_cal)\n",
    "    errors = y_cal - yhat\n",
    "    conf_score = np.sort(errors)\n",
    "    lower = conf_score[int(alpha/2 * len(conf_score))]\n",
    "    upper = conf_score[int((1 - alpha/2) * len(conf_score))]\n",
    "    preds = svr.predict(df['x'].sort_values().values.reshape(-1, 1))\n",
    "    pred_upper = preds + upper\n",
    "    pred_lower = preds + lower\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.scatter(df['x'], df['y'])\n",
    "    ax.plot(df['x'].sort_values(), preds, color='black', lw=1, label='SVM fit')\n",
    "    ax.fill_between(df['x'].sort_values(), pred_lower, pred_upper, color='red', alpha=0.2, label=f'{100 * (1-alpha)}% PI')\n",
    "    ax.scatter(df_new['x'], df_new['y'], color='red', label='New Data')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53422a62",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e365f",
   "metadata": {},
   "source": [
    "# Conformal Predcition Intuition\n",
    "\n",
    "Single predictor give  you a scaler value, but conformal predictor give you a set, similar to confidence or prediction interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8f864",
   "metadata": {},
   "source": [
    "More formally, let\n",
    "\n",
    "$$D_n = \\{(x_1, y_1), \\dots, (x_n, y_n)\\}$$\n",
    "\n",
    "be a dataset of $n$ observations, where $x_i \\in \\mathcal{X}$ and $y_i \\in \\mathcal{Y}$.\n",
    "\n",
    "| Model | $\\mathcal{X}$ | $\\mathcal{Y}$ |\n",
    "|--------|------------|------------|\n",
    "| Simple Regression | $\\mathbb{R}$ | $\\mathbb{R}$ |\n",
    "| Multiple Regression, SVMs, Neural Networks, etc. | $\\mathbb{R}^p$ | $\\mathbb{R}$ |\n",
    "| Logistic Regression, or any binary classifier | $\\mathbb{R}^p$ | $\\{0,1\\}$ |\n",
    "| Multinomial Logistic Regression, or any $k$-class classifier | $\\mathbb{R}^p$ | $\\{C_1, C_2, \\dots, C_k\\}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176a0cb",
   "metadata": {},
   "source": [
    "For $\\alpha \\in (0,1)$, a **conformal predictor** is a function $\\Gamma_\\alpha$ that takes as input:\n",
    "\n",
    "- A dataset $D_n$,\n",
    "- A new observation $x \\in \\mathcal{X}$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- A prediction set $\\Gamma_\\alpha(x) \\subset \\mathcal{Y}$ which is **not the same** as $\\Gamma_\\alpha \\in \\mathcal{Y}$\n",
    "\n",
    "such that for a new observation $(x_{n+1}, y_{n+1})$, the following holds:\n",
    "\n",
    "$$ P \\left( y_{n+1} \\in \\Gamma_\\alpha(x_{n+1}) \\right) \\geq 1 - \\alpha $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390a3a1",
   "metadata": {},
   "source": [
    "**So we try to predict a set and talk about the probability of the true value $y$ that will be showing up in this prediction set $\\Gamma_\\alpha(x_{n+1})$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a1b15",
   "metadata": {},
   "source": [
    "## Inductive Conformal Prediction\n",
    "\n",
    "Developed different from traditional ideas of probability using measure theory. Instead, it is purely developed from the idea of game theory and was later on discovered to have great usage for machine learning and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c2c96",
   "metadata": {},
   "source": [
    "With this background, the arguably most popular conformal prediction method is the **Inductive Conformal Prediction** method. The procedure is as follows:\n",
    "\n",
    "> #### Step 1: Given data\n",
    "> $$ D_n = \\{(x_1, y_1), \\dots, (x_n, y_n)\\} $$\n",
    ">\n",
    "> split the data into two parts:\n",
    ">\n",
    "> - **Training Set** ($D_{\\text{train}}$): Used to fit the model.\n",
    "> - **Calibration Set** ($D_{\\text{cal}}$): Used to calibrate the prediction intervals such that\n",
    ">\n",
    ">  $$ D_{\\text{train}} \\cap D_{\\text{cal}} = \\emptyset \\quad \\text{and} \\quad D_{\\text{train}} \\cup D_{\\text{cal}} = D_n $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6818ffc",
   "metadata": {},
   "source": [
    "> #### Step 2: Fit the model on the training set \n",
    "> Fit the model on the training set $D_{\\text{train}}$ to obtain a prediction function $\\hat{f}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff503647",
   "metadata": {},
   "source": [
    "> #### Step 3: Apply the prediction function \n",
    "> Apply the prediction function $\\hat{f}$ to the calibration set $D_{\\text{cal}}$ to obtain predictions $\\hat{y}_i$ for each $x_i$ in the calibration > set, i.e.,\n",
    ">\n",
    "> $$ \\hat{y}_i = \\hat{f}(x_i) \\quad \\text{for} \\quad (x_i, y_i) \\in D_{\\text{cal}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de6c14",
   "metadata": {},
   "source": [
    "> #### Step 4: Define a heuristic (non)-conformity measure\n",
    ">\n",
    "> Define a (non)-conformity measure $s(y, x)$ which measures how well the prediction $\\hat{y}_i$ fits the actual value $y_i$.\n",
    "> \n",
    "> **Note:** this must take into account the nature of the model $f$, for example:\n",
    ">\n",
    ">  If $\\mathcal{Y} = \\mathbb{R}$ (Linear Regression) then:\n",
    ">  - $s(y_i, \\hat{y}) = |y_i - \\hat{y}_i|$ \n",
    ">  - $s(y_i, \\hat{y}) = (y_i - \\hat{y}_i)^2$.\n",
    ">  \n",
    "> If $\\mathcal{Y} = \\{0,1\\}$ and $f(x) = (p_0, p_1)$, (Logistic Regression) then:\n",
    "> - $s(y, x) = y \\log(p_1) + (1 - y) \\log(p_0)$.\n",
    ">\n",
    "> If $\\mathcal{Y} = \\{C_1, C_2, \\dots, C_k\\}$ and $f(x) = (p_1, p_2, \\dots, p_k)$, (Multi-nomial Logistic Regression) then:\n",
    "> - $s(y, x) = -\\log(p_y)$.\n",
    ">\n",
    "> This would give as a ranked set of all the predictions we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528fc28",
   "metadata": {},
   "source": [
    "This conformity score sort of resembles the functionality of loss function as well. It is essentially a way of **quantifying how well our predictions are**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cbd7fd",
   "metadata": {},
   "source": [
    "> #### Step 5: Compute non-conformity scores and quantile\n",
    ">\n",
    "> For each $x_i$ in the calibration set, compute the **non-conformity score $s(y_i, \\hat{y}_i)$** that quantifies how \"unusual\" the true label is relative to the modelâ€™s prediction, i.e.,\n",
    ">\n",
    "> $$\n",
    "> s_i = s(y_i, \\hat{y}_i) \\quad \\text{for} \\quad (x_i, y_i) \\in \\mathcal{D}_{\\text{cal}}\n",
    "> $$\n",
    ">\n",
    "> For the **desired coverage** $\\alpha$, to construct a confidence set, we must determine the threshold, which ensures that at least $1 - \\alpha$ fraction of future predictions fall within this threshold. Then we just compute the $\\lfloor (1 - \\alpha) \\cdot n_{\\text{cal}} \\rfloor / n_{\\text{cal}}$-th quantile of the scores $\\{s_i\\}$, i.e.,\n",
    ">\n",
    "> $$\n",
    "> \\hat{q} = \\text{quantile} \\left( \\frac{\\lfloor (1 - \\alpha) \\cdot n_{\\text{cal}} \\rfloor}{n_{\\text{cal}}} ; \\{s_i\\} \\right)\n",
    "> $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809bfd32",
   "metadata": {},
   "source": [
    "> #### Step 6: Compute the Prediction Set\n",
    ">\n",
    "> Given $\\hat{q}$ from step 5, the prediction set for a new observation $x$ is the ones that are beneath this quantile:\n",
    ">\n",
    "> $$\n",
    "> \\Gamma_{\\alpha}(x) = \\{ y \\in \\mathcal{Y} \\mid s(y, \\hat{f}(x)) \\leq \\hat{q} \\}\n",
    "> $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce3a40",
   "metadata": {},
   "source": [
    "Notice that no matter waht your model is, blackbox or not, you can do these 6 steps and you can quantify teh uncertainty of your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772aa907",
   "metadata": {},
   "source": [
    "***This gives a more robust method of demonstrating predictions, quantifying the uncertainty and coverage probability of these sets -> meaningful method for quantifying uncertainty.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb80927",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf4b6b",
   "metadata": {},
   "source": [
    "# MNIST Dataset Example\n",
    "\n",
    "To demonstarte the power of doing comformal inferences, which give us **the ability to reason with uncertainty**, we will be looking at the classic MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e27139",
   "metadata": {},
   "source": [
    "> #### Step 1: Given data\n",
    "> $$ D_n = \\{(x_1, y_1), \\dots, (x_n, y_n)\\} $$\n",
    ">\n",
    "> split the data into two parts:\n",
    ">\n",
    "> - **Training Set** ($D_{\\text{train}}$): Used to fit the model.\n",
    "> - **Calibration Set** ($D_{\\text{cal}}$): Used to calibrate the prediction intervals such that\n",
    ">\n",
    ">  $$ D_{\\text{train}} \\cap D_{\\text{cal}} = \\emptyset \\quad \\text{and} \\quad D_{\\text{train}} \\cup D_{\\text{cal}} = D_n $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "752cfb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "device = 'cpu'\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training data into training and calibration sets\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "calib_size = len(full_train_dataset) - train_size\n",
    "train_dataset, calib_dataset = random_split(full_train_dataset, [train_size, calib_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d90beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = range(11, 20)\n",
    "fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    k = ids[i]\n",
    "    ax.imshow(test_dataset[k][0].squeeze(), cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a83f3",
   "metadata": {},
   "source": [
    "> #### Step 2: Fit the model on the training set \n",
    "> Fit the model on the training set $D_{\\text{train}}$ to obtain a prediction function $\\hat{f}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f92457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cpu'\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "class convNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = convNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a5190",
   "metadata": {},
   "source": [
    "> #### Step 3: Apply the prediction function \n",
    "> Apply the prediction function $\\hat{f}$ to the calibration set $D_{\\text{cal}}$ to obtain predictions $\\hat{y}_i$ for each $x_i$ in the calibration > set, i.e.,\n",
    ">\n",
    "> $$ \\hat{y}_i = \\hat{f}(x_i) \\quad \\text{for} \\quad (x_i, y_i) \\in D_{\\text{cal}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888f674",
   "metadata": {},
   "source": [
    "We try to get as et ofpredictions first instead of just one single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def get_predictions(model, loader):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_outputs.append(probs.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_outputs, axis=0), np.concatenate(all_targets, axis=0)\n",
    "\n",
    "calib_loader = DataLoader(calib_dataset, batch_size=batch_size, shuffle=True)\n",
    "calib_outputs, calib_targets = get_predictions(model, calib_loader)\n",
    "calib_outputs[ids, :].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac3cde",
   "metadata": {},
   "source": [
    "> #### Step 4: Define a heuristic (non)-conformity measure\n",
    ">\n",
    "> Define a (non)-conformity measure $s(y, x)$ which measures how well the prediction $\\hat{y}_i$ fits the actual value $y_i$.\n",
    "> \n",
    "> If $\\mathcal{Y} = \\{ C_1, C_2, \\dots, C_k \\}$ and $f(x) = (p_1, p_2, \\dots, p_k)$, then $s(y, x) = -\\log(p_y)$.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_score = 1 - calib_outputs[np.arange(len(calib_outputs)), calib_targets]\n",
    "conf_score[ids].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efb91d",
   "metadata": {},
   "source": [
    "> #### Step 5: Compute non-conformity scores and quantile\n",
    ">\n",
    "> For each $x_i$ in the calibration set, compute the **non-conformity score $s(y_i, \\hat{y}_i)$** that quantifies how \"unusual\" the true label is relative to the modelâ€™s prediction, i.e.,\n",
    ">\n",
    "> $$\n",
    "> s_i = s(y_i, \\hat{y}_i) \\quad \\text{for} \\quad (x_i, y_i) \\in \\mathcal{D}_{\\text{cal}}\n",
    "> $$\n",
    ">\n",
    "> For the **desired coverage** $\\alpha$, to construct a confidence set, we must determine the threshold, which ensures that at least $1 - \\alpha$ fraction of future predictions fall within this threshold. Then we just compute the $\\lfloor (1 - \\alpha) \\cdot n_{\\text{cal}} \\rfloor / n_{\\text{cal}}$-th quantile of the scores $\\{s_i\\}$, i.e.,\n",
    ">\n",
    "> $$\n",
    "> \\hat{q} = \\text{quantile} \\left( \\frac{\\lfloor (1 - \\alpha) \\cdot n_{\\text{cal}} \\rfloor}{n_{\\text{cal}}} ; \\{s_i\\} \\right)\n",
    "> $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calQuantile(nonconf_scores, alpha):\n",
    "    m = len(nonconf_scores)\n",
    "    level = np.ceil((m + 1) * (1 - alpha)) / m\n",
    "    quantile = np.quantile(nonconf_scores, level, method='higher')\n",
    "    return quantile\n",
    "\n",
    "@interact(alpha=(0.01, 0.5, 0.01))\n",
    "def plot_nonconf_scores(alpha):\n",
    "    quantile = calQuantile(conf_score, alpha)\n",
    "    plt.hist(conf_score, bins=200, density=True)\n",
    "    plt.vlines(quantile, ymin=0, ymax=150, color='red', ls='dashed', \n",
    "               label=f'alpha = {round(alpha, ndigits=2)}')\n",
    "    plt.title(\"Distribution of nonconformity scores\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Scores\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380f221",
   "metadata": {},
   "source": [
    "As $\\alpha$ decreases the cut off of prediction increases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa14d0",
   "metadata": {},
   "source": [
    "> #### Step 6: Compute the Prediction Set\n",
    ">\n",
    "> Given $\\hat{q}$ from step 5, the prediction set for a new observation $x$ is the ones that are beneath this quantile:\n",
    ">\n",
    "> $$\n",
    "> \\Gamma_{\\alpha}(x) = \\{ y \\in \\mathcal{Y} \\mid s(y, \\hat{f}(x)) \\leq \\hat{q} \\}\n",
    "> $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b872a",
   "metadata": {},
   "source": [
    "**Key step is to get the predictions in this prediction set that satisfy this cutoff values of the quantile!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48157989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_set(test_outputs, calib_outputs, calib_targets, alpha):\n",
    "    conf_score = 1 - calib_outputs[np.arange(len(calib_outputs)), calib_targets]\n",
    "    quantile = calQuantile(conf_score, alpha)\n",
    "    pred_set_idx = test_outputs > 1 - quantile\n",
    "    pred_sets = [set(np.where(row)[0]) for row in pred_set_idx]\n",
    "    return pred_sets\n",
    "\n",
    "test_outputs, test_targets = get_predictions(model, test_loader)\n",
    "\n",
    "pred_set = prediction_set(test_outputs, calib_outputs, calib_targets, alpha=0.01)\n",
    "\n",
    "for i in range(11, 20):\n",
    "    print(f'Target: {test_targets[i]}, Prediction set {i}: {pred_set[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "\n",
    "ids = range(11, 20)\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    k = ids[i]\n",
    "    ax.imshow(test_dataset[k][0].squeeze(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Target: {test_targets[k]}, Pred set: {pred_set[k]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6185211",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
