{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d499b8cf",
   "metadata": {},
   "source": [
    "# Math 189 Week 10 Summary\n",
    "> NAME: $\\color{blue}{\\text{Kaiwen Bian}}$\n",
    "> \n",
    "> PID: $\\color{blue}{\\text{A17316568}}$\n",
    ">\n",
    "> \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce0cf8",
   "metadata": {},
   "source": [
    "I certify that the following write-up is my own work, and have abided by the UCSD Academic Integrity Guidelines.\n",
    "\n",
    "- [x] Yes\n",
    "- [ ] No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c41b1",
   "metadata": {},
   "source": [
    "% # %load tex-macros\n",
    "<div hidden>\n",
    "\\newcommand{\\require}[1]{}\n",
    "\n",
    "$\\require{begingroup}\\require{newcommand}$\n",
    "$\\long\\def \\forcecommand #1{\\providecommand{#1}{}\\renewcommand{#1}}$\n",
    "$\\forcecommand{\\defeq}{\\stackrel{\\small\\bullet}{=}}$\n",
    "$\\forcecommand{\\ra}{\\rangle}$\n",
    "$\\forcecommand{\\la}{\\langle}$\n",
    "$\\forcecommand{\\pr}{{\\mathbb P}}$\n",
    "$\\forcecommand{\\qr}{{\\mathbb Q}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\av}{{\\boldsymbol{a}}}$\n",
    "$\\forcecommand{\\bv}{{\\boldsymbol{b}}}$\n",
    "$\\forcecommand{\\cv}{{\\boldsymbol{c}}}$\n",
    "$\\forcecommand{\\dv}{{\\boldsymbol{d}}}$\n",
    "$\\forcecommand{\\ev}{{\\boldsymbol{e}}}$\n",
    "$\\forcecommand{\\fv}{{\\boldsymbol{f}}}$\n",
    "$\\forcecommand{\\gv}{{\\boldsymbol{g}}}$\n",
    "$\\forcecommand{\\hv}{{\\boldsymbol{h}}}$\n",
    "$\\forcecommand{\\nv}{{\\boldsymbol{n}}}$\n",
    "$\\forcecommand{\\sv}{{\\boldsymbol{s}}}$\n",
    "$\\forcecommand{\\tv}{{\\boldsymbol{t}}}$\n",
    "$\\forcecommand{\\uv}{{\\boldsymbol{u}}}$\n",
    "$\\forcecommand{\\vv}{{\\boldsymbol{v}}}$\n",
    "$\\forcecommand{\\wv}{{\\boldsymbol{w}}}$\n",
    "$\\forcecommand{\\zerov}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\onev}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\phiv}{{\\boldsymbol{\\phi}}}$\n",
    "$\\forcecommand{\\cc}{{\\check{C}}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\Xv}{{\\boldsymbol{X}\\!}}$\n",
    "$\\forcecommand{\\yv}{{\\boldsymbol{y}}}$\n",
    "$\\forcecommand{\\Yv}{{\\boldsymbol{Y}}}$\n",
    "$\\forcecommand{\\zv}{{\\boldsymbol{z}}}$\n",
    "$\\forcecommand{\\Zv}{{\\boldsymbol{Z}}}$\n",
    "$\\forcecommand{\\Iv}{{\\boldsymbol{I}}}$\n",
    "$\\forcecommand{\\Jv}{{\\boldsymbol{J}}}$\n",
    "$\\forcecommand{\\Cv}{{\\boldsymbol{C}}}$\n",
    "$\\forcecommand{\\Ev}{{\\boldsymbol{E}}}$\n",
    "$\\forcecommand{\\Fv}{{\\boldsymbol{F}}}$\n",
    "$\\forcecommand{\\Gv}{{\\boldsymbol{G}}}$\n",
    "$\\forcecommand{\\Hv}{{\\boldsymbol{H}}}$\n",
    "$\\forcecommand{\\alphav}{{\\boldsymbol{\\alpha}}}$\n",
    "$\\forcecommand{\\epsilonv}{{\\boldsymbol{\\epsilon}}}$\n",
    "$\\forcecommand{\\betav}{{\\boldsymbol{\\beta}}}$\n",
    "$\\forcecommand{\\deltav}{{\\boldsymbol{\\delta}}}$\n",
    "$\\forcecommand{\\gammav}{{\\boldsymbol{\\gamma}}}$\n",
    "$\\forcecommand{\\etav}{{\\boldsymbol{\\eta}}}$\n",
    "$\\forcecommand{\\piv}{{\\boldsymbol{\\pi}}}$\n",
    "$\\forcecommand{\\thetav}{{\\boldsymbol{\\theta}}}$\n",
    "$\\forcecommand{\\tauv}{{\\boldsymbol{\\tau}}}$\n",
    "$\\forcecommand{\\muv}{{\\boldsymbol{\\mu}}}$\n",
    "$%$\n",
    "$\\forcecommand{\\sd}{\\text{SD}}$\n",
    "$\\forcecommand{\\se}{\\text{SE}}$\n",
    "$\\forcecommand{\\med}{\\text{median}}$\n",
    "$\\forcecommand{\\median}{\\text{median}}$\n",
    "$%$\n",
    "$\\forcecommand{\\supp}{\\text{supp}}$\n",
    "$\\forcecommand{\\E}{\\mathbb{E}}$\n",
    "$\\forcecommand{\\var}{\\text{Var}}$\n",
    "$\\forcecommand{\\Ber}{{\\text{Ber}}}$\n",
    "$\\forcecommand{\\Bin}{{\\text{Bin}}}$\n",
    "$\\forcecommand{\\Geo}{{\\text{Geo}}}$\n",
    "$\\forcecommand{\\Unif}{{\\text{Unif}}}$\n",
    "$\\forcecommand{\\Poi}{{\\text{Poi}}}$\n",
    "$\\forcecommand{\\Exp}{{\\text{Exp}}}$\n",
    "$\\forcecommand{\\Chisq}{{\\chi^2}}$\n",
    "$\\forcecommand{\\N}{\\mathbb{N}}$\n",
    "$\\forcecommand{\\iid}{{\\stackrel{iid}{\\sim}}}$\n",
    "$\\forcecommand{\\px}{p_{X}}$\n",
    "$\\forcecommand{\\fx}{f_{X}}$\n",
    "$\\forcecommand{\\Fx}{F_{X}}$\n",
    "$\\forcecommand{\\py}{p_{Y}}$\n",
    "$\\forcecommand{\\pxy}{p_{X,Y}}$\n",
    "$\\forcecommand{\\po}{{p_0}}$\n",
    "$\\forcecommand{\\pa}{{p_a}}$\n",
    "$\\forcecommand{\\Xbar}{\\overline{X}}$\n",
    "$\\forcecommand{\\Ybar}{\\overline{Y}}$\n",
    "$\\forcecommand{\\Zbar}{\\overline{Z}}$\n",
    "$\\forcecommand{\\nXbar}{n \\cdot \\overline{X}}$\n",
    "$\\forcecommand{\\nYbar}{n \\cdot \\overline{Y}}$\n",
    "$\\forcecommand{\\nZbar}{n \\cdot \\overline{Z}}$\n",
    "$\\forcecommand{\\Xn}{X_1, X_2, \\dots, X_n}$\n",
    "$\\forcecommand{\\Xm}{{X_1, X_2, \\dots, X_m}}$\n",
    "$\\forcecommand{\\Yn}{Y_1, Y_2, \\dots, Y_n}$\n",
    "$\\forcecommand{\\Ym}{{Y_1, Y_2, \\dots, Y_m}}$\n",
    "$\\forcecommand{\\sumXn}{X_1 + X_2 + \\dots + X_n}$\n",
    "$\\forcecommand{\\sumym}{Y_1 + Y_2 + \\dots + Y_m}$\n",
    "$\\forcecommand{\\la}{\\ell_\\alpha}$\n",
    "$\\forcecommand{\\ua}{u_\\alpha}$\n",
    "$\\forcecommand{\\at}{{\\alpha/2}}$\n",
    "$\\forcecommand{\\mux}{\\mu_{X}}$\n",
    "$\\forcecommand{\\muy}{\\mu_{Y}}$\n",
    "$\\forcecommand{\\sx}{\\sigma_{X}}$\n",
    "$\\forcecommand{\\sy}{\\sigma_{Y}}$\n",
    "$\\forcecommand{\\ci}{\\text{CI}}$\n",
    "$\\forcecommand{\\pvalue}{$p$-value}$\n",
    "$\\forcecommand{\\Ho}{H_{0}}$\n",
    "$\\forcecommand{\\Ha}{H_{a}}$\n",
    "\n",
    "\\vskip-\\parskip\n",
    "\\vskip-\\baselineskip\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f9025a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "# pio.renderers.default='notebook'\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# Optional \n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "matplotlib.rcParams['figure.figsize'] = 7, 7\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c4b4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def plot_X(X, ax, type='pmf', **kwargs):\n",
    "    ax.set_xlabel('Support')\n",
    "    ax.set_title(f'{X.dist.name}{X.args}')\n",
    "    \n",
    "    min_X, max_X = X.ppf((1e-3, 1-1e-3))\n",
    "    supp_X = np.linspace(min_X-1, max_X + 1, 200)\n",
    "    \n",
    "    if type == 'pmf':\n",
    "        supp_X = np.arange(min_X-1, max_X + 1)\n",
    "        ax.bar(supp_X, X.pmf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PMF')\n",
    "    elif type == 'pdf':\n",
    "        ax.plot(supp_X, X.pdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PDF')\n",
    "    elif type == 'cdf':\n",
    "        ax.plot(supp_X, X.cdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('CDF')\n",
    "    else:\n",
    "        raise ValueError('type must be pmf or cdf')\n",
    "\n",
    "def decision(pvalue, alpha):\n",
    "    if pvalue < alpha:\n",
    "        print(f'reject H0: pvalue={pvalue} < {alpha}')  \n",
    "    else: \n",
    "        print(f'fail to reject H0: pvalue={pvalue} â‰¥ {alpha}')\n",
    "\n",
    "def standardize(X):\n",
    "    return (X - X.mean()) / X.std()\n",
    "\n",
    "\n",
    "def make_data(errors):\n",
    "    n = len(errors)\n",
    "    x1 = np.linspace(0, 1, n)\n",
    "    x2 = np.random.rand(n)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'x1': x1, \n",
    "        'x2': x2, \n",
    "        'y': 2 + 3*x1 + 4*x2 + errors\n",
    "    })\n",
    "\n",
    "def plot_regression(data, fit, residuals=True):\n",
    "    b = fit.params\n",
    "    b0, b1, b2 = *b, *np.zeros(3 - len(b))\n",
    "    y, x1, x2 = data.y, data.x1, data.x2\n",
    "    fig = px.scatter_3d(x=x1, y=x2, z=y)\n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "            xaxis_title='X1',\n",
    "            yaxis_title='X2',\n",
    "            zaxis_title='Y'),\n",
    "            margin=dict(l=0, r=0, b=0, t=0\n",
    "        )\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    \n",
    "    x1_grid, x2_grid = np.meshgrid(x1, x2)\n",
    "    yhat = b0 + (b1 * x1_grid) + (b2 * x2_grid)\n",
    "    fig.add_trace(\n",
    "        go.Surface(x=x1_grid, y=x2_grid, z=yhat, opacity=0.5,colorscale='Gray')\n",
    "    )\n",
    "    if residuals:\n",
    "        for i in range(len(x1)):\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(x=[x1[i], x1[i]], y=[x2[i], x2[i]], z=[b0 + b1*x1[i] + b2*x2[i], y[i]], mode='lines', line=dict(color='black', width=2))\n",
    "            )\n",
    "    fig.update_layout(showlegend=False, scene_camera=dict(eye=dict(x=2.0, y=0.5, z=0.1)))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6dc53",
   "metadata": {},
   "source": [
    "## Key Takeaways from Week 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc992f31",
   "metadata": {},
   "source": [
    "\n",
    "#### Tuesday: \n",
    "\n",
    "... insert your key takeaways here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69de7b",
   "metadata": {},
   "source": [
    "#### Thursday\n",
    "\n",
    "... insert your takeaway here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaadbca8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071d8aa",
   "metadata": {},
   "source": [
    "# Prediction Intervals\n",
    "Previously we ahve pretty much looked at simple models, models that we can perform **uncertainty quantification** on the parameter of the model:\n",
    "\n",
    "All of these model are `creating manifolds on the data` by using parameter, we test our hypothesis on the parameters of the model:\n",
    "\n",
    "- Confidence intervals for the coefficients\n",
    "- Hypothesis tests for the coefficients\n",
    "- Hypothesis tests for linear functions of the coefficients\n",
    "\n",
    "Can we **quantify the uncertainties in the predictions themselves?**: `predictions themeselves are uncertain!`\n",
    "\n",
    "- Confidence interval for the mean response\n",
    "- Prediction interval for individual response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11abc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(43)\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "x = np.random.rand(num_samples)\n",
    "\n",
    "# true linear relationship\n",
    "true_slope = 2\n",
    "true_intercept = 1\n",
    "y = true_slope * x + true_intercept + np.random.normal(scale=0.2, size=num_samples)\n",
    "\n",
    "# introduce some outliers\n",
    "num_outliers = 10\n",
    "outlier_indices = np.random.choice(num_samples, num_outliers, replace=False)\n",
    "y_outliers = y[outlier_indices] + np.random.normal(scale=1.0, size=num_outliers)\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df_new = pd.DataFrame({'x': x[outlier_indices], 'y': y_outliers})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.scatter(df_new['x'], df_new['y'], color='red')\n",
    "ax.plot(df['x'], true_intercept + true_slope * df['x'], color='green', ls='--', lw=0.5, label='True line')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a40cc",
   "metadata": {},
   "source": [
    "### Confidence intervals for response in expectation\n",
    "\n",
    "A confidence interval for the mean response $\\mathbb{E}(y_0 \\mid x_0)$ at a given point $x_0$ is an interval $[\\ell_{\\alpha}, u_{\\alpha}]$ such that\n",
    "\n",
    "$$\n",
    "P(\\ell_{\\alpha} \\leq \\mathbb{E}(y_0 \\mid x_0) \\leq u_{\\alpha}) = 1 - \\alpha\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the confidence level. **It try to give the expected value of the predictions, which accounts in the noises**ã€‚\n",
    "- It tries to capture the mean response as much as possible!\n",
    "- 0.9 CI is a random interval that cpatures 90% of the predictions\n",
    "- Quantify the unceratinty of the predictions you make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ecb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols('y~x', df).fit()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.scatter(df_new['x'], df_new['y'], color='red', label='New data')\n",
    "ax.plot(df['x'], 1 + 2*df['x'], color='green', lw=0.5, ls='--', label='True line')\n",
    "ax.plot(df['x'], model.fittedvalues, label='fitted line', color='black')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcae5a0",
   "metadata": {},
   "source": [
    "### Prediction intervals for individual responses\n",
    "\n",
    "A prediction interval for the response $y_0$ at a given point $x_0$ is an interval $[\\ell_{\\alpha}, u_{\\alpha}]$ such that\n",
    "\n",
    "$$\n",
    "P(\\ell_{\\alpha} \\leq y_0 \\leq u_{\\alpha}) = 1 - \\alpha\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the confidence level. **It try to give the noises in the predictions**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953bf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pi = df.copy()\n",
    "df_pi_new = df_new.copy()\n",
    "\n",
    "model = smf.ols('y ~ x', df_pi).fit()\n",
    "\n",
    "@interact(alpha=(0.01, 0.5, 0.01))\n",
    "def plot_intervals(alpha):\n",
    "    preds = model.get_prediction(df_pi).summary_frame(alpha=alpha)\n",
    "    # preds = model.get_prediction(df_pi_new).summary_frame(alpha=alpha)\n",
    "\n",
    "    # plot confidence interval\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.scatter(df_pi['x'], df_pi['y'])\n",
    "    ax.scatter(df_pi_new['x'], df_pi_new['y'], color='red', label='New data')\n",
    "    ax.plot(df_pi['x'], 1 + 2*df_pi['x'], color='green', lw=0.5, ls='--', label='True line')\n",
    "    ax.plot(df_pi['x'], model.fittedvalues, c='black', label='fitted line')\n",
    "\n",
    "#     ax.fill_between(df_pi['x'], preds['mean_ci_lower'], preds['mean_ci_upper'], color='blue', alpha=0.2)\n",
    "    ax.fill_between(df_pi['x'], preds['obs_ci_lower'], preds['obs_ci_upper'], color='red', alpha=0.2)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(0.5, 3.5)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d585bd2",
   "metadata": {},
   "source": [
    "Another more complicated example with more complicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 300\n",
    "x = np.linspace(-3, 3, num_samples)\n",
    "\n",
    "# true nonlinear function\n",
    "fx = lambda x: x**2 + 10 * np.sin(x**2) + np.exp(x)\n",
    "\n",
    "# generate y values with noise\n",
    "y = fx(x) + np.random.normal(scale=1.0, size=num_samples)\n",
    "\n",
    "# introduce some outliers\n",
    "num_outliers = 30\n",
    "outlier_indices = np.random.choice(num_samples, num_outliers, replace=False)\n",
    "y_outliers = y[outlier_indices] + np.random.normal(scale=5.0, size=num_outliers)\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y}).sort_values('x')\n",
    "df_new = pd.DataFrame({'x': x[outlier_indices], 'y': y_outliers}).sort_values('x')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.plot(df['x'], fx(df['x']), color='green', lw=1, ls='--', label='True function')\n",
    "ax.scatter(df_new['x'], df_new['y'], color='red')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48601b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols('y ~ x', df).fit()\n",
    "\n",
    "@interact(alpha=(0.01, 0.5, 0.01))\n",
    "def plot_intervals(alpha):\n",
    "    preds = model.get_prediction(df).summary_frame(alpha=alpha)\n",
    "    # preds = model.get_prediction(df_new).summary_frame(alpha=alpha)\n",
    "\n",
    "    # plot confidence interval\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.scatter(df['x'], df['y'])\n",
    "    ax.scatter(df_new['x'], df_new['y'], color='red', label='New data')\n",
    "    ax.plot(df['x'].sort_values(), fx(df['x'].sort_values()), color='green', lw=0.5, ls='--', label='True line')\n",
    "    ax.plot(df['x'], model.fittedvalues, c='black', label='fitted line')\n",
    "\n",
    "    # ax.fill_between(df['x'], preds['mean_ci_lower'], preds['mean_ci_upper'], color='blue', alpha=0.2)\n",
    "    ax.fill_between(df['x'], preds['obs_ci_lower'], preds['obs_ci_upper'], color='red', alpha=0.2)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlim(-3.5, 3.5)\n",
    "    # ax.set_ylim(0.5, 3.5)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd786a01",
   "metadata": {},
   "source": [
    "Obviously, this prediction interval doesn't seem to be that good, we need some alternative ways to try to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a6afc",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4918a6a0",
   "metadata": {},
   "source": [
    "# Statistical Leraning Theory\n",
    "We will be looking at a very traditional perspective on machine learning from the `statistical learning` paradigm. This is where ERM comes from as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9065d3d",
   "metadata": {},
   "source": [
    "> Goal:\n",
    "\n",
    "Find an appropriate model $f$ which fits $y$, i.e.,\n",
    "\n",
    "$$\n",
    "y = f(X_1, X_2, \\dots, X_p)\n",
    "$$\n",
    "\n",
    "> Statistical Learning:\n",
    "\n",
    "$$\n",
    "\\min_{f \\in \\mathcal{F}} \\sum_{i=1}^{n} L \\left( y_i, f(X_{i1}, \\dots, X_{ip}) \\right) + \\lambda p(f)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $L$ is the loss function,\n",
    "- $\\mathcal{F}$ is the set of possible models,\n",
    "- $\\lambda p(f)$ is a penalty term to prevent overfitting.\n",
    "\n",
    "\n",
    "**This is the `Empirical Risk Minimization` paradigm in statistical learning theory**. Actually, most of traditional machine elarning comes from paradigms in statistical learning (i.e. SVM tries to reproduce margin minimization on hillbert space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffbfa16",
   "metadata": {},
   "source": [
    "Much of the course has been focused on the following examples:\n",
    "\n",
    "| Problem                      | $y$                  | $f \\in \\mathcal{F}$                            | $\\mathcal{L}$                                            | $p$                                      |\n",
    "|------------------------------|----------------------|--------------------------------|------------------------------------------------|--------------------------------|\n",
    "| **Regression**               | $\\mathbb{R}$         | $\\beta_0 + x^T \\beta_1$       | $\\|y - f(X)\\|^2$                              | $0$                        |\n",
    "| **Logistic**                 | $\\{0,1\\}$           | $\\sigma(\\beta_0 + x^T \\beta_1)$ | $y \\log(f(X)) + (1 - y) \\log(1 - f(X))$      | $0$                        |\n",
    "| **Multinomial Logistic**     | $\\{C_1 \\dots C_k\\}$ | $\\text{softmax}(B_{0,1:k} + x^T B_{1,1:k})$ | $\\text{OneHot}(y) \\cdot \\log(f(X))$ | $0$                        |\n",
    "| **Ridge**                    | $\\mathbb{R}$         | $\\beta_0 + x^T \\beta_1$       | $\\|y - f(X)\\|^2$                              | $\\|\\beta_1\\|^2$              |\n",
    "| **LASSO**                    | $\\mathbb{R}$         | $\\beta_0 + x^T \\beta_1$       | $\\|y - f(X)\\|^2$                              | $\\|\\beta_1\\|_1$              |\n",
    "| **Elastic Net**              | $\\mathbb{R}$         | $\\beta_0 + x^T \\beta_1$       | $\\|y - f(X)\\|^2$                              | $\\gamma \\|\\beta_1\\|_1 + (1 - \\gamma) \\|\\beta_1\\|^2$ |\n",
    "| **Logistic + Elastic Net**   | $\\{0,1\\}$           | $\\sigma(\\beta_0 + x^T \\beta_1)$ | $y \\log(f(X)) + (1 - y) \\log(1 - f(X))$      | $\\gamma \\|\\beta_1\\|_1 + (1 - \\gamma) \\|\\beta_1\\|^2$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eaa162",
   "metadata": {},
   "source": [
    "Most supervised learning problems can be cast into the same framework:\n",
    "\n",
    "| Problem                  | $y$                  | $f \\in \\mathcal{F}$                                      | $\\mathcal{L}$                                        | $p$  |\n",
    "|--------------------------|----------------------|------------------------------------------------|------------------------------------------------|----|\n",
    "| **Regression**           | $\\mathbb{R}$         | $f(X) = \\beta_0 + x^T \\beta_1$                 | $\\|y - f(X)\\|^2$                              | $0$ |\n",
    "| **Logistic**             | $\\{0,1\\}$           | $f(X) = \\sigma(\\beta_0 + x^T \\beta_1)$         | $y \\log(f(X)) + (1 - y) \\log(1 - f(X))$      | $0$ |\n",
    "| **SVM Classification**   | $\\{-1,1\\}$          | $f(X) = \\sum_{i=1}^{n} K_{\\sigma}(\\cdot, X_i)$ | $\\max(0, 1 - y f(X))$                         | $\\|\\beta_1\\|^2$ |\n",
    "| **MLP**                 | $\\mathbb{R}$         | $f(X) = \\psi(W_2 \\psi(W_1 x + b_1) + b_2)$     | $\\|y - f(X)\\|^2$                              | $0$ |\n",
    "| **MLP Classification**   | $\\{0,1\\}$           | $f(X) = \\text{softmax}(\\psi(W_2 \\psi(W_1 x + b_1) + b_2))$ | $\\text{OneHot}(y) \\cdot \\log(f(X))$ | $0$ |\n",
    "| **DNNs**                | $\\mathbb{R}$         | $f(X) = \\psi(W_D \\circ \\dots \\circ (W_1 x + b_1)) + \\dots + b_D$ | $\\|y - f(X)\\|^2$ | $0$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9489b073",
   "metadata": {},
   "source": [
    "Let's look at an example of support vector machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3baabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df['x'].values.reshape(-1, 1)\n",
    "y = df['y']\n",
    "\n",
    "svr = make_pipeline(StandardScaler(), SVR(C=1e4, epsilon=1.0))\n",
    "svr.fit(X, y)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.scatter(df_new['x'], df_new['y'], color='red')\n",
    "ax.plot(df['x'], fx(df['x']), color='green', lw=1, ls='--', label='True function')\n",
    "ax.plot(df['x'], svr.predict(X), color='black', lw=1, label='SVM fit')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf50e10",
   "metadata": {},
   "source": [
    "Let's do the same with a neural network and visualzie the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "X = torch.tensor(df['x'].values).float().reshape(-1, 1)\n",
    "Y = torch.tensor(df['y'].values).float().reshape(-1, 1)\n",
    "\n",
    "model_linear = nn.Linear(1, 1)\n",
    "model_dnn = nn.Sequential(\n",
    "    nn.Linear(1, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 1)\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_linear = torch.optim.Adam(model_linear.parameters(), lr=0.05)\n",
    "optimizer_dnn = torch.optim.Adam(model_dnn.parameters(), lr=0.05)\n",
    "\n",
    "# Visualization setup\n",
    "epochs_to_plot = [0, 10, 50, 99, 150, 299]  # Chosen epochs to visualize\n",
    "predictions_linear = {}\n",
    "predictions_dnn = {}\n",
    "\n",
    "for epoch in range(300):\n",
    "    optimizer_linear.zero_grad()\n",
    "    optimizer_dnn.zero_grad()\n",
    "\n",
    "    output_linear = model_linear(X)\n",
    "    output_dnn = model_dnn(X)\n",
    "\n",
    "    loss_linear = criterion(output_linear, Y)\n",
    "    loss_dnn = criterion(output_dnn, Y)\n",
    "\n",
    "    print(f'Epoch {epoch}, Loss linear: {loss_linear.item():.4f}, Loss DNN: {loss_dnn.item():.4f}')\n",
    "\n",
    "    loss_linear.backward()\n",
    "    loss_dnn.backward()\n",
    "\n",
    "    optimizer_linear.step()\n",
    "    optimizer_dnn.step()\n",
    "\n",
    "    # Store predictions at specific epochs\n",
    "    if epoch in epochs_to_plot:\n",
    "        predictions_linear[epoch] = output_linear.detach().numpy()\n",
    "        predictions_dnn[epoch] = output_dnn.detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "for i, epoch in enumerate(epochs_to_plot):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(df['x'], df['y'], alpha=0.3, label='True Data')\n",
    "    ax.plot(df['x'], predictions_linear[epoch], color='blue', lw=1, label=f'Linear Model (Epoch {epoch})')\n",
    "    ax.plot(df['x'], predictions_dnn[epoch], color='red', lw=1, label=f'DNN Model (Epoch {epoch})')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'Epoch {epoch}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf69ccc",
   "metadata": {},
   "source": [
    "However, we bump into an issue. Some sort of inference can be done on the learned model, but it gets increasingly more difficult as the complexity of the model increases.\n",
    "\n",
    "Types of inference:\n",
    "- **Variable Importance**: Which variables are most important in predicting the response? (**No uncertainty quantification** ðŸ˜ž)\n",
    "- **Confidence Intervals**: How confident are we in our predictions?\n",
    "\n",
    "\n",
    "We lose out some of the **inferential** ability we have in the simpler models. However, in the past 5-6 years, there came out an new method of doing `uncertainty quantification`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ee6ee",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3dbaf3",
   "metadata": {},
   "source": [
    "# Conformal Inference\n",
    "\n",
    "Now, we don't make any assumptions on the underlaying model, but rather treat it as a `black-box` directly! Ideas dates back to the 90th!\n",
    "\n",
    "- Conformal inference is a framework for **constructing prediction intervals and confidence regions for black-box machine learning models**.\n",
    "\n",
    "- It doesnâ€™t rely on any assumptions about the underlying model, and can be used with any machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a048a",
   "metadata": {},
   "source": [
    "## Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc33ad9",
   "metadata": {},
   "source": [
    "#### 1. Split the Data:\n",
    "- Split the dataset into two independent subsets:\n",
    "  - **Training Set** ($D_{\\text{train}}$): Used to fit the model.\n",
    "  - **Calibration Set** ($D_{\\text{cal}}$): Used to calibrate the prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b3d83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_cal, y_train, y_cal = train_test_split(df['x'], df['y'], test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abce656",
   "metadata": {},
   "source": [
    "#### 2. Fit the Model:\n",
    "- Use the training set $D_{\\text{train}}$ to fit your predictive model.\n",
    "- The choice of model depends on the problem (e.g., linear regression, decision tree, neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d119048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit SVM on training data\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_cal = X_cal.values.reshape(-1, 1)\n",
    "\n",
    "svr = make_pipeline(StandardScaler(), SVR(C=1e4, epsilon=1.0))\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e592326",
   "metadata": {},
   "source": [
    "#### 3. Generate Predictions:\n",
    "- Apply the fitted model to the calibration set $D_{\\text{cal}}$ to obtain predictions.\n",
    "- For each instance in the calibration set, compute the prediction error:\n",
    "  - **Error**: $e_i = | y_i - \\hat{y}_i |$\n",
    "  - Where $y_i$ is the actual value and $\\hat{y}_i$ is the predicted value for the $i$-th observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d94bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = svr.predict(X_cal)\n",
    "\n",
    "errors = y_cal - yhat\n",
    "\n",
    "plt.hist(errors, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87193ccf",
   "metadata": {},
   "source": [
    "#### 4. Calculate Conformity Scores:\n",
    "- Use a conformity measure to calculate a score for each prediction in the calibration set.\n",
    "- A common choice for the conformity score is the absolute prediction error, as calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4597ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort errors\n",
    "conf_score = np.sort(errors)\n",
    "\n",
    "# get the empirical 90% interval\n",
    "alpha = 0.1\n",
    "lower = conf_score[int(alpha/2 * len(conf_score))]\n",
    "upper = conf_score[int((1 - alpha/2) * len(conf_score))]\n",
    "lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96250ae7",
   "metadata": {},
   "source": [
    "#### 5. Determine the **Prediction Interval**:\n",
    "- Sort the conformity scores in ascending order.\n",
    "- Determine the $(1 - \\alpha)$-quantile of the conformity scores, where $\\alpha$ is the significance level (e.g., $0.05$ for $95\\%$ coverage).\n",
    "- The $(1 - \\alpha)$-quantile is the threshold value $q$.\n",
    "\n",
    "#### 6. Construct **Prediction Intervals**:\n",
    "- For a new data point $x$, after fitting the model on $D_{\\text{train}}$, predict the value $\\hat{y}(x)$.\n",
    "- Construct the prediction interval as $[\\hat{y} - q, \\hat{y} + q]$, ensuring that the interval will cover the true value of $y$ with probability at least $(1 - \\alpha)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = svr.predict(df['x'].sort_values().values.reshape(-1, 1))\n",
    "pred_upper = preds + upper\n",
    "pred_lower = preds + lower\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.plot(df['x'].sort_values(), preds, color='black', lw=1, label='SVM fit')\n",
    "ax.fill_between(df['x'].sort_values(), pred_lower, pred_upper, color='red', alpha=0.2, label='90% PI')\n",
    "ax.scatter(df_new['x'], df_new['y'], color='red', label='New Data')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8371eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(alpha=(0.01, 0.5, 0.01))\n",
    "def plot_intervals(alpha):\n",
    "    X_train, X_cal, y_train, y_cal = train_test_split(df['x'], df['y'], test_size=0.25, random_state=0)\n",
    "    X_train = X_train.values.reshape(-1, 1)\n",
    "    X_cal = X_cal.values.reshape(-1, 1)\n",
    "    svr = make_pipeline(StandardScaler(), SVR(C=1e3, epsilon=1.0))\n",
    "    svr.fit(X_train, y_train)\n",
    "    yhat = svr.predict(X_cal)\n",
    "    errors = y_cal - yhat\n",
    "    conf_score = np.sort(errors)\n",
    "    lower = conf_score[int(alpha/2 * len(conf_score))]\n",
    "    upper = conf_score[int((1 - alpha/2) * len(conf_score))]\n",
    "    preds = svr.predict(df['x'].sort_values().values.reshape(-1, 1))\n",
    "    pred_upper = preds + upper\n",
    "    pred_lower = preds + lower\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.scatter(df['x'], df['y'])\n",
    "    ax.plot(df['x'].sort_values(), preds, color='black', lw=1, label='SVM fit')\n",
    "    ax.fill_between(df['x'].sort_values(), pred_lower, pred_upper, color='red', alpha=0.2, label=f'{100 * (1-alpha)}% PI')\n",
    "    ax.scatter(df_new['x'], df_new['y'], color='red', label='New Data')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53422a62",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
