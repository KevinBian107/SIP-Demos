{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d499b8cf",
   "metadata": {},
   "source": [
    "# Math 189 Week 6 Summary\n",
    "> NAME: $\\color{blue}{\\text{Kaiwen Bian}}$\n",
    "> \n",
    "> PID: $\\color{blue}{\\text{A17316568}}$\n",
    ">\n",
    "> \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce0cf8",
   "metadata": {},
   "source": [
    "I certify that the following write-up is my own work, and have abided by the UCSD Academic Integrity Guidelines.\n",
    "\n",
    "- [x] Yes\n",
    "- [ ] No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c41b1",
   "metadata": {},
   "source": [
    "% # %load tex-macros\n",
    "<div hidden>\n",
    "\\newcommand{\\require}[1]{}\n",
    "\n",
    "$\\require{begingroup}\\require{newcommand}$\n",
    "$\\long\\def \\forcecommand #1{\\providecommand{#1}{}\\renewcommand{#1}}$\n",
    "$\\forcecommand{\\defeq}{\\stackrel{\\small\\bullet}{=}}$\n",
    "$\\forcecommand{\\ra}{\\rangle}$\n",
    "$\\forcecommand{\\la}{\\langle}$\n",
    "$\\forcecommand{\\pr}{{\\mathbb P}}$\n",
    "$\\forcecommand{\\qr}{{\\mathbb Q}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\av}{{\\boldsymbol{a}}}$\n",
    "$\\forcecommand{\\bv}{{\\boldsymbol{b}}}$\n",
    "$\\forcecommand{\\cv}{{\\boldsymbol{c}}}$\n",
    "$\\forcecommand{\\dv}{{\\boldsymbol{d}}}$\n",
    "$\\forcecommand{\\ev}{{\\boldsymbol{e}}}$\n",
    "$\\forcecommand{\\fv}{{\\boldsymbol{f}}}$\n",
    "$\\forcecommand{\\gv}{{\\boldsymbol{g}}}$\n",
    "$\\forcecommand{\\hv}{{\\boldsymbol{h}}}$\n",
    "$\\forcecommand{\\nv}{{\\boldsymbol{n}}}$\n",
    "$\\forcecommand{\\sv}{{\\boldsymbol{s}}}$\n",
    "$\\forcecommand{\\tv}{{\\boldsymbol{t}}}$\n",
    "$\\forcecommand{\\uv}{{\\boldsymbol{u}}}$\n",
    "$\\forcecommand{\\vv}{{\\boldsymbol{v}}}$\n",
    "$\\forcecommand{\\wv}{{\\boldsymbol{w}}}$\n",
    "$\\forcecommand{\\zerov}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\onev}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\phiv}{{\\boldsymbol{\\phi}}}$\n",
    "$\\forcecommand{\\cc}{{\\check{C}}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\Xv}{{\\boldsymbol{X}\\!}}$\n",
    "$\\forcecommand{\\yv}{{\\boldsymbol{y}}}$\n",
    "$\\forcecommand{\\Yv}{{\\boldsymbol{Y}}}$\n",
    "$\\forcecommand{\\zv}{{\\boldsymbol{z}}}$\n",
    "$\\forcecommand{\\Zv}{{\\boldsymbol{Z}}}$\n",
    "$\\forcecommand{\\Iv}{{\\boldsymbol{I}}}$\n",
    "$\\forcecommand{\\Jv}{{\\boldsymbol{J}}}$\n",
    "$\\forcecommand{\\Cv}{{\\boldsymbol{C}}}$\n",
    "$\\forcecommand{\\Ev}{{\\boldsymbol{E}}}$\n",
    "$\\forcecommand{\\Fv}{{\\boldsymbol{F}}}$\n",
    "$\\forcecommand{\\Gv}{{\\boldsymbol{G}}}$\n",
    "$\\forcecommand{\\Hv}{{\\boldsymbol{H}}}$\n",
    "$\\forcecommand{\\alphav}{{\\boldsymbol{\\alpha}}}$\n",
    "$\\forcecommand{\\epsilonv}{{\\boldsymbol{\\epsilon}}}$\n",
    "$\\forcecommand{\\betav}{{\\boldsymbol{\\beta}}}$\n",
    "$\\forcecommand{\\deltav}{{\\boldsymbol{\\delta}}}$\n",
    "$\\forcecommand{\\gammav}{{\\boldsymbol{\\gamma}}}$\n",
    "$\\forcecommand{\\etav}{{\\boldsymbol{\\eta}}}$\n",
    "$\\forcecommand{\\piv}{{\\boldsymbol{\\pi}}}$\n",
    "$\\forcecommand{\\thetav}{{\\boldsymbol{\\theta}}}$\n",
    "$\\forcecommand{\\tauv}{{\\boldsymbol{\\tau}}}$\n",
    "$\\forcecommand{\\muv}{{\\boldsymbol{\\mu}}}$\n",
    "$%$\n",
    "$\\forcecommand{\\sd}{\\text{SD}}$\n",
    "$\\forcecommand{\\se}{\\text{SE}}$\n",
    "$\\forcecommand{\\med}{\\text{median}}$\n",
    "$\\forcecommand{\\median}{\\text{median}}$\n",
    "$%$\n",
    "$\\forcecommand{\\supp}{\\text{supp}}$\n",
    "$\\forcecommand{\\E}{\\mathbb{E}}$\n",
    "$\\forcecommand{\\var}{\\text{Var}}$\n",
    "$\\forcecommand{\\Ber}{{\\text{Ber}}}$\n",
    "$\\forcecommand{\\Bin}{{\\text{Bin}}}$\n",
    "$\\forcecommand{\\Geo}{{\\text{Geo}}}$\n",
    "$\\forcecommand{\\Unif}{{\\text{Unif}}}$\n",
    "$\\forcecommand{\\Poi}{{\\text{Poi}}}$\n",
    "$\\forcecommand{\\Exp}{{\\text{Exp}}}$\n",
    "$\\forcecommand{\\Chisq}{{\\chi^2}}$\n",
    "$\\forcecommand{\\N}{\\mathbb{N}}$\n",
    "$\\forcecommand{\\iid}{{\\stackrel{iid}{\\sim}}}$\n",
    "$\\forcecommand{\\px}{p_{X}}$\n",
    "$\\forcecommand{\\fx}{f_{X}}$\n",
    "$\\forcecommand{\\Fx}{F_{X}}$\n",
    "$\\forcecommand{\\py}{p_{Y}}$\n",
    "$\\forcecommand{\\pxy}{p_{X,Y}}$\n",
    "$\\forcecommand{\\po}{{p_0}}$\n",
    "$\\forcecommand{\\pa}{{p_a}}$\n",
    "$\\forcecommand{\\Xbar}{\\overline{X}}$\n",
    "$\\forcecommand{\\Ybar}{\\overline{Y}}$\n",
    "$\\forcecommand{\\Zbar}{\\overline{Z}}$\n",
    "$\\forcecommand{\\nXbar}{n \\cdot \\overline{X}}$\n",
    "$\\forcecommand{\\nYbar}{n \\cdot \\overline{Y}}$\n",
    "$\\forcecommand{\\nZbar}{n \\cdot \\overline{Z}}$\n",
    "$\\forcecommand{\\Xn}{X_1, X_2, \\dots, X_n}$\n",
    "$\\forcecommand{\\Xm}{{X_1, X_2, \\dots, X_m}}$\n",
    "$\\forcecommand{\\Yn}{Y_1, Y_2, \\dots, Y_n}$\n",
    "$\\forcecommand{\\Ym}{{Y_1, Y_2, \\dots, Y_m}}$\n",
    "$\\forcecommand{\\sumXn}{X_1 + X_2 + \\dots + X_n}$\n",
    "$\\forcecommand{\\sumym}{Y_1 + Y_2 + \\dots + Y_m}$\n",
    "$\\forcecommand{\\la}{\\ell_\\alpha}$\n",
    "$\\forcecommand{\\ua}{u_\\alpha}$\n",
    "$\\forcecommand{\\at}{{\\alpha/2}}$\n",
    "$\\forcecommand{\\mux}{\\mu_{X}}$\n",
    "$\\forcecommand{\\muy}{\\mu_{Y}}$\n",
    "$\\forcecommand{\\sx}{\\sigma_{X}}$\n",
    "$\\forcecommand{\\sy}{\\sigma_{Y}}$\n",
    "$\\forcecommand{\\ci}{\\text{CI}}$\n",
    "$\\forcecommand{\\pvalue}{$p$-value}$\n",
    "$\\forcecommand{\\Ho}{H_{0}}$\n",
    "$\\forcecommand{\\Ha}{H_{a}}$\n",
    "\n",
    "\\vskip-\\parskip\n",
    "\\vskip-\\baselineskip\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0120db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "# pio.renderers.default='notebook'\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# Optional \n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "matplotlib.rcParams['figure.figsize'] = 7, 7\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b264b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_X(X, ax, type='pmf', **kwargs):\n",
    "    ax.set_xlabel('Support')\n",
    "    ax.set_title(f'{X.dist.name}{X.args}')\n",
    "    \n",
    "    min_X, max_X = X.ppf((1e-3, 1-1e-3))\n",
    "    supp_X = np.linspace(min_X-1, max_X + 1, 200)\n",
    "    \n",
    "    if type == 'pmf':\n",
    "        supp_X = np.arange(min_X-1, max_X + 1)\n",
    "        ax.bar(supp_X, X.pmf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PMF')\n",
    "    elif type == 'pdf':\n",
    "        ax.plot(supp_X, X.pdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PDF')\n",
    "    elif type == 'cdf':\n",
    "        ax.plot(supp_X, X.cdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('CDF')\n",
    "    else:\n",
    "        raise ValueError('type must be pmf or cdf')\n",
    "\n",
    "def decision(pvalue, alpha):\n",
    "    if pvalue < alpha:\n",
    "        print(f'reject H0: pvalue={pvalue} < {alpha}')  \n",
    "    else: \n",
    "        print(f'fail to reject H0: pvalue={pvalue} ≥ {alpha}')\n",
    "\n",
    "def standardize(X):\n",
    "    return (X - X.mean()) / X.std()\n",
    "\n",
    "\n",
    "def make_data(errors):\n",
    "    n = len(errors)\n",
    "    x1 = np.linspace(0, 1, n)\n",
    "    x2 = np.random.rand(n)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'x1': x1, \n",
    "        'x2': x2, \n",
    "        'y': 2 + 3*x1 + 4*x2 + errors\n",
    "    })\n",
    "\n",
    "def plot_regression(data, fit, residuals=True):\n",
    "    b = fit.params\n",
    "    b0, b1, b2 = *b, *np.zeros(3 - len(b))\n",
    "    y, x1, x2 = data.y, data.x1, data.x2\n",
    "    fig = px.scatter_3d(x=x1, y=x2, z=y)\n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "            xaxis_title='X1',\n",
    "            yaxis_title='X2',\n",
    "            zaxis_title='Y'),\n",
    "            margin=dict(l=0, r=0, b=0, t=0\n",
    "        )\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    \n",
    "    x1_grid, x2_grid = np.meshgrid(x1, x2)\n",
    "    yhat = b0 + (b1 * x1_grid) + (b2 * x2_grid)\n",
    "    fig.add_trace(\n",
    "        go.Surface(x=x1_grid, y=x2_grid, z=yhat, opacity=0.5,colorscale='Gray')\n",
    "    )\n",
    "    if residuals:\n",
    "        for i in range(len(x1)):\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(x=[x1[i], x1[i]], y=[x2[i], x2[i]], z=[b0 + b1*x1[i] + b2*x2[i], y[i]], mode='lines', line=dict(color='black', width=2))\n",
    "            )\n",
    "    fig.update_layout(showlegend=False, scene_camera=dict(eye=dict(x=2.0, y=0.5, z=0.1)))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfec7ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6dc53",
   "metadata": {},
   "source": [
    "## Key Takeaways from Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc992f31",
   "metadata": {},
   "source": [
    "#### Tuesday: \n",
    "We begined to discuss linear regression analysis (LRA), covering the basic format of it, matrix form of it, and how do we do the estimation. We then dive further into the three main interpretation of doing LRA, from optimization, correlation, and probabilistic MLE's perspective. We also discussed about both the model and coefficient interpretation along with anatomy of the regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69de7b",
   "metadata": {},
   "source": [
    "#### Thursday\n",
    "We discussed the main assumptions that we need for LRA to be perfomed, including independent, identical, normality, and linear in errors. We then discussed about main diffeernces in outliers and influential points. We further extended LRA to two extension cases using categorical $x$ and also multi independent variable $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaadbca8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abad0d9",
   "metadata": {},
   "source": [
    "# Regression Analysis: Quantiative Response & Predictor\n",
    "**We want to quantify what is the  uncertainty is and how to use them in inference.** Regression is a technqiue of both **Predictions**, **Inference**, and also **Analysis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a23137",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-2, 3, 50)\n",
    "y_values = 2 * x_values + 1 + np.random.normal(scale=0.3, size=x_values.shape)\n",
    "xy_numpy = np.round(np.column_stack((x_values, y_values)), 3)\n",
    "\n",
    "df = pd.DataFrame(np.round(xy_numpy, 2), columns=['x', 'y'])\n",
    "x, y = df['x'], df['y']\n",
    "sns.scatterplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8c2ea",
   "metadata": {},
   "source": [
    "### Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313ab33",
   "metadata": {},
   "source": [
    "The simple linear regression model assumes that the relationship between the two variables is linear, and can be expressed as:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $\\beta_0$ and $\\beta_1$ are the intercept and slope of the line, and $\\epsilon_i$ is the error term. The error term captures the difference between the observed value $y_i$ and the value predicted by the model $\\beta_0 + \\beta_1 x_i$. The goal of simple linear regression is to **estimate the values of $\\beta_0$ and $\\beta_1$ that best fit the data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8df81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(b0=(-4, 4, 0.5), b1=(-2, 2, 0.25))\n",
    "def plot_regression(b0, b1, residuals=False):\n",
    "    plt.plot(x, b0 + b1*x, color='r')\n",
    "    if residuals:\n",
    "        for i in range(len(x)):\n",
    "            plt.plot([x[i], x[i]], [b0 + b1*x[i], y[i]], color='black', alpha=0.5, linestyle='--')\n",
    "    sns.scatterplot(x=x, y=y)\n",
    "    plt.xlim(-3, 5); plt.ylim(-2, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402fb517",
   "metadata": {},
   "source": [
    "Residual says that what is the quantity of error that we are making comparing our regression prediction and the actual observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79db641",
   "metadata": {},
   "source": [
    "The line of best fit is given by the equation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n",
    "$$\n",
    "\n",
    "where $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are the estimated values of the intercept and slope, and $\\hat{y}$ is the predicted value of $y$. **So how can we best estimate these two values of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e6daf1",
   "metadata": {},
   "source": [
    "The method of least squares is used to estimate the values of $\\beta_0$ and $\\beta_1$ that minimize the sum of the squared differences between the observed and predicted values $\\hat{y}$:\n",
    "\n",
    "$$\n",
    "\\text{Loss}(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0, \\hat{\\beta}_1 = \\arg\\min_{\\beta_0 \\in \\mathbb{R}, \\beta_1 \\in \\mathbb{R}} \\text{Loss}(\\beta_0, \\beta_1)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f52bb",
   "metadata": {},
   "source": [
    "### Regression Equation In Matrix Form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52d679",
   "metadata": {},
   "source": [
    "Let $\\mathbf{x} = (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n$ be the vector of independent variables and $\\mathbf{y}$ be the vector of dependent variables, and let $\\mathbf{X}$ be the matrix of the independent variables along with a column of 1’s, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then the estimated values of $\\beta_0$ and $\\beta_1$ are given by:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{pmatrix} =\n",
    "(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} =\n",
    "\\begin{pmatrix} n & \\sum x_i \\\\ \\sum x_i & \\sum x_i^2 \\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix} \\sum y_i \\\\ \\sum x_i y_i \\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118dd49",
   "metadata": {},
   "source": [
    "First interpretation is that the regression is just a minimizor of this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ab325",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(x)\n",
    "\n",
    "X = np.column_stack((np.ones(n), x))\n",
    "b0, b1 = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "b0, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62901fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols('y ~ x', df).fit()\n",
    "\n",
    "model_table = model.summary().tables\n",
    "print(model_table[1])\n",
    "\n",
    "sns.scatterplot(x=x, y=y)\n",
    "plt.plot(x, b0 + b1*x, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c3688",
   "metadata": {},
   "source": [
    "### Estimation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6060a2",
   "metadata": {},
   "source": [
    "- The coefficients of the regression line are the estimated values of $\\beta_0$ and $\\beta_1$, i.e., $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ccd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model.params\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c68e1",
   "metadata": {},
   "source": [
    "- The predicted/fitted value of $y$ for a given value of $x$ is given by:\n",
    "    $$\n",
    "    \\hat{y} = \\begin{bmatrix} 1 & x \\end{bmatrix}\n",
    "    \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{pmatrix}\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b27cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = X @ b\n",
    "yhat_alt = model.fittedvalues\n",
    "\n",
    "np.testing.assert_allclose(yhat, yhat_alt)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(x, yhat, color='red')\n",
    "ax.scatter(x, y, color='dodgerblue', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9022b4",
   "metadata": {},
   "source": [
    "- Given a new value $x^*$, the predicted value of $y$ is given by:\n",
    "\n",
    "    $$\n",
    "    \\hat{y}^* = \\begin{bmatrix} 1 & x^* \\end{bmatrix}\n",
    "    \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{pmatrix}\n",
    "    = \\hat{\\beta}_0 + \\hat{\\beta}_1 x^*\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = 1.0\n",
    "np.array([1, x_star]).T.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d206ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\n",
    "    {'x':[x_star]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c493f",
   "metadata": {},
   "source": [
    "- The residual for the $i$-th observation is the difference between the observed and predicted values of $y$, i.e.,\n",
    "\n",
    "    $$\n",
    "    e_i = y_i - \\hat{y}_i\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de61066",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y - yhat\n",
    "residuals_alt = model.resid\n",
    "\n",
    "np.testing.assert_allclose(residuals, residuals_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a24096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(predicted = False, residuals = False)\n",
    "def plot_regression(predicted, residuals):\n",
    "    # plt.plot(x, b0 + b1*x, color='r')\n",
    "    if predicted:\n",
    "        plt.scatter(x, b0 + b1*x, color='r')\n",
    "    if residuals:\n",
    "        for i in range(len(x)):\n",
    "            plt.plot([x[i], x[i]], [b0 + b1*x[i], y[i]], color='black', alpha=0.5, linestyle='--')\n",
    "    sns.scatterplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad9867",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739bc658",
   "metadata": {},
   "source": [
    "# Interpretation of LR\n",
    "\n",
    "The optimziation problem we have setup previously can be calculated or derived from many forms/methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0be9c",
   "metadata": {},
   "source": [
    "> #### Optimization: Sum of Squares Interpretation\n",
    "It is just an optimziatipn problem!\n",
    "\n",
    "The fitted regression equation $ y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x $ can be interpreted as the best linear approximation to the relationship between $ x $ and $ y $ in terms of minimizing the sum of squares of the residuals, i.e.,\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0, \\hat{\\beta}_1 = \\min_{\\beta_0 \\in \\mathbb{R}, \\beta_1 \\in \\mathbb{R}} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0, \\hat{\\beta}_1 = \\min_{\\beta_0 \\in \\mathbb{R}, \\beta_1 \\in \\mathbb{R}} \\sum_{i=1}^{n} (e_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4a3e2",
   "metadata": {},
   "source": [
    "> #### Correlation: Coefficients Interpretation\n",
    "Correlation coefficient looks at associations between varaibales:\n",
    "\n",
    "The slope of the regression line $\\beta_1$ is a measure of the correlation between the independent and dependent variables, i.e.,\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum\\limits_{i=1}^{n} (x_i - \\bar{x})^2} = r_{xy} \\cdot \\frac{s_y}{s_x}\n",
    "$$\n",
    "\n",
    "where $s_x$ and $s_y$ are the standard deviations of $x$ and $y$, and $r_{xy}$ is the correlation coefficient between $x$ and $y$ given by:\n",
    "\n",
    "$$\n",
    "r_{xy} = \\frac{\\sum\\limits_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum\\limits_{i=1}^{n} (x_i - \\bar{x})^2 \\cdot \\sum\\limits_{i=1}^{n} (y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "Regression slope essentially **captures a rescaled version of the correlation between x and y**. It is rescaled by the standadr deviation of x and standard deviation of y.\n",
    "\n",
    "- It is rescale is to to handle the scale differences between variables if they exist on different scale.\n",
    "\n",
    "- There is no optimziation happening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a16f88",
   "metadata": {},
   "source": [
    "> #### Probabilistic: MLE Interpretation\n",
    "\n",
    "The simple linear regression model assumes that the relationship between the two variables is linear, and can be expressed as:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n",
    "$$\n",
    "\n",
    "- $\\beta_0$ and $\\beta_1$ are the intercept and slope of the line, and $\\epsilon_i$ is the error term.\n",
    "- The error term captures the difference between the observed value $y_i$ and the value predicted by the model $\\beta_0 + \\beta_1 x_i$.\n",
    "\n",
    "Now suppose the error terms $\\epsilon_i$ are normally distributed with mean 0 and variance $\\sigma^2$, i.e.,\n",
    "\n",
    "$$\n",
    "\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_n \\sim \\mathcal{N}(0, \\sigma^2).\n",
    "$$\n",
    "\n",
    "Then the distribution of the dependent variable $y$ **given** the independent variable $x$ is given by:\n",
    "\n",
    "$$\n",
    "y_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, \\sigma^2).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a481ebe",
   "metadata": {},
   "source": [
    "If $y_i | x_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, \\sigma^2)$, the probability density function of $y_i$ given $x_i$ is given by:\n",
    "\n",
    "$$\n",
    "f(y_i | x_i, \\beta_0, \\beta_1, \\sigma^2) =\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n",
    "\\exp\\left(-\\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "and that\n",
    "\n",
    "$$\n",
    "\\log f(y_i | x_i, \\beta_0, \\beta_1, \\sigma^2) =\n",
    "\\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) -\n",
    "\\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "The log-likelihood function of the parameters $\\beta_0, \\beta_1,$ and $\\sigma^2$ given the observed data is given by:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta_0, \\beta_1, \\sigma^2 | x, y) = \\sum_{i=1}^{n} \\log f(y_i | x_i, \\beta_0, \\beta_1, \\sigma^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "This looks very much like the negative of the loss function we had earlier in the optimization interpretation. The maximum likelihood estimates of the parameters $\\beta_0$, $\\beta_1$, and $\\sigma^2$ are given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_0}, \\hat{\\beta_1}, \\hat{\\sigma^2} = \\arg\\max_{\\substack{\\sigma^2 \\in \\mathbb{R}^+ \\\\ \\beta_0 \\in \\mathbb{R} \\\\ \\beta_1 \\in \\mathbb{R}}} \\ell(\\beta_0, \\beta_1, \\sigma^2 | x, y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\arg\\min_{\\substack{\\sigma^2 \\in \\mathbb{R}^+ \\\\ \\beta_0 \\in \\mathbb{R} \\\\ \\beta_1 \\in \\mathbb{R}}} \\left( \\underbrace{\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2}_{\\text{Sum of squares}} - \\frac{n}{2} \\log(2\\pi\\sigma^2) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630259d",
   "metadata": {},
   "source": [
    "From this, we get the same estimates of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ as before, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} \n",
    "\\hat{\\beta}_0 \\\\ \n",
    "\\hat{\\beta}_1 \n",
    "\\end{pmatrix}\n",
    "=\n",
    "(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "But, we also get an estimate of the variance of the error term $\\hat{\\sigma}^2$ given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n - 2} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n - 2} \\sum_{i=1}^{n} e_i^2\n",
    "$$\n",
    "\n",
    "*(this is actually a REML estimator as opposed to an ML estimator)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a629bc",
   "metadata": {},
   "source": [
    "**It is this perspective of the regression analysis that can allow us to form hypothesis test, confidence intervals, and find $p$-values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8775c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1ceb9",
   "metadata": {},
   "source": [
    "# Interpretations of Model & Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ebd14",
   "metadata": {},
   "source": [
    "> #### Interpretation of Probabilistic Model\n",
    "\n",
    "Conditional distribution of y given x:\n",
    "\n",
    "$$\n",
    "y | x \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x, \\sigma^2)\n",
    "$$\n",
    "\n",
    "therefore,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(y | x) = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "and, **based on the data** we have, **our best guess** of $\\mathbb{E}(y | x)$ is given by $\\hat{y}(x)$, i.e.,\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fdf05f",
   "metadata": {},
   "source": [
    "> #### Interpretation of Coefficients\n",
    "\n",
    "##### Interpretation of $\\beta_0$:\n",
    "if $x = 0$, then\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(y | x = 0) = \\beta_0\n",
    "$$\n",
    "\n",
    "so,\n",
    "\n",
    "$$\n",
    "\\hat{y}(0) = \\hat{\\beta}_0\n",
    "$$\n",
    "\n",
    "In other words, $\\hat{\\beta}_0$ is the **expected value of the dependent variable when $x = 0$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea6b14",
   "metadata": {},
   "source": [
    "##### Interpretation of $\\beta_1$:\n",
    "\n",
    "Again, back to this model:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(y | x) = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} \\mathbb{E}(y | x) = \\beta_1\n",
    "$$\n",
    "\n",
    "Therefore, $\\beta_1$ is the **expected change in the dependent variable $y$** for an **infinitesimal change in the independent variable $x$**.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} \\hat{y}(x) = \\hat{\\beta}_1\n",
    "$$\n",
    "\n",
    "And, $\\hat{\\beta}_1$ is **our best guess** of the expected change in the dependent variable $y$ for an infinitesimal change in the independent variable $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d5ad21",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd893a57",
   "metadata": {},
   "source": [
    "# Anatomy of Regression Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83f04c",
   "metadata": {},
   "source": [
    "Again, there are a lot of values that we can get from a regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aca237",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10e689",
   "metadata": {},
   "source": [
    "> #### Significance of Coefficients\n",
    "\n",
    "Each p-value in the summary table is the p-value of the two-sided hypothesis test:\n",
    "\n",
    "$$\n",
    "H_0: \\beta = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_a: \\beta \\neq 0\n",
    "$$\n",
    "\n",
    "It is a hypotheiss test checking the associations between varaiables (not independent, independent is a **stronger** statement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ea6b57",
   "metadata": {},
   "source": [
    "> #### Predcited Values & Residuals\n",
    "We have three main quantities that are used to evaluate the goodness of fit of the regression model:\n",
    "\n",
    "| Quantity  | Description |\n",
    "|-----------|------------------------------------------|\n",
    "| $ y $   | Observed values of the dependent variable |\n",
    "| $ \\hat{y} $ | Predicted values of the dependent variable |\n",
    "| $ \\bar{y} $ | Mean of the observed values of the dependent variable |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79609cf9",
   "metadata": {},
   "source": [
    "> #### Different Sums of Squares\n",
    "\n",
    "In this, we have three important **sums of squares** in the regression model:\n",
    "\n",
    "| **Sum of squares** | **Equation** | **Intuition** |\n",
    "|--------------------|-------------|--------------|\n",
    "| $SS_{\\text{Tot}}$ | $\\sum\\limits_{i=1}^{n} (y_i - \\bar{y})^2$ | How much variability is present in the observed values $y$ |\n",
    "| $SS_{\\text{Reg}}$ | $\\sum\\limits_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2$ | How much variability the regression model $\\hat{y}$ reproduces |\n",
    "| $SS_{\\text{Res}}$ | $\\sum\\limits_{i=1}^{n} (y_i - \\hat{y}_i)^2$ | How much variability in $y$ the regression model $\\hat{y}$ is unable to reproduce |\n",
    "\n",
    "1. **Total sum of squares (TSS):** The sum of the squared differences between the observed values of $y$ and the mean of $y$.\n",
    "\n",
    "2. **Regression sum of squares (RSS):** The sum of the squared differences between the predicted values of $y$ and the mean of $y$. This is how many `total explained variability`.\n",
    "\n",
    "3. **Residual sum of squares (RSS):** The sum of the squared differences between the observed values of $y$ and the predicted values of $y$. This is the `total unexplained variance`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba75d1",
   "metadata": {},
   "source": [
    "> #### R-squared\n",
    "The coefficient of determination $R^2$ is a measure of how well the regression model fits the data, and is given by:\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{SS_{\\text{Reg}}}{SS_{\\text{Tot}}}\n",
    "$$\n",
    "\n",
    "Intuitively, $R^2$ is the proportion of the total variability in the dependent variable $y$ that is explained by the regression model.\n",
    "\n",
    "This is the main formula for $R^2$, it **only has the valid interpretation** if you set up an proper linear regression model like the ones we did. You may get a  `negative` $R^2$ if your assumptions of LR is not met.\n",
    "\n",
    "- In addition, when the **Pythagorean Theorem** holds:\n",
    "\n",
    "    $$\n",
    "    SS_{\\text{Tot}} = SS_{\\text{Reg}} + SS_{\\text{Rss}}\n",
    "    $$\n",
    "\n",
    "- We can recover the correlation coefficient:\n",
    "\n",
    "    $$\n",
    "    R^2 = \\frac{SS_{\\text{Tot}} - SS_{\\text{Res}}}{SS_{\\text{Tot}}} = 1 - \\frac{SS_{\\text{Res}}}{SS_{\\text{Tot}}}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7761b417",
   "metadata": {},
   "source": [
    "> #### F-statistic\n",
    "\n",
    "The $F$-statistic is a measure of how well the model explains the variance in the data. It is given by:\n",
    "\n",
    "$$\n",
    "\\hat{F} = \\frac{SS_{\\text{Reg}} / df_{\\text{Reg}}}{SS_{\\text{Res}} / df_{\\text{Res}}}\n",
    "$$\n",
    "\n",
    "The degree of regression is how many covariance, how many independent variables you have in the regression model:\n",
    "\n",
    "$$\n",
    "df_{\\text{Reg}} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "df_{\\text{Res}} = n - 2\n",
    "$$\n",
    "\n",
    "Intuitively, the $F$-statistic is the **ratio** of the explained variance to the unexplained variance in the data.\n",
    "\n",
    "- We try to have $SS_{\\text{Res}}$ to be as low as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b339f8",
   "metadata": {},
   "source": [
    "### F-statistic Hypothesis Test\n",
    "\n",
    "Idea is that there is a Chi-square distribution some where. Maybe we can use it for hypothesis tesing?\n",
    "\n",
    "The $F$-statistic is used to test the null hypothesis that the model is no better than the null model, i.e., the model with no predictors.\n",
    "\n",
    "$$\n",
    "H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_a: \\text{At least one } \\beta_j \\neq 0\n",
    "$$\n",
    "\n",
    "The $F$-statistic follows an $F(df_{\\text{Reg}}, df_{\\text{Res}})$ distribution under the null hypothesis, i.e.,\n",
    "\n",
    "$$\n",
    "\\hat{F} = \\frac{SS_{\\text{Reg}} / df_{\\text{Reg}}}{SS_{\\text{Res}} / df_{\\text{Res}}} \\sim F(df_{\\text{Reg}}, df_{\\text{Res}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59353c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary().tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a6016",
   "metadata": {},
   "source": [
    "⚠️ More on this later during ANOVA ⚠️ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae523a5e",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8f965",
   "metadata": {},
   "source": [
    "# Diagnostics Via Holding Assumption\n",
    "LR analysis result is very delicate and we need to make sure that **all of the rpevious assumptions are hold**. Here are the main asssumptions of the regression model. Let's use the following generated two data set as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(-2, 3, 50)\n",
    "y1 = 2 * x1 + 1 + np.random.normal(scale=0.1, size=x1.shape)\n",
    "df1 = pd.DataFrame({'x': x1, 'y': y1})\n",
    "\n",
    "x2 = np.linspace(-2, 4, 100)\n",
    "y2 = 2 * x2 + 1 + 3 * np.sin(1.5 * x2) + np.random.normal(scale=0.1, size=x2.shape)\n",
    "df2 = pd.DataFrame({'x': x2, 'y': y2})\n",
    "\n",
    "dfs = [df1, df2]\n",
    "models = [smf.ols('y ~ x', df).fit() for df in dfs]\n",
    "residuals = [model.resid for model in models]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f'Data {i+1}: \\n {model.summary().tables[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4811ba",
   "metadata": {},
   "source": [
    "Notice that these two dataset's coefficent and intercept are essentially identical. **so relying on the table is not sufficient enough**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195780d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for i, (df, model, ax) in enumerate(zip(dfs, models, axs)):\n",
    "    sns.scatterplot(x='x', y='y', data=df, ax=ax)\n",
    "    ax.plot(df['x'], model.fittedvalues, color='red')\n",
    "    ax.set_title(f'Data {i+1}')\n",
    "    ax.set_xlim(-3, 5); ax.set_ylim(-4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6e86e8",
   "metadata": {},
   "source": [
    "We usually say that errors are `normally distributed`, they are `independent`, they are `identical`:\n",
    "\n",
    "> 1. **Independence**\n",
    "\n",
    "$$\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_n \\quad \\text{are independent}$$\n",
    "\n",
    "> 2. **Identical / homoscedasticity** \n",
    "\n",
    "$$\\text{The errors have constant variance, i.e.,} \\quad \\text{Var}(\\epsilon_i) = \\sigma^2 \\quad \\text{for all }i.\n",
    "$$\n",
    "\n",
    "> 3. **Normally distributed** \n",
    "\n",
    "$$\\epsilon_i \\sim N(0, \\sigma^2)$$\n",
    "\n",
    "> 4. **Linearity** \n",
    "\n",
    "The relationship between the independent variables and the response is linear\n",
    "\n",
    "> 5. **Completeness** \n",
    "\n",
    "All the variables which influence the response $y$ are included in the covariates.\n",
    "- This assumption can be handled by considering regression models with random slope and/or random intercept\n",
    "- It is a important assumption, but we  will not go into it in this class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57138ea8",
   "metadata": {},
   "source": [
    "**ANY INFERENCE FROM THE REGRESSION MODEL IS VALID ONLY IF THE ASSUMPTIONS ARE MET**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b1d87",
   "metadata": {},
   "source": [
    "## Checking Assumptions: More of an Art than Science\n",
    "\n",
    "**Notice, we are checking residuals, or essentially the  distribution of `noises` in the probabilistic reasoning sense**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c33cc",
   "metadata": {},
   "source": [
    "> ##### Assumption 1: Independence in Errors\n",
    "The **independence** assumption on the $\\epsilon_i$ requires that the errors are all independent.\n",
    "\n",
    "* plotting the `residuals` against the `covariates` $x$\n",
    "\n",
    "* plotting the `ACF functions` for the residuals\n",
    "    - At lag zero, it should have correlation 1 and then immediately after\n",
    "    - Something slowly decaying is a clear sign of association across observations, it is sequential observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "lags = 20\n",
    "for i, resid in enumerate(residuals):\n",
    "    ax[i].bar(range(lags+1), acf(resid, nlags=lags))\n",
    "    ax[i].set_xlabel('lag'); ax[i].set_ylabel('ACF'); ax[i].set_title(f'Data {i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2684ef",
   "metadata": {},
   "source": [
    "> ##### Assumption 2: Heteroscedasticity in Errors\n",
    "The **identical** assumption on the $\\epsilon_i$ requires that **the errors have constant variance**. We look at the standardlized resisduals:\n",
    "\n",
    "$$\n",
    "e_1, \\cdots e_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "e_i \\to \\frac{e_i - \\overline{e}}{\\text{SD}(e)}\n",
    "$$\n",
    "\n",
    "\n",
    "* plotting the `residuals` against the `predicted values` of $y$\n",
    "* plotting the `residuals` against the `independent variable` $x$.\n",
    "\n",
    "Ideally, you wnat to see uniform randomness instead of obvious patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ad7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for i, (df, residual) in enumerate(zip(dfs, residuals)):\n",
    "    sns.scatterplot(x=df['x'], y=standardize(residual), ax=axs[i])\n",
    "    axs[i].xaxis.set_label_text('x')\n",
    "    axs[i].yaxis.set_label_text('Standardized residuals')\n",
    "    axs[i].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    axs[i].set_title(f'Data {i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571454f0",
   "metadata": {},
   "source": [
    "> ##### Assumption 3: Normality Distributed Errors\n",
    "The residuals should be **normally distributed**.\n",
    "\n",
    "* Plotting a `histogram of the residuals` and checking for symmetry and normality.\n",
    "    - You want to see it more bell shapes\n",
    "* Using a `QQ-plot` to compare the residuals to a normal distribution.\n",
    "    - See if it is along the axis\n",
    "* Performing a `Shapiro-Wilk test` for normality.\n",
    "    - Only large samples would this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc3859",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for i, resid in enumerate(residuals):\n",
    "    sns.histplot(standardize(resid), bins=50, ax=axs[0, i])\n",
    "    sm.qqplot(standardize(resid), line='45', ax=axs[1, i])\n",
    "    axs[0, i].set_title(f'Data {i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2cb9d",
   "metadata": {},
   "source": [
    "Usually we wouldn't see normally distributed in real data, but we try to see symetrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f383c8",
   "metadata": {},
   "source": [
    "Running the `Shapiro-Wilk test` for normality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de42919",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "\n",
    "for i, resid in enumerate(residuals):\n",
    "    shapiro = stats.shapiro(standardize(resid))\n",
    "    print(f' Data {i+1}: \\t')\n",
    "    decision(shapiro.pvalue, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c7daa",
   "metadata": {},
   "source": [
    "> ##### Assumption 4: Linearity Errors\n",
    "\n",
    "The **relationship between the independent and dependent variables should be linear**. We look at the standardlized resisduals:\n",
    "\n",
    "$$\n",
    "e_1, \\cdots e_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "e_i \\to \\frac{e_i - \\overline{e}}{\\text{SD}(e)}\n",
    "$$\n",
    "\n",
    "* plotting the `residuals` against the `Fitted values` $x$.\n",
    "    - Use the `Box-cox` analysis.\n",
    "* If the residuals are **randomly scattered around 0**, then the assumption of linearity is likely satisfied.\n",
    "    - We want to see that points should be **isotopically** randomly distributed (data 1 holds, not data 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eafa96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for i, (model, resid) in enumerate(zip(models, residuals)):\n",
    "    sns.scatterplot(x=model.fittedvalues, y=standardize(resid), ax=axs[i])\n",
    "    axs[i].xaxis.set_label_text('Fitted values')\n",
    "    axs[i].yaxis.set_label_text('Standardized residuals')\n",
    "    axs[i].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    axs[i].set_title(f'Data {i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c63ab8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1d209",
   "metadata": {},
   "source": [
    "# Unusual Observations &  Influential points\n",
    "NOTCIE!!! They are not the same thing!!\n",
    "\n",
    "* **Outliers** are observations with **large residuals**.\n",
    "    * $x_i$ is an outlier if $e_i$ is large\n",
    "    \n",
    "* **Influential points** are observations that **have a large effect on the regression model**. \n",
    "    * $x_i$ is influential if $|\\hat\\beta_{(-i)} - \\hat\\beta|$ is large (regression model after having deleted the $i$-th observation) Ideally, if we just delete one data, we should see the model to about the same\n",
    "\n",
    "The most famous example comes from [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75774ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\n",
    "x4 = [8.001, 8.002, 8.003, 8.004, 7.999, 7.998, 7.997, 19, 8.000, 8.005, 7.996]\n",
    "\n",
    "ys = [\n",
    "    [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68], \n",
    "    [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74], \n",
    "    [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73], \n",
    "    [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n",
    "]\n",
    "\n",
    "data = [pd.DataFrame({'x': x, 'y': y}) for y in ys]\n",
    "data[4-1].x = x4\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
    "\n",
    "for i, (df, ax) in enumerate(zip(data, axs.ravel())):\n",
    "    ax.scatter(df.x, df.y)\n",
    "    ax.set_title(f'Dataset {i+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363891a3",
   "metadata": {},
   "source": [
    "Same `statiscial meaning` are calculated here but the true generated data comes from 4 very different data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [smf.ols('y ~ x', data=df).fit() for df in data]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
    "\n",
    "for i, (df, ax) in enumerate(zip(data, axs.ravel())):\n",
    "    ax.scatter(df.x, df.y)\n",
    "    x = np.linspace(df.x.min(), df.x.max(), 100)\n",
    "    y = results[i].params['Intercept'] + results[i].params['x'] * x\n",
    "    ax.plot(x, y, color='red')\n",
    "    ax.set_title(f'y = {results[i].params[\"Intercept\"]:.2f} + {results[i].params[\"x\"]:.2f}x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c0187",
   "metadata": {},
   "source": [
    "All same linear regression line! Intercept 3 and slope of 1/2\n",
    "\n",
    "- 3rd data set has an outlier (large residuals)\n",
    "- 4th data set has influential point (remove a point, significant different regressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970d71f",
   "metadata": {},
   "source": [
    "| Anscombe's Quartet | $\\phantom{1}$  |\n",
    "|---|---|\n",
    "Satisfies assumptions | Nonlinear data |\n",
    "Outlier | Influential point |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89ea21",
   "metadata": {},
   "source": [
    "The influence of a point is usually measured using Cook's distance. Let's try to get influence of points in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afc591",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[3].get_influence().summary_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca4492",
   "metadata": {},
   "source": [
    "If we are gonna delete the 7th data point, we wil get a slope with `-1657.969406`\n",
    "- Cook's distance is the effect something would have on the residuals\n",
    "\n",
    "Let's look at the cook's  distance graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'cooks_d'\n",
    "fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "for i, (result, ax) in enumerate(zip(results, axs.ravel())):\n",
    "    yvalue = result.get_influence().summary_frame()[metric]\n",
    "    xvalue = range(len(yvalue))\n",
    "    ax.scatter(xvalue, yvalue)\n",
    "    ax.set_xlabel('observation')\n",
    "    ax.set_title(f'Dataset {i+1}')\n",
    "    ax.set_ylabel(f'{metric}'); plt.title(f'{metric} for each observation')\n",
    "    # ax.set_ylim(-50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2269bc01",
   "metadata": {},
   "source": [
    "similarly, we can look at the influence graph where the size of the buble is proportional to Cook's distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c4320",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "for result, ax in zip(results, axs.ravel()):\n",
    "    # sm.graphics.influence_plot(result, ax=ax)\n",
    "    result.get_influence().plot_influence(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15473cf",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a0171",
   "metadata": {},
   "source": [
    "# Regression: Continuous Response $y$ vs. Categorical Covariate $x$\n",
    "\n",
    "Suppose we have a dataset of $n$ observations $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$, where \n",
    "\n",
    "* $x_i$ is a categorical variable with $k$ levels ($x_j \\in \\{0, 1, 2, \\cdots, k-1\\}$)\n",
    "* $y_i$ is a continuous variable ($y \\in \\R$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527eb8f",
   "metadata": {},
   "source": [
    "Can we can still fit the same regression model as before?\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\epsilon_i$ is the error term.\n",
    "\n",
    "**What does it even mean for $\\beta_1$ to be the expected change in $y$ for a one-unit change in $x$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71142f73",
   "metadata": {},
   "source": [
    "> **NOTICE THAT**:\n",
    "> \n",
    ">* For an ordinal covariate $x_j$, a ``unit change`` **is not the same across the different levels**\n",
    ">     * e.g., if $x_j = 0, 1, 2$ corresponds to $x_j = \\text{low}, \\text{medium}, \\text{high}$, then the change from low to medium is not necessarily the same as the change from medium to high.\n",
    "> * For a categorical covariate $x_j$, a ``unit change`` **may not even have a meaningful interpretation**\n",
    ">    * e.g., if $x_j = 0, 1$ corresponds to $x_j = \\text{red}, \\text{blue}$, then the change from $x_j = 0$ to $x_j = 1$ needs to be interpreted with caution.\n",
    "\n",
    "Let's say that:\n",
    "\n",
    "* $x_j = 1$ for `smokers` and $x_j = 0$ for `non-smokers` then $\\beta_j$ measures the expected difference in $y$ for `smokers` - `non-smokers`\n",
    "* $x_j = 1$ for `non-smokers` and $x_j = 0$ for `smokers` then $\\beta_j$ measures the expected difference in $y$ for `non-smokers` - `smokers`\n",
    "\n",
    "> This is known as the **reference level** problem and is a common source of confusion in interpreting the coefficients of a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a276a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['A', 'B', 'C']\n",
    "x = np.random.choice(categories, size=100)\n",
    "\n",
    "y = []\n",
    "for category in x:\n",
    "    if category == 'A':\n",
    "        y.append(np.random.normal(loc=3.5, scale=0.7))  # Higher mean\n",
    "    elif category == 'B':\n",
    "        y.append(np.random.normal(loc=1, scale=0.5))  # Lower mean, some outliers\n",
    "    else:  # Category 'C'\n",
    "        y.append(np.random.normal(loc=1.5, scale=1.2))  # More spread\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "sns.boxplot(x='x', y='y', hue='x', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93fd616",
   "metadata": {},
   "source": [
    "How does what boxplot here effects the other boxplot, but we want to do it in the fashion taht it is agonistic of the orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4781ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols('y~x', df).fit()\n",
    "print(model.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879db976",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dummy = pd.get_dummies(df['x'], drop_first=True)\n",
    "x_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f185456",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((np.ones(df.shape[0]), x_dummy))\n",
    "y = df['y']\n",
    "\n",
    "b = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.linalg.det(X.T @ X), ndigits=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d0d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x'] = pd.Categorical(df['x'], ['B', 'C', 'A'])\n",
    "model_alt = smf.ols('y~x', df).fit()\n",
    "print(model_alt.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802055b",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "When $x$ is categorical with $k$ levels, the model\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "$$\n",
    "\n",
    "is actually, \n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_{1, 1} \\mathbf{1}(x_i = 1) + \\beta_{1, 2} \\mathbf{1}(x_i = 2) + \\cdots + \\beta_{1, k-1} \\mathbf{1}(x_i = k-1) + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\mathbf{1}(x = i)$ is the indicator function, i.e, \n",
    "\n",
    "$$\n",
    "\\mathbf{1}(x = i) = \\begin{cases}\n",
    "1 & \\text{if } x = i\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "and the category $x = 0$ is the **reference category**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d32dbc",
   "metadata": {},
   "source": [
    "If $x = 0$:\n",
    "$$\n",
    "\\E(y | x=0) = \\beta_0\n",
    "$$\n",
    "\n",
    "and if $x = j$ for some category $j \\neq 0$\n",
    "$$\n",
    "\\E(y | x=j) = \\beta_0 + \\beta_{1, j}\n",
    "$$\n",
    "\n",
    "So, the coefficient $\\beta_{1, j}$  can be written as\n",
    "\n",
    "$$\n",
    "\\beta_{1, j} = \\E(y | x=j) - \\E(y | x=0)\n",
    "$$\n",
    "\n",
    "In other words, it's the expected change in $y$ when $x=j$ is from category $j$ **as opposed to** the reference category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d631ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b025551",
   "metadata": {},
   "source": [
    "# Regression: Multiple Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ad60f",
   "metadata": {},
   "source": [
    "Suppose we $p$ variables $x_1, x_2, \\dots, x_p$ and a dependent variable $y$, and a dataset of $n$ observations\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x_{11} & x_{12} & \\dots & x_{1p} & y_1 \\\\\n",
    "x_{21} & x_{22} & \\dots & x_{2p} & y_2 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\dots & x_{np} & y_n \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "where $x_{ij}$ is the value of:\n",
    "\n",
    "* the $i$-th observation of\n",
    "* the $j$-th independent variable\n",
    "\n",
    "and $y_i$ is the value of the dependent variable for the $i$-th observation.\n",
    "\n",
    "\n",
    "> **How can we model the relationship between the $p$ independent variables and the dependent variable?**\n",
    "\n",
    "Suppose $p=2$ and we have two independent variables $x_1$ and $x_2$, and a dependent variable $y$\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n",
    "$$\n",
    "\n",
    "$$\n",
    "min \\sum_{i} (y - \\beta_0 - \\beta_1 x_1 - \\beta_2 x_2)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc1d4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.rand(200) - 0.5\n",
    "x2 = np.random.rand(200) - 0.5\n",
    "y = 5 + 2*x1 + 3*x2 + 0.5 * np.random.normal(size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ef1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(b0=(-10, 10, 0.5), b1=(-5, 5, 0.25), b2=(-5, 5, 0.25), residuals=False)\n",
    "def _(b0, b1, b2, residuals):\n",
    "    fig = px.scatter_3d(x=x1, y=x2, z=y)\n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "            xaxis_title='X1',\n",
    "            yaxis_title='X2',\n",
    "            zaxis_title='Y'),\n",
    "            margin=dict(l=0, r=0, b=0, t=0\n",
    "        )\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    \n",
    "    x1_grid, x2_grid = np.meshgrid(np.linspace(-0.5, 0.5, 100), np.linspace(-0.5, 0.5, 100))\n",
    "    yhat = b0 + b1 * x1_grid + b2 * x2_grid\n",
    "    fig.add_trace(\n",
    "        go.Surface(x=x1_grid, y=x2_grid, z=yhat, opacity=0.5)\n",
    "    )\n",
    "    if residuals:\n",
    "        for i in range(len(x1)):\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(x=[x1[i], x1[i]], y=[x2[i], x2[i]], z=[b0 + b1*x1[i] + b2*x2[i], y[i]], mode='lines', line=dict(color='black', width=2))\n",
    "            )\n",
    "    fig.update_layout(showlegend=False, scene_camera=dict(eye=dict(x=2.0, y=0.5, z=0.1)))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014c9fc5",
   "metadata": {},
   "source": [
    "> #### Estimation\n",
    "\n",
    "The line of best fit is given by the equation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\dots + \\hat{\\beta}_p x_p\n",
    "$$\n",
    "\n",
    "where $\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\dots, \\hat{\\beta}_p$ are the estimated values of the intercept and slopes, and $\\hat{y}$ is the predicted value of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c78c0c",
   "metadata": {},
   "source": [
    "> #### Matrix form\n",
    "\n",
    "Let $\\Xv$ be the matrix of the independent variables along with a column of 1's, i.e.,\n",
    "\n",
    "$$\n",
    "\\Xv = \\Big[\\mathbf{1} \\Big| \\xv_1 \\Big| \\xv_2 \\Big| \\dots \\Big| \\xv_p\\Big] =\n",
    "\\begin{pmatrix}\n",
    "1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
    "1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_{n1} & x_{n2} & \\dots & x_{np} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and let \n",
    "* $\\yv = (y_1, y_2, \\dots, y_n) \\in \\R^n$ be the vector of observed values\n",
    "* $\\epsilonv = (\\epsilon_1, \\epsilon_2, \\cdots, \\epsilon_n)$ be the vector of errors\n",
    "* $\\betav = (\\beta_0, \\beta_1, \\cdots, \\beta_p)$ be the vector of regression coefficients\n",
    "\n",
    "Then the system of equations is given by\n",
    "\n",
    "$$\n",
    "\\yv = \\Xv \\betav + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124009f4",
   "metadata": {},
   "source": [
    "> #### Coefficients\n",
    "\n",
    "By maximum likelihood estimation, the estimated values of $\\betav = (\\beta_0, \\beta_1, \\dots, \\beta_p) \\in \\R^{p+1}$ are given by:\n",
    "\n",
    "$$\n",
    "\\color{blue}{\\boxed{\\hat\\betav = (\\Xv^T \\Xv)^{-1} \\Xv^T \\yv}}\n",
    "$$\n",
    "\n",
    "\n",
    "Equivalently,\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\hat{\\beta}_0 \\\\ \\\\\n",
    "\\hat{\\beta}_1 \\\\ \\\\\n",
    "\\vdots \\\\ \\\\\n",
    "\\hat{\\beta}_p\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "n & \\sum x_{i1} & \\sum x_{i2} & \\dots & \\sum x_{ip} \\\\ \\\\\n",
    "\\sum x_{i1} & \\sum x_{i1}^2 & \\sum x_{i1}x_{i2} & \\dots & \\sum x_{i1}x_{ip} \\\\ \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\\\\n",
    "\\sum x_{ip} & \\sum x_{i1}x_{ip} & \\sum x_{i2}x_{ip} & \\dots & \\sum x_{ip}^2\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "\\sum y_i \\\\ \\\\\n",
    "\\sum x_{i1}y_i \\\\ \\\\\n",
    "\\vdots \\\\ \\\\\n",
    "\\sum x_{ip}y_i\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n",
    "model = smf.ols('y ~ x1 + x2', df).fit()\n",
    "print(model.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab4e41",
   "metadata": {},
   "source": [
    "### Anatomy of This Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a395045",
   "metadata": {},
   "source": [
    "> #### Predicted values\n",
    "The predicted/fitted value of $y$ for a given value of $x$ is given by:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46389145",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat y = [1 | x_1 | x_2 \\dots x_p]^\\top \\begin{bmatrix}\n",
    "\\hat\\beta_0\\\\\n",
    "\\hat\\beta_1\\\\\n",
    "\\dots\\\\\n",
    "\\hat\\beta_1\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959de38",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat y = \\hat\\beta_0 + \\hat\\beta_1x_1 + \\dots + \\hat\\beta_p x_p\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10f3458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.fittedvalues\n",
    "# sns.histplot(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d2b92",
   "metadata": {},
   "source": [
    "Given a new value $x^*$ the predicted value of $y$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y^*} &= \n",
    "\\begin{bmatrix}\n",
    "1 | & x_1^* | \\dots | x_p^*\n",
    "\\end{bmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\hat\\beta_0 \\\\\n",
    "\\hat\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat\\beta_p\n",
    "\\end{pmatrix}\\\\ \\\\\n",
    "&= \\hat\\beta_0 + \\hat\\beta_1x_1^* + \\dots + \\hat\\beta_p x_p^*\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afefa55e",
   "metadata": {},
   "source": [
    "> #### Residuals\n",
    "The residual for the $i$-th observation is the difference between the observed and predicted values of $y$, i.e.,\n",
    "\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = model.resid\n",
    "sns.histplot(residuals, bins=50, stat='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ebf92",
   "metadata": {},
   "source": [
    "### Interpretation of Model and Coefficients\n",
    "\n",
    "\n",
    "> #### Interpretation of Model\n",
    "The interpretation of the regression model is similar to the univariate regression model. The observed value of $y$ is coming from a normal distribution with the mean of the **linear combinations**:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$, and the distribution of $y_i$ given the independent variables is given by:\n",
    "\n",
    "$$\n",
    "y_i | x_{i1}, x_{i2}, \\dots, x_{ip} \\sim N(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}, \\sigma^2)\n",
    "$$\n",
    "\n",
    "The estimated value $\\hat\\betav$ is the maximum likelihood estimate of the population parameters $\\betav$, and the estimated value $\\hat\\sigma^2$ is the maximum likelihood estimate of the variance of the error term $\\epsilon_i$ given by:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2= \\frac{1}{n-p} \\sum_{i=1}^n (\\epsilon_i)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b855c3",
   "metadata": {},
   "source": [
    "**But the interpretation of the coefficients is slightly different.**\n",
    "\n",
    "> #### Interpretation of Coefficients\n",
    "\n",
    "The coefficients of the regression model are the estimated values of $\\beta_0, \\beta_1, \\dots, \\beta_p$, i.e., $\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p$.\n",
    "\n",
    "\n",
    "Recall that if $y_i \\sim N(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}, \\sigma^2)$, then the expected value of $y_i$ given the independent variables is given by:\n",
    "\n",
    "$$\n",
    "\\mathbb E\\Big(y_i \\Big| x_{i1}, x_{i2}, \\dots, x_{ip}\\Big) = \\hat y_i = \\hat\\beta_0 + \\hat\\beta_1 x_{11} + \\hat\\beta_2 x_2 + \\dots + \\hat\\beta_p x_p\n",
    "$$\n",
    "\n",
    "Then, the partial derivative of the expected value of $y_i$ with respect to the $j$-th independent variable is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat y}{\\partial x_j} = \\hat\\beta_j\n",
    "$$\n",
    "\n",
    "The interpretation of the coefficients is as follows:\n",
    "\n",
    "* $\\hat{\\beta}_0$ is the expected value of the dependent variable when all the independent variables are 0.\n",
    "* $\\hat{\\beta}_1$ is the change in the expected value of $y$ for a one-unit change in $x_1$, holding all other variables constant.\n",
    "* $\\hat{\\beta}_2$ is the change in the expected value of $y$ for a one-unit change in $x_2$, holding all other variables constant.\n",
    "* $\\vdots$\n",
    "* $\\hat{\\beta}_p$ is the change in the expected value of $y$ for a one-unit change in $x_p$, holding all other variables constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd73e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c8f3c1",
   "metadata": {},
   "source": [
    "## Check of Assumptions\n",
    "Let's check the previous assumptions that we made earlier and ensure that they still holds\n",
    "\n",
    "- (**Independence**) $\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_n$ are independent\n",
    "- (**Identical / homoscedasticity**) The errors have constant variance, i.e., $\\text{Var}(\\epsilon_i) = \\sigma^2$ for all $i$\n",
    "- (**Normally distributed**) $\\epsilon_i \\sim N(0, \\sigma^2)$\n",
    "- (**Linearity**) The relationship between the independent variables and the response is linear\n",
    "\n",
    "> **_ANY INFERENCE FROM THE REGRESSION MODEL IS VALID  \n",
    "> ONLY IF THE ASSUMPTIONS ARE MET_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f94922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "\n",
    "# normal iid satisfying assumptions\n",
    "epsilon = stats.norm(0, 0.5).rvs(n)\n",
    "\n",
    "# iid expoenential errors\n",
    "exp_epsilon = stats.expon(10).rvs(n)\n",
    "\n",
    "# non-independent errors\n",
    "ar_epsilon = np.zeros(n)\n",
    "for i in range(2, n):\n",
    "    ar_epsilon[i] = 0.8 * ar_epsilon[i-1] + 0.2 * ar_epsilon[i-2] + 0.5 * epsilon[i]\n",
    "\n",
    "# heteroscedastic errors\n",
    "het_epsilon = epsilon * np.array([0.1*(n-i) for i in range(n)])\n",
    "\n",
    "data = [make_data(x) for x in [epsilon, exp_epsilon, ar_epsilon, het_epsilon]]\n",
    "models = [smf.ols('y ~ x1 + x2', data=df).fit() for df in data]\n",
    "residuals = [model.resid for model in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1111f4",
   "metadata": {},
   "source": [
    "### Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 14))\n",
    "\n",
    "for i, (resid, ax) in enumerate(zip(residuals, axs.ravel())):\n",
    "    sm.qqplot(standardize(resid), line='45', ax=ax)\n",
    "    ax.set_title(f'Data {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276fc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "for i, resid in enumerate(residuals):\n",
    "    shapiro = stats.shapiro(standardize(resid))\n",
    "    print(f'Data {i+1}:')\n",
    "    decision(shapiro.pvalue, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f9627",
   "metadata": {},
   "source": [
    "### Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 14))\n",
    "\n",
    "lags = 20\n",
    "for i, (resid, ax) in enumerate(zip(residuals, axs.ravel())):\n",
    "    ax.bar(range(lags+1), acf(resid, nlags=lags))\n",
    "    ax.set_title(f'Data {i+1}'); ax.set_xlabel('lag'); ax.set_ylabel('ACF'); \n",
    "    ax.set_ylim(-0.5, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a58b1d",
   "metadata": {},
   "source": [
    "### Heteroscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44db5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 14))\n",
    "\n",
    "for i, (model, resid, ax) in enumerate(zip(models, residuals, axs.ravel())):\n",
    "    ax.scatter(model.fittedvalues, standardize(resid))\n",
    "    ax.set_xlabel('Fitted values'); ax.set_ylabel('Standardized residuals'); ax.set_title(f'Data {i+1}')\n",
    "    ax.set_ylim(-5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 14))\n",
    "\n",
    "variable =  2\n",
    "for i, (df, resid, ax) in enumerate(zip(data, residuals, axs.ravel())):\n",
    "    ax.scatter(df.iloc[:, variable], standardize(resid))\n",
    "    ax.set_xlabel(f'X{variable}'); ax.set_ylabel('Standardized residuals'); ax.set_title(f'Data {i+1}')\n",
    "    ax.set_ylim(-5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2054701",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
