{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d499b8cf",
   "metadata": {},
   "source": [
    "# Math 189 Week 5 Summary\n",
    "> NAME: $\\color{blue}{\\text{Kaiwen Bian}}$\n",
    "> \n",
    "> PID: $\\color{blue}{\\text{A17316568}}$\n",
    ">\n",
    "> \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce0cf8",
   "metadata": {},
   "source": [
    "I certify that the following write-up is my own work, and have abided by the UCSD Academic Integrity Guidelines.\n",
    "\n",
    "- [x] Yes\n",
    "- [ ] No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c41b1",
   "metadata": {},
   "source": [
    "% # %load tex-macros\n",
    "<div hidden>\n",
    "\\newcommand{\\require}[1]{}\n",
    "\n",
    "$\\require{begingroup}\\require{newcommand}$\n",
    "$\\long\\def \\forcecommand #1{\\providecommand{#1}{}\\renewcommand{#1}}$\n",
    "$\\forcecommand{\\defeq}{\\stackrel{\\small\\bullet}{=}}$\n",
    "$\\forcecommand{\\ra}{\\rangle}$\n",
    "$\\forcecommand{\\la}{\\langle}$\n",
    "$\\forcecommand{\\pr}{{\\mathbb P}}$\n",
    "$\\forcecommand{\\qr}{{\\mathbb Q}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\av}{{\\boldsymbol{a}}}$\n",
    "$\\forcecommand{\\bv}{{\\boldsymbol{b}}}$\n",
    "$\\forcecommand{\\cv}{{\\boldsymbol{c}}}$\n",
    "$\\forcecommand{\\dv}{{\\boldsymbol{d}}}$\n",
    "$\\forcecommand{\\ev}{{\\boldsymbol{e}}}$\n",
    "$\\forcecommand{\\fv}{{\\boldsymbol{f}}}$\n",
    "$\\forcecommand{\\gv}{{\\boldsymbol{g}}}$\n",
    "$\\forcecommand{\\hv}{{\\boldsymbol{h}}}$\n",
    "$\\forcecommand{\\nv}{{\\boldsymbol{n}}}$\n",
    "$\\forcecommand{\\sv}{{\\boldsymbol{s}}}$\n",
    "$\\forcecommand{\\tv}{{\\boldsymbol{t}}}$\n",
    "$\\forcecommand{\\uv}{{\\boldsymbol{u}}}$\n",
    "$\\forcecommand{\\vv}{{\\boldsymbol{v}}}$\n",
    "$\\forcecommand{\\wv}{{\\boldsymbol{w}}}$\n",
    "$\\forcecommand{\\zerov}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\onev}{{\\mathbf{0}}}$\n",
    "$\\forcecommand{\\phiv}{{\\boldsymbol{\\phi}}}$\n",
    "$\\forcecommand{\\cc}{{\\check{C}}}$\n",
    "$\\forcecommand{\\xv}{{\\boldsymbol{x}}}$\n",
    "$\\forcecommand{\\Xv}{{\\boldsymbol{X}\\!}}$\n",
    "$\\forcecommand{\\yv}{{\\boldsymbol{y}}}$\n",
    "$\\forcecommand{\\Yv}{{\\boldsymbol{Y}}}$\n",
    "$\\forcecommand{\\zv}{{\\boldsymbol{z}}}$\n",
    "$\\forcecommand{\\Zv}{{\\boldsymbol{Z}}}$\n",
    "$\\forcecommand{\\Iv}{{\\boldsymbol{I}}}$\n",
    "$\\forcecommand{\\Jv}{{\\boldsymbol{J}}}$\n",
    "$\\forcecommand{\\Cv}{{\\boldsymbol{C}}}$\n",
    "$\\forcecommand{\\Ev}{{\\boldsymbol{E}}}$\n",
    "$\\forcecommand{\\Fv}{{\\boldsymbol{F}}}$\n",
    "$\\forcecommand{\\Gv}{{\\boldsymbol{G}}}$\n",
    "$\\forcecommand{\\Hv}{{\\boldsymbol{H}}}$\n",
    "$\\forcecommand{\\alphav}{{\\boldsymbol{\\alpha}}}$\n",
    "$\\forcecommand{\\epsilonv}{{\\boldsymbol{\\epsilon}}}$\n",
    "$\\forcecommand{\\betav}{{\\boldsymbol{\\beta}}}$\n",
    "$\\forcecommand{\\deltav}{{\\boldsymbol{\\delta}}}$\n",
    "$\\forcecommand{\\gammav}{{\\boldsymbol{\\gamma}}}$\n",
    "$\\forcecommand{\\etav}{{\\boldsymbol{\\eta}}}$\n",
    "$\\forcecommand{\\piv}{{\\boldsymbol{\\pi}}}$\n",
    "$\\forcecommand{\\thetav}{{\\boldsymbol{\\theta}}}$\n",
    "$\\forcecommand{\\tauv}{{\\boldsymbol{\\tau}}}$\n",
    "$\\forcecommand{\\muv}{{\\boldsymbol{\\mu}}}$\n",
    "$%$\n",
    "$\\forcecommand{\\sd}{\\text{SD}}$\n",
    "$\\forcecommand{\\se}{\\text{SE}}$\n",
    "$\\forcecommand{\\med}{\\text{median}}$\n",
    "$\\forcecommand{\\median}{\\text{median}}$\n",
    "$%$\n",
    "$\\forcecommand{\\supp}{\\text{supp}}$\n",
    "$\\forcecommand{\\E}{\\mathbb{E}}$\n",
    "$\\forcecommand{\\var}{\\text{Var}}$\n",
    "$\\forcecommand{\\Ber}{{\\text{Ber}}}$\n",
    "$\\forcecommand{\\Bin}{{\\text{Bin}}}$\n",
    "$\\forcecommand{\\Geo}{{\\text{Geo}}}$\n",
    "$\\forcecommand{\\Unif}{{\\text{Unif}}}$\n",
    "$\\forcecommand{\\Poi}{{\\text{Poi}}}$\n",
    "$\\forcecommand{\\Exp}{{\\text{Exp}}}$\n",
    "$\\forcecommand{\\Chisq}{{\\chi^2}}$\n",
    "$\\forcecommand{\\N}{\\mathbb{N}}$\n",
    "$\\forcecommand{\\iid}{{\\stackrel{iid}{\\sim}}}$\n",
    "$\\forcecommand{\\px}{p_{X}}$\n",
    "$\\forcecommand{\\fx}{f_{X}}$\n",
    "$\\forcecommand{\\Fx}{F_{X}}$\n",
    "$\\forcecommand{\\py}{p_{Y}}$\n",
    "$\\forcecommand{\\pxy}{p_{X,Y}}$\n",
    "$\\forcecommand{\\po}{{p_0}}$\n",
    "$\\forcecommand{\\pa}{{p_a}}$\n",
    "$\\forcecommand{\\Xbar}{\\overline{X}}$\n",
    "$\\forcecommand{\\Ybar}{\\overline{Y}}$\n",
    "$\\forcecommand{\\Zbar}{\\overline{Z}}$\n",
    "$\\forcecommand{\\nXbar}{n \\cdot \\overline{X}}$\n",
    "$\\forcecommand{\\nYbar}{n \\cdot \\overline{Y}}$\n",
    "$\\forcecommand{\\nZbar}{n \\cdot \\overline{Z}}$\n",
    "$\\forcecommand{\\Xn}{X_1, X_2, \\dots, X_n}$\n",
    "$\\forcecommand{\\Xm}{{X_1, X_2, \\dots, X_m}}$\n",
    "$\\forcecommand{\\Yn}{Y_1, Y_2, \\dots, Y_n}$\n",
    "$\\forcecommand{\\Ym}{{Y_1, Y_2, \\dots, Y_m}}$\n",
    "$\\forcecommand{\\sumXn}{X_1 + X_2 + \\dots + X_n}$\n",
    "$\\forcecommand{\\sumym}{Y_1 + Y_2 + \\dots + Y_m}$\n",
    "$\\forcecommand{\\la}{\\ell_\\alpha}$\n",
    "$\\forcecommand{\\ua}{u_\\alpha}$\n",
    "$\\forcecommand{\\at}{{\\alpha/2}}$\n",
    "$\\forcecommand{\\mux}{\\mu_{X}}$\n",
    "$\\forcecommand{\\muy}{\\mu_{Y}}$\n",
    "$\\forcecommand{\\sx}{\\sigma_{X}}$\n",
    "$\\forcecommand{\\sy}{\\sigma_{Y}}$\n",
    "$\\forcecommand{\\ci}{\\text{CI}}$\n",
    "$\\forcecommand{\\pvalue}{$p$-value}$\n",
    "$\\forcecommand{\\Ho}{H_{0}}$\n",
    "$\\forcecommand{\\Ha}{H_{a}}$\n",
    "\n",
    "\\vskip-\\parskip\n",
    "\\vskip-\\baselineskip\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb0120db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Optional \n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "matplotlib.rcParams['figure.figsize'] = 7, 7\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b475c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_X(X, ax, type='pmf', **kwargs):\n",
    "    ax.set_xlabel('Support')\n",
    "    ax.set_title(f'{X.dist.name}{X.args}')\n",
    "    \n",
    "    min_X, max_X = X.ppf((1e-3, 1-1e-3))\n",
    "    supp_X = np.linspace(min_X-1, max_X + 1, 200)\n",
    "    \n",
    "    if type == 'pmf':\n",
    "        supp_X = np.arange(min_X-1, max_X + 1)\n",
    "        ax.bar(supp_X, X.pmf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PMF')\n",
    "    elif type == 'pdf':\n",
    "        ax.plot(supp_X, X.pdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('PDF')\n",
    "    elif type == 'cdf':\n",
    "        ax.plot(supp_X, X.cdf(supp_X), **kwargs)\n",
    "        ax.set_ylabel('CDF')\n",
    "    else:\n",
    "        raise ValueError('type must be pmf or cdf')\n",
    "\n",
    "def decision(pvalue, alpha):\n",
    "    if pvalue < alpha:\n",
    "        print(f'reject H0: pvalue={pvalue} < {alpha}')  \n",
    "    else: \n",
    "        print(f'fail to reject H0: pvalue={pvalue} ≥ {alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfec7ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6dc53",
   "metadata": {},
   "source": [
    "# Key Takeaways from Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc992f31",
   "metadata": {},
   "source": [
    "#### Tuesday:\n",
    "\n",
    "In the first lecture, we went over the basic ideas of using less assumptions during hypothesis testing (using minimal assumptions), which lead us to many non-parametric tests like  Wald-Wolfowitz Runs Test and KS tests. In addition, we went over the basic formulation of them and discussed about the pros & cons of them in practical usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69de7b",
   "metadata": {},
   "source": [
    "#### Thursday:\n",
    "\n",
    "One of the most important thing before conducting any parametric test is to check the assumption of the test, whether what we think is true about teh data is actually true. On Thursday's lecture we discussed ways that we can check for different assumptions on the data (i.e. independnence, distributional assumtion) through techniques such as QQ-plot or autocorrelation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaadbca8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af54d945",
   "metadata": {},
   "source": [
    "# Hypothesis Testing with Minimal Assumptions\n",
    "\n",
    "> **Nonparametric tests** are hypothesis tests that make minimal assumptions about the distribution of the data.\n",
    "> \n",
    "> They are used when it's not reasonable to assume that the data follows a known distribution, or when the data is ordinal or nominal.\n",
    "\n",
    "The main difference between parametric and nonparametric tests is that **non-parameteric test makes no assumption of the distribution that our data comes from** & we 0nly want to tell if tewo distributions are apart (they are strctly more general)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8201782",
   "metadata": {},
   "source": [
    "### Parametric Tests\n",
    "The tests which we have looked at so far, such as the t-test, z-test, and $\\chi^2$-test, are called **parametric tests** because they make assumptions about the distribution of the data, such as the mean, variance, or proportion, e.g.,  \n",
    "\n",
    "- $X_1, X_2, \\dots, X_n \\sim \\mathcal{N}(\\mu, \\sigma^2)$,  \n",
    "- $X_1, X_2, \\dots, X_n \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$ and $Y_1, Y_2, \\dots, Y_m \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y^2)$,  \n",
    "- $X_1, X_2, \\dots, X_n \\sim \\text{Ber}(p)$,  \n",
    "- $X_1, X_2, \\dots, X_n \\sim \\text{Exp}(\\lambda)$,  \n",
    "\n",
    "The parameters here are the population parameters $\\mu, \\sigma^2, \\mu_X, \\sigma_X, \\mu_Y, \\sigma_Y, p, \\lambda, \\pi$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3bc4d3",
   "metadata": {},
   "source": [
    "### Nonparametric Tests\n",
    "Nonparametric tests, on the other hand, are based on assumptions such as:\n",
    "\n",
    "- The data is exchangeable, i.e., the order in which the data is observed does not matter,\n",
    "  - Does not holds for time series data\n",
    "- $X_1, X_2, \\dots, X_n \\sim F$\n",
    "  - $F$ is some probability distribution, but we don't know what $F$ is,\n",
    "- $X_1, X_2, \\dots, X_n \\sim F_X$ and $Y_1, Y_2, \\dots, Y_m \\sim F_Y$\n",
    "  - where $F_X$ and $F_Y$ are some probability distributions, but we don't know what $F_X$ and $F_Y$ are\n",
    "  \n",
    "⚠️ **In fact, the family of $\\chi^2$ tests we looked at in the last lecture are actually nonparametric tests.** ⚠️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec5e23",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ae5b4",
   "metadata": {},
   "source": [
    "An example where a nonparametric test would want to be used is in the following scenarios: \n",
    "* When the sample size is too small such that we can not let CLT take place and then we wouldn't know what would be the actual distribution to parametrize to.\n",
    "* When the data is large but we have seen that it is not normal or approaching to any of the known distribution, very complicated distribution CDF functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf189a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5d9d1",
   "metadata": {},
   "source": [
    "# Wald-Wolfowitz Runs Test\n",
    "\n",
    "Simplest test for seeing if two sampels come from the same distribution (the genralization of it `All of Non-parametric Statistics`) The **Wald-Wolfowitz runs test** is a nonparametric test for two independent samples. The test is used to determine if the two samples are from the same distribution, i.e.,  \n",
    "\n",
    "$$\n",
    "X_1, X_2, \\dots, X_n \\sim F_X \\quad \\text{and} \\quad Y_1, Y_2, \\dots, Y_m \\sim F_Y\n",
    "$$\n",
    "\n",
    "and,  \n",
    "\n",
    "$$\n",
    "H_0: F_X = F_Y \\quad \\text{vs} \\quad H_a: F_X \\neq F_Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16866c4b",
   "metadata": {},
   "source": [
    "### Telling Things From Runs\n",
    "A **run** is a theoritical construct coming from CS. It is a sequence of consecutive observations of the same type, i.e., all $X$'s or all $Y$'s. More concretely,\n",
    "\n",
    "- Arrange $X_1, X_2, \\dots, X_n, Y_1, Y_2, \\dots, Y_m$ in increasing order,\n",
    "- Suppose it looks like  \n",
    "  $$\n",
    "  Y_{10}, X_3, X_7, Y_5, X_1, X_2, Y_1, Y_2, X_4, X_5, X_6, Y_3, Y_4, X_8, X_9, Y_6, Y_7, Y_8, Y_9, Y_{11}\n",
    "  $$\n",
    "\n",
    "For simplicity, let \"+\" denote a sample from $X$ and \"-\" denote a sample from $Y$. Then the sequence above looks like the following (this is our observations, you can think of them as 0 and 1):\n",
    "\n",
    "$$\n",
    "-++-++--+++--++-----\n",
    "$$\n",
    "\n",
    "Then we create partitions: a **run** is a sequence of consecutive `+`'s or `-`'s. In the sequence above, there are 9 runs, i.e.,  \n",
    "\n",
    "$$\n",
    "| ++ | - | ++ | -- | +++ | -- | ++ | - -\n",
    "$$\n",
    "\n",
    "We are not intersting in what is in run but the length of the run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2ea52",
   "metadata": {},
   "source": [
    "### Different Mean\n",
    "If the two distributions $F_X$ and $F_Y$ are **different**, then the number of runs will be small:\n",
    "\n",
    "- If the **two means or medians are very different**, then your ordered sequence will look something like:\n",
    "\n",
    "$$\n",
    "- - - - -| + | - | + + + + + +\n",
    "$$\n",
    "\n",
    "- This is because you have a region where there is a bunch of \"-\" on one side and \"+\" on the other side.\n",
    "\n",
    "- Minimum runs are 2 as they are completley seperated.\n",
    "\n",
    "### Different Variance\n",
    "If the two **means are similar** but the **variances are very different**, then your ordered sequence will look something like:\n",
    "\n",
    "- If $F_y$ has lower SD and $F_x$ has a greater SD, assumng they have similar means, than you would see a good mix in the middle with one type centered and teh other type spreading out.\n",
    "\n",
    "$$\n",
    "+ + + + | - - - - | + + + +\n",
    "$$\n",
    "\n",
    "- The number of runs is still small.\n",
    "\n",
    "\n",
    "### Indistinguishable\n",
    "If the two distributions $F_X$ and $F_Y$ are **indistinguishable**, then the number of runs will be large:\n",
    "\n",
    "- If the two means or medians are very different, then your ordered sequence will look something like:\n",
    "\n",
    "$$\n",
    "| + | - | ++ | - | + | -- | ++ | -\n",
    "$$\n",
    "\n",
    "- Large number of runs if they are thes ame distribution, all fixes together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a634c",
   "metadata": {},
   "source": [
    "### Wald-Wolfowitz Runs Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd4159",
   "metadata": {},
   "source": [
    "- **Assumption**:\n",
    "\n",
    "    $$\n",
    "    X_1, X_2, \\dots, X_n \\sim F_X \\quad \\text{and} \\quad Y_1, Y_2, \\dots, Y_m \\sim F_Y\n",
    "    $$\n",
    "\n",
    "- **Hypotheses**:\n",
    "    - If $H_0$ is true, number of runs should be high, else, the number of runs should be low\n",
    "    - Then $\\mu_R$ shoud be high and $\\hat{R} - \\mu_R$ will be very negative, test statistics falls into negative zone.\n",
    "\n",
    "    $$\n",
    "    H_0: F_X = F_Y \\quad \\text{vs} \\quad H_a: F_X \\neq F_Y\n",
    "    $$\n",
    "\n",
    "- **Test Statistic**:\n",
    "    - If $\\hat{R}$ is the observed number of runs, then the test statistic is the following:\n",
    "    - $\\mu_R$ is the number of runs under the assumption that Null Hypothesis is true. These are the expected value and variance of the number of runs under the null hypothesis.\n",
    "\n",
    "    $$\n",
    "    Z = \\frac{\\hat{R} - \\mu_R}{s_R}\n",
    "    $$\n",
    "    $$\n",
    "    \\mu_R = 1 + 2 \\frac{n_X \\cdot n_Y}{n_X + n_Y}\n",
    "    $$  \n",
    "\n",
    "    $$\n",
    "    s_R = \\sqrt{\\frac{2 n_X \\cdot n_Y \\cdot (2 n_X n_Y - n_X - n_Y)}{(n_X + n_Y)^2 \\cdot (n_X + n_Y - 1)}}\n",
    "    $$  \n",
    "\n",
    "- **Rejection Region**:\n",
    "    - For the most common use case, we reject the null hypothesis if the number of runs is too small, i.e.\n",
    "    - $x_{\\alpha} = z_{\\alpha}$ is the $\\alpha$-th **upper quantile** of the standard normal distribution $\\mathcal{N}(0,1)$.\n",
    "    - $x_{\\alpha}$ comes from the central limit theorem, which holds neglecting what the distribution is.\n",
    "\n",
    "\n",
    "    $$\n",
    "    \\text{rejection region} = (-\\infty, -x_{\\alpha})\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484966cf",
   "metadata": {},
   "source": [
    "The run test does not exist in the `scipy` module, not in production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fx = stats.uniform(5, 13)\n",
    "Fy = stats.chi2(12)\n",
    "\n",
    "ax = plt.gca()\n",
    "plot_X(Fx, type=\"pdf\", ax=ax)\n",
    "plot_X(Fy, type=\"pdf\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R, pvalue = statsmodels.sandbox.stats.runs.runstest_2samp(\n",
    "    Fx.rvs(100), \n",
    "    Fy.rvs(100)\n",
    ")\n",
    "\n",
    "alpha = 0.05\n",
    "decision(pvalue, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e7f47",
   "metadata": {},
   "source": [
    "### Anatomy\n",
    "\n",
    "| Anatomy of the hypothesis test |  Answer  |\n",
    "|:------------------------------:|:--------:|\n",
    "| **Assumption**                     | The data consists of two independent samples from two distributions, denoted as $F_X$ and $F_Y$. |\n",
    "| **Null hypothesis**                | $H_0: F_X = F_Y$ (The two samples come from the same distribution). |\n",
    "| **Alternate hypothesis**           | $H_a: F_X \\neq F_Y$ (The two samples come from different distributions). |\n",
    "| **Test statistic**                 | The number of runs $\\hat{R}$ in the combined sequence of $X$'s and $Y$'s. The standardized test statistic is:  $$ Z = \\frac{\\hat{R} - \\mu_R}{s_R} $$ where $\\mu_R$ and $s_R$ are the mean and standard deviation of runs under the null hypothesis. |\n",
    "| **Rejection region shape**         | Left-tailed: Reject $H_0$ if the number of runs is significantly small (i.e., $Z < -z_{\\alpha}$ for a given significance level $\\alpha$). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb99aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e79cb2",
   "metadata": {},
   "source": [
    "# Kolmogorov-Smirnov Test\n",
    "The Kolmogorov-Smirnov test is a nonparametric test for the equality of two probability distributions. It comes in two variants:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde02ca",
   "metadata": {},
   "source": [
    "### One Sample K-S Test\n",
    "\n",
    "We make the assumtion that there is some distribution, or even just a function (satisfying assumption of CDF), and we want to test our hypothesis about **whether our data is generated from such distribution or from  different distribution**.\n",
    "\n",
    "$$\n",
    "X_1, X_2, \\dots, X_n \\sim F_X\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_0: F_X = F_0 \\quad \\text{vs} \\quad H_a: F_X \\neq F_0\n",
    "$$\n",
    "\n",
    "where $F_0$ is some known hypothesized, reference distribution, e.g., $\\mathcal{N}(\\mu, \\sigma^2)$, $\\Gamma(\\alpha, \\lambda)$, $\\text{Beta}(\\alpha, \\beta)$, etc.\n",
    "\n",
    "### Two Sample K-S Test\n",
    "\n",
    "$$\n",
    "X_1, X_2, \\dots, X_n \\sim F_X \\quad \\text{and} \\quad Y_1, Y_2, \\dots, Y_m \\sim F_Y\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_0: F_X = F_Y \\quad \\text{vs} \\quad H_a: F_X \\neq F_Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd845f",
   "metadata": {},
   "source": [
    "### More Into One-sample K-S Test\n",
    "\n",
    "To develop some intuition, let's take a look at the one-sample K-S test. Suppose we have a collection $X_1, X_2, \\dots, X_n \\sim F_X$, where $F_X$ is some unknown distribution, and we want to test the hypothesis:\n",
    "\n",
    "$$\n",
    "H_0: F_X = F_0 \\quad \\text{vs} \\quad H_a: F_X \\neq F_0\n",
    "$$\n",
    "\n",
    "We can use the **empirical distribution function (EDF)** to estimate $F_X$. The EDF is defined as  \n",
    "\n",
    "$$\n",
    "\\hat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}(X_i \\leq x)\n",
    "$$\n",
    "\n",
    "where $\\mathbb{1}(X_i \\leq x)$ is the indicator function, i.e.,  \n",
    "\n",
    "$$\n",
    "\\mathbb{1}(X_i \\leq x) =\n",
    "\\begin{cases} \n",
    "1 & \\text{if } X_i \\leq x \\\\\n",
    "0 & \\text{if } X_i > x\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffd0364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true = stats.norm(0, 2)\n",
    "X_loc = stats.norm(2, 2)\n",
    "X_scale = stats.norm(0, 4)\n",
    "data = X_true.rvs(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = np.linspace(-6, 6, 100)\n",
    "\n",
    "F0 = X_true.cdf(xrange)\n",
    "\n",
    "FX = np.array([sum(data < x) for x in xrange]) / len(data)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.plot(xrange, F0, label='Null CDF')\n",
    "ax.plot(xrange, FX, label='EDF')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = X_loc.rvs(20)\n",
    "data_scale = X_scale.rvs(20)\n",
    "\n",
    "FX_loc = np.array([sum(data_loc < x) for x in xrange]) / len(data_loc)\n",
    "FX_scale = np.array([sum(data_scale < x) for x in xrange]) / len(data_scale)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].plot(xrange, F0, label='Null CDF'); axs[0].plot(xrange, FX, label='EDF');        axs[0].legend()\n",
    "axs[1].plot(xrange, F0, label='Null CDF'); axs[1].plot(xrange, FX_loc, label='EDF');    axs[1].legend()\n",
    "axs[2].plot(xrange, F0, label='Null CDF'); axs[2].plot(xrange, FX_scale, label='EDF');  axs[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29816340",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(n = (10, 1000, 2))\n",
    "\n",
    "def plot_edf(n=20):\n",
    "\n",
    "    data = X_true.rvs(n)\n",
    "    data_loc = X_loc.rvs(n)\n",
    "    data_scale = X_scale.rvs(n)\n",
    "\n",
    "    F0 = X_true.cdf(xrange)\n",
    "    FX = np.array([sum(data < x) for x in xrange]) / len(data)\n",
    "    FX_loc = np.array([sum(data_loc < x) for x in xrange]) / len(data_loc)\n",
    "    FX_scale = np.array([sum(data_scale < x) for x in xrange]) / len(data_scale)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axs[0].plot(xrange, F0, label='Null CDF'); axs[0].plot(xrange, FX, label='EDF');        axs[0].legend()\n",
    "    axs[1].plot(xrange, F0, label='Null CDF'); axs[1].plot(xrange, FX_loc, label='EDF');    axs[1].legend()\n",
    "    axs[2].plot(xrange, F0, label='Null CDF'); axs[2].plot(xrange, FX_scale, label='EDF');  axs[2].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd9de9",
   "metadata": {},
   "source": [
    "### Test Statistic\n",
    "\n",
    "The test statistic is the **Kolmogorov-Smirnov statistic**, defined as  \n",
    "\n",
    "$$\n",
    "D_n = \\sup_x \\left| \\hat{F}_n(x) - F_0(x) \\right|\n",
    "$$\n",
    "\n",
    "where $\\sup_x$ is the supremum over all $x$ in the support of $F_0$. Intuitively, this is the **maximum vertical distance (deviation)** between the orange and the blue lines in the plots above.\n",
    "\n",
    "- If $D_n$ is small, then we have evidence that $F_X$ is close to $F_0$.\n",
    "- If $D_n$ is large, then we have evidence that $F_X$ is different from $F_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a623d7",
   "metadata": {},
   "source": [
    "Intuitively, the KS test statistic measures the **maximum vertical distance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f5795",
   "metadata": {},
   "source": [
    "### Extension to Two-sample K-S Test\n",
    "\n",
    "The two-sample K-S test is a generalization of the one-sample K-S test. Given  \n",
    "\n",
    "$$\n",
    "X_1, X_2, \\dots, X_n \\sim F_X \\quad \\text{and} \\quad Y_1, Y_2, \\dots, Y_m \\sim F_Y\n",
    "$$\n",
    "\n",
    "and the hypotheses  \n",
    "\n",
    "$$\n",
    "H_0: F_X = F_Y \\quad \\text{vs} \\quad H_a: F_X \\neq F_Y\n",
    "$$\n",
    "\n",
    "the test statistic is given by  \n",
    "\n",
    "$$\n",
    "D_{n,m} = \\sup_x \\left| \\hat{F}_n(x) - \\hat{F}_m(x) \\right|\n",
    "$$\n",
    "\n",
    "where $\\hat{F}_n(x)$ and $\\hat{F}_m(x)$ are the empirical distribution functions of $X_1, X_2, \\dots, X_n$ and $Y_1, Y_2, \\dots, Y_m$, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5d845",
   "metadata": {},
   "source": [
    "### Rejection Region\n",
    "\n",
    "We reject the null hypothesis if the deviation of EDFs, $D_{m,n}$, is large, i.e.,  \n",
    "\n",
    "$$\n",
    "\\text{rejection region} = (x_{\\alpha}, \\infty)\n",
    "$$\n",
    "\n",
    "where $x_{\\alpha}$ is given by  \n",
    "\n",
    "$$\n",
    "x_{\\alpha} = \\sqrt{\\frac{1}{2} \\log \\left(\\frac{\\alpha}{2} \\right) \\times \\frac{m + n}{m \\cdot n}}\n",
    "$$\n",
    "\n",
    "This has to do with CLT and stochastics processings with Brownian motion.\n",
    "- Caluclating p-value is complicated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9731c",
   "metadata": {},
   "source": [
    "### Anatomy\n",
    "\n",
    "| Anatomy of the hypothesis test |  Answer  |\n",
    "|:------------------------------:|:--------:|\n",
    "| **Assumption**                     | The data consists of one or two independent samples drawn from continuous distributions. |\n",
    "| **Null hypothesis**                | **One-sample K-S test:** $H_0: F_X = F_0$ (The sample follows the hypothesized distribution $F_0$).  <br> **Two-sample K-S test:** $H_0: F_X = F_Y$ (The two samples come from the same distribution). |\n",
    "| **Alternate hypothesis**           | **One-sample K-S test:** $H_a: F_X \\neq F_0$ (The sample does not follow the hypothesized distribution). <br> **Two-sample K-S test:** $H_a: F_X \\neq F_Y$ (The two samples come from different distributions). |\n",
    "| **Test statistic**                 | The Kolmogorov-Smirnov statistic:  $$ D_n = \\sup_x \\left| \\hat{F}_n(x) - F_0(x) \\right| $$ (One-sample K-S test) <br> $$ D_{n,m} = \\sup_x \\left| \\hat{F}_n(x) - \\hat{F}_m(x) \\right| $$ (Two-sample K-S test) |\n",
    "| **Rejection region shape**         | Right-tailed: Reject $H_0$ if $D_n$ (or $D_{n,m}$) is greater than the critical value $$ x_{\\alpha} = \\sqrt{\\frac{1}{2} \\log \\left(\\frac{\\alpha}{2} \\right) \\times \\frac{m + n}{m \\cdot n}} $$ based on the sample size and significance level $\\alpha$. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fx = stats.norm(100, 200)\n",
    "# Fy = stats.chi2(100)\n",
    "Fy = stats.norm(140, 200)\n",
    "\n",
    "ax = plt.gca()\n",
    "plot_X(Fx, type=\"pdf\", ax=ax)\n",
    "plot_X(Fy, type=\"pdf\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ba07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fy = stats.chi2(100)\n",
    "Fy = stats.norm(105, 200)\n",
    "\n",
    "X_data = Fx.rvs(100)\n",
    "Y_data = Fy.rvs(100)\n",
    "\n",
    "KS_result = stats.kstest(X_data, Y_data)\n",
    "\n",
    "alpha = 0.02\n",
    "decision(KS_result.pvalue, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc4665",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fx = stats.norm(100, 200)\n",
    "Fy = stats.norm(105, 200)\n",
    "\n",
    "X_data = Fx.rvs(100)\n",
    "Y_data = Fy.rvs(100)\n",
    "\n",
    "KS_result = stats.kstest(X_data, Y_data)\n",
    "\n",
    "alpha = 0.02\n",
    "decision(KS_result.pvalue, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue_ttest = stats.ttest_ind(X_data, Y_data, equal_var=True, alternative='two-sided').pvalue\n",
    "decision(pvalue_ttest, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913f0bc",
   "metadata": {},
   "source": [
    "### Example\n",
    "Data on course evaluations, course characteristics, and professor characteristics for 463 courses for the academic years 2000–2002 at the University of Texas at Austin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://vincentarelbundock.github.io/Rdatasets/csv/AER/TeachingRatings.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddef216",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_samples = df.groupby('minority')['beauty'].apply(lambda x: x.values)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.histplot(two_samples[0], kde=True, ax=axs[0], bins=7, color='dodgerblue')\n",
    "sns.histplot(two_samples[1], kde=True, ax=axs[1], bins=7, color='firebrick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.15\n",
    "t_test_result = stats.ttest_ind(*two_samples, equal_var=False)\n",
    "decision(t_test_result.pvalue, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc5ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_test_result = stats.kstest(*two_samples)\n",
    "decision(ks_test_result.pvalue, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecba40a",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde99e5",
   "metadata": {},
   "source": [
    "# Tests for Assumptions\n",
    "\n",
    "To use an parametric test, we have much assumption. Hence, an key point is to check if the assumption that we assume actually holds.\n",
    "\n",
    "So statistical inerence to make **correct inferences** and provding **sufficient evidence for the assumption that leads to the inference we make is indded valid** should be like:\n",
    "\n",
    "1. Make some assumptions.\n",
    "2. Justify why assumptions are valid.\n",
    "3. Conduct statistical inference procedures for testing your main question of inference.\n",
    "\n",
    "This is before doing regression analysis and establishing **causal relationship**. We need to ensure that the assumption that leads to the regression inferecnes is in fact valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d2239",
   "metadata": {},
   "source": [
    "### Example:\n",
    "When your data is actually normal, your t-test will have more power than KS-test:\n",
    "\n",
    "- However, if your data is no longer normal, your assumption for t-test fails and then any conclusion drawn from using an t-test is not legit anymore.\n",
    "\n",
    "- KS test is really good when you have fairly large sample sizes as you need really large sample statistics to aproach to an good empirical distribution. You can us  the KS-test in any conditions as long as you have enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d1729e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529da33",
   "metadata": {},
   "source": [
    "## More Explorative Data Analysis: QQ-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392df8f5",
   "metadata": {},
   "source": [
    "Suppose we are given a sample $X_1, X_2, \\dots, X_n$, can we visualize if it comes from a reference distribution $D(\\theta)$?\n",
    "\n",
    "- Previously we do historgram and overlay CDF\n",
    "- Or we just learned to use the K-S test\n",
    "- Now we use a QQ-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08267167",
   "metadata": {},
   "source": [
    "### Quantile-Quantile Plot\n",
    "\n",
    "A **quantile-quantile (QQ) plot** is a graphical tool to compare two probability distributions. It is used to check if a given data follows a known distribution, or if two data sets are from the same distribution.\n",
    "- The quantiles are inverse CDF. So QQ-plot is essentially saying that instead looking at CDF, let's look at the **inverse of CDF**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfefc4",
   "metadata": {},
   "source": [
    "We cans ee an randomly sampled distribution comparing with it's theoritical distribution. Let's satrt with a `uniform` distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fx = stats.uniform(-10, 13)\n",
    "samples = Fx.rvs(200)\n",
    "sm.qqplot(samples, Fx, line='45');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173bf278",
   "metadata": {},
   "source": [
    "Similarly wec an do the samew ith a `chi-square` distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fx = stats.chi2(5)\n",
    "samples = Fx.rvs(200)\n",
    "sm.qqplot(samples, Fx, line='45');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556f61ef",
   "metadata": {},
   "source": [
    "However, if we compare two different distribution, let's say an `exponential` distribution and and `normal` distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8144daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fx = stats.expon(2)\n",
    "samples = stats.norm().rvs(200)\n",
    "sm.qqplot(samples, Fx, line='45');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709cc3e",
   "metadata": {},
   "source": [
    "Essentially, a QQ plot is a scatter plot of the **quantiles of one distribution** against the quantiles of another distribution. **This is similar to the idea of plotting two CDFs against each other, but it's easier to visualize.**\n",
    "- Plot the theoritical quantiles for $D(\\theta)$ against the actual quantitle for the sample distribution.\n",
    "- If sample come from the theoritical distribution, it will be an staright line with slope 1, passing through origin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8516e1a5",
   "metadata": {},
   "source": [
    "The distance between the line and the sample line is known as **optimal transport distance**, or the w1-warsertein distance. It charcterize the **convergence of distribution** in optimal transport.\n",
    "- It give measure of distance between two probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d5e844",
   "metadata": {},
   "source": [
    "### Effects of Varibales on QQ-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f50c0",
   "metadata": {},
   "source": [
    "> The effect of mean\n",
    "* When mean $\\mu$ changes, the location on the axis of the graph changes, it chifts horizontally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2127ef7",
   "metadata": {},
   "source": [
    "> The effect of variance\n",
    "* If $\\text{Var}(Y) \\uparrow$, the slope of the QQ-plot $\\uparrow$\n",
    "* If $\\text{Var}(Y) \\downarrow$, the slope of the QQ-plot $\\downarrow$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7297bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fx = stats.norm(0, 1)\n",
    "\n",
    "@interact(mu=(-3, 3, 0.1), sigma=(0.1, 3, 0.1), n=(10, 1000, 10))\n",
    "def qqplot(mu, sigma, n):\n",
    "    Y = stats.norm(mu, sigma)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sm.qqplot(Y.rvs(n), Fx, ax=ax[1], line='45')\n",
    "    \n",
    "    plot_X(Fx, ax[0], type='pdf', label='Theoretical', color='red')\n",
    "    plot_X(Y, ax[0], type='pdf', label='Samples', color='dodgerblue')\n",
    "    ax[0].set_xlim(-5, 5);  ax[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c623ef",
   "metadata": {},
   "source": [
    "> The effect of skew\n",
    "* As $\\text{skew}(Y) \\uparrow$, the curvature of the QQ-plot becomes more pronounced below the line $y=x$.\n",
    "* As $\\text{skew}(Y) \\downarrow$, the curvature of the QQ-plot becomes more pronounced above the line $y=x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299437d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reference = stats.beta(1, 1)\n",
    "\n",
    "@interact(alpha=(0.1, 5, 0.1), beta=(0.1, 5, 0.1), n=(10, 1000, 10))\n",
    "def qqplot(alpha, beta, n):\n",
    "    Y = stats.beta(alpha, beta)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sm.qqplot(Y.rvs(n), X_reference, ax=ax[1], line='45')\n",
    "\n",
    "    plot_X(X_reference, ax[0], type='pdf', label='Theoretical', color='red')\n",
    "    plot_X(Y, ax[0], type='pdf', label='Samples', color='dodgerblue')\n",
    "    ax[0].set_xlim(-0.1, 1.1);   ax[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca684a47",
   "metadata": {},
   "source": [
    "> Effect of the tails\n",
    "\n",
    "* If the tails of $Y$ are heavier than the tails of $X$, then the QQ-plot will curve upwards.\n",
    "* On the other hand, if the tails of $Y$ are lighter than the tails of $X$, then the QQ-plot will curve downwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa294ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(df=(2, 20, 1), n=(10, 1000, 10), swap=(False, True))\n",
    "def qqplot(df, n, swap=False):\n",
    "    X_reference = stats.norm(0, 1)\n",
    "    Y = stats.t(df)\n",
    "    if swap:\n",
    "        Z = X_reference\n",
    "        X_reference = Y\n",
    "        Y = Z\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    sm.qqplot(Y.rvs(n), X_reference, ax=ax[1], line='45')\n",
    "\n",
    "    plot_X(X_reference, ax[0], type='pdf', label='Theoretical', color='red')\n",
    "    plot_X(Y, ax[0], type='pdf', label='Samples', color='dodgerblue')\n",
    "    ax[0].set_xlim(-5, 5); ax[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bc3161",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk Test for Normality\n",
    "\n",
    "The method of looking at the quantiles of the QQ-plot is a good way to check if the data is normally distributed. In fact, this idea is used in the **Shapiro-Wilk test** for normality.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_0&: X_1, X_2, \\dots, X_n \\sim N(\\mu, \\sigma^2) && \\text{ for some } \\mu \\in \\R, \\ \\sigma > 0\\\\ \\\\\n",
    "H_a&: X_1, X_2, \\dots, X_n \\neq N(\\mu, \\sigma^2) && \\text{ for any } \\mu \\in \\R, \\ \\sigma > 0\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc626551",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_normal = stats.chi2(3).rvs(100)\n",
    "normal = stats.norm(0, 1).rvs(100)\n",
    "\n",
    "alpha = 0.01\n",
    "decision(stats.shapiro(not_normal).pvalue, alpha), \n",
    "decision(stats.shapiro(normal).pvalue, alpha)\n",
    "# stats.shapiro(normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de62033",
   "metadata": {},
   "source": [
    "**Essentially, we can use QQ-polt to check whether our distributional assumption is actually legit**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15473cf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274cc86c",
   "metadata": {},
   "source": [
    "## Checking for Independence\n",
    "> Suppose we are given a sample $X_1, X_2, \\dots, X_n$ and we want to check **if the observations are independent**.\n",
    "\n",
    "How to test for truly independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c92fb",
   "metadata": {},
   "source": [
    "### Autocorrelation\n",
    "The **autocorrelation function (ACF)** is a measure of the **correlation between a time series and a lagged version of itself**. It comes from the area of `time-series analysis`:\n",
    "\n",
    "- Essentially the idea is that **we want to check if an observation is an older version of itself** by creating `lagged` observation manually.\n",
    "    - You end up with differeent versions fo your sample.\n",
    "\n",
    "- Note that the ACF presented here is actually a **biased** estimate of the true ACF. But this method suffices for getting a \"_general sense of the data_\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549ac2e",
   "metadata": {},
   "source": [
    "The ACF is defined as the following where $k$ is the lag (i.e. the number of time steps between the two observations).\n",
    "\n",
    "$$\n",
    "\\text{ACF}(k) = \\frac{\\sum_{i=1}^{n-k} (X_i - \\bar X)(X_{i+k} - \\bar X)}{\\sum_{i=1}^n (X_i - \\bar X)^2}\n",
    "$$\n",
    "\n",
    "Let's look at a particular lag and tehn look at the correlation between the two lags. If the observations are independent, then **the ACF should be close to zero for all lags**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3520f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "n = 2000\n",
    "X = np.random.normal(size=n)\n",
    "Y = np.zeros(n)\n",
    "Y[0] = X[0]\n",
    "for t in range(1, n):\n",
    "    Y[t] = 0.5*Y[t-1] + X[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42732a6",
   "metadata": {},
   "source": [
    "First case is that all observation is independent and second case is that they are related to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158cd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15, 7))\n",
    "ax[0, 0].plot(X, label='X', alpha=0.5, linewidth=1); ax[0, 0].legend()\n",
    "\n",
    "ax[1, 0].plot(Y, label='Y', alpha=0.5, linewidth=1); ax[0, 1].legend()\n",
    "\n",
    "ax[0, 1].bar(range(0, 21), acf(X, nlags=20))\n",
    "ax[0, 1].set_xlabel('Lag'); ax[0, 1].set_ylabel('ACF')\n",
    "\n",
    "ax[1, 1].bar(range(0, 21), acf(Y, nlags=20))\n",
    "ax[1, 1].set_xlabel('Lag'); ax[1, 1].set_ylabel('ACF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c938d9",
   "metadata": {},
   "source": [
    "### Ljung-Box Test\n",
    "Now to come to an conclusion of whether these variables are actually independent, we ned something else that stil takes the form of sort of  a `hypothesis testing`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75233d",
   "metadata": {},
   "source": [
    "The **Ljung-Box test** uses the autocorrelation function to test for independence. The test statistic is given by\n",
    "\n",
    "$$\n",
    "Q = n(n+2) \\sum_{k=1}^h \\frac{\\text{ACF}(k)^2}{n-k}\n",
    "$$\n",
    "\n",
    "where $h$ is the number of lags to consider. The test statistic $Q$ is $\\Chisq$-distributed with $h$ degrees of freedom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d4ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "acorr_ljungbox(X, lags=3, return_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a89d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acorr_ljungbox(Y, lags=3, return_df=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428439f5",
   "metadata": {},
   "source": [
    "## Checking for Identical Assumption\n",
    " \n",
    "> Suppose we are given a sample $X_1, X_2, \\dots, X_n$ and we want to check if the observations are identically distributed. How do we do that?\n",
    "\n",
    "\n",
    "We need to use a mixture of the methods we have looked at so far, such as the QQ-plot, the K-S test.\n",
    "\n",
    "- Usually making a plot lie this to show outliers is `sufficient to show violations in the identical assumption`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.append(stats.chi2(20).rvs(3), stats.norm().rvs(100))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sm.qqplot(data, stats.norm, line='45', ax=ax[1])\n",
    "sns.scatterplot(x=np.arange(len(data)), y=data, ax=ax[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528418a",
   "metadata": {},
   "source": [
    "In the QQ-plot, the three observations that come from a different distribution actually show up as outlers, though techniqually they can show up anywhere.\n",
    "\n",
    "- QQ-plot is robust to where the observations are, it just shwos if there are deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300\n",
    "x = np.array([stats.norm(0, i).rvs(5) for i in range(n)]).reshape(-1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sm.qqplot(x, stats.norm, line='45', ax=ax[1])\n",
    "sns.scatterplot(x=np.arange(len(x)), y=x, ax=ax[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0001698",
   "metadata": {},
   "source": [
    "This is `hetero-stadasticity`, essentially it is when there are `multiplicative noises`.\n",
    "- In here it is all normal distribution with varaicne increasing at every iterations.\n",
    "- This is a `Fanning out` effect plot.\n",
    "- Deviation has similar statistical assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2054701",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
